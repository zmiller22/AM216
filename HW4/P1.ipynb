{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:46:46.494051Z",
     "start_time": "2020-02-24T16:46:45.817790Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import gc\n",
    "from numba import cuda\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:46:46.494051Z",
     "start_time": "2020-02-24T16:46:45.817790Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# This is how the triagle lattice data is generated. You may find it helpful to generate some \n",
    "# of your own data\n",
    "class Ising_tri():\n",
    "    ''' Simulating the Ising model '''  \n",
    "    def __init__(self, size, temp):\n",
    "        self.temp = temp\n",
    "        self.N = int(size)\n",
    "    ## monte carlo moves\n",
    "    \n",
    "    def mcmove(self, config, N, beta):\n",
    "        ''' This is to execute the monte carlo moves using \n",
    "        Metropolis algorithm such that detailed\n",
    "        balance condition is satisified'''\n",
    "        for i in range(N):\n",
    "            for j in range(N):            \n",
    "                    a = np.random.randint(0, N) # select a row\n",
    "                    b = np.random.randint(0, N) # select a column\n",
    "                    s =  config[a, b] # current state at (a, b)\n",
    "                    if a%2:\n",
    "                        nb = config[(a+1)%N,b] +config[(a+1)%N,(b+1)%N] + config[a,(b+1)%N] + \\\n",
    "                        config[(a-1)%N,b] + config[(a-1)%N,(b+1)%N] + config[a,(b-1)%N]\n",
    "                    else:\n",
    "                        nb = config[(a+1)%N,b] +config[(a+1)%N,(b-1)%N] + config[a,(b+1)%N] + \\\n",
    "                        config[(a-1)%N,b] + config[(a-1)%N,(b-1)%N] + config[a,(b-1)%N]\n",
    "                    \n",
    "                    \n",
    "                    cost = 2*s*nb\n",
    "                    if cost < 0:\t\n",
    "                        s *= -1\n",
    "                    elif rand() < np.exp(-cost*beta):\n",
    "                        s *= -1\n",
    "                    config[a, b] = s\n",
    "        return config\n",
    "    \n",
    "    def simulate(self):   \n",
    "        ''' This module simulates the Ising model'''\n",
    "        config = 2*np.random.randint(2, size=(self.N,self.N))-1   \n",
    "        msrmnt = 81\n",
    "        for i in range(msrmnt):\n",
    "            self.mcmove(config, self.N, 1.0/self.temp)\n",
    "        return config\n",
    "\n",
    "class Ising_sq():\n",
    "    ''' Simulating the Ising model '''  \n",
    "    def __init__(self, size, temp):\n",
    "        self.temp = temp\n",
    "        self.N = int(size)\n",
    "    ## monte carlo moves\n",
    "\n",
    "    def mcmove(self, config, N, beta):\n",
    "        ''' This is to execute the monte carlo moves using \n",
    "        Metropolis algorithm such that detailed\n",
    "        balance condition is satisified'''\n",
    "        for i in range(N):\n",
    "            for j in range(N):            \n",
    "                    a = np.random.randint(0, N) # select a row\n",
    "                    b = np.random.randint(0, N) # select a column\n",
    "                    s =  config[a, b] # current state at (a, b)\n",
    "                    # periodic boundary condition imposed\n",
    "                    nb = config[(a+1)%N,b] + config[a,(b+1)%N] + config[(a-1)%N,b] + config[a,(b-1)%N]\n",
    "                    cost = 2*s*nb\n",
    "                    if cost < 0:\n",
    "                        s *= -1\n",
    "                    elif rand() < np.exp(-cost*beta):\n",
    "                        s *= -1\n",
    "                    config[a, b] = s\n",
    "        return config\n",
    "    \n",
    "    def simulate(self):   \n",
    "        ''' This module simulates the Ising model'''\n",
    "        config = 2*np.random.randint(2, size=(self.N,self.N))-1   \n",
    "        times = 100\n",
    "        for i in range(times):\n",
    "            self.mcmove(config, self.N, 1.0/self.temp)\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import 4-temp data for square and triangular lattices as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:30:37.313572Z",
     "start_time": "2020-02-20T22:30:34.531268Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "nx, ny = 32, 32\n",
    "\n",
    "Xsq = np.ndarray((4*N,nx,ny,1))\n",
    "ysq = np.ndarray(4*N)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    Xsq[i + 0*N] = np.loadtxt(\"./square_T1/square_T1/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 0*N] = 0\n",
    "    Xsq[i + 1*N] = np.loadtxt(\"./square_T2/square_T2/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 1*N] = 1\n",
    "    Xsq[i + 2*N] = np.loadtxt(\"./square_T3/square_T3/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 2*N] = 2\n",
    "    Xsq[i + 3*N] = np.loadtxt(\"./square_T4/square_T4/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 3*N] = 3\n",
    "\n",
    "Xsq_train, Xsq_test, ysq_train, ysq_test = train_test_split(Xsq, ysq, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:31:02.786262Z",
     "start_time": "2020-02-20T22:31:00.698414Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "nx, ny = 32, 32\n",
    "\n",
    "Xtri = np.ndarray((4*N,nx,ny,1))\n",
    "ytri = np.ndarray(4*N)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    Xtri[i + 0*N] = np.loadtxt(\"./triangle_T1/triangle_T1/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 0*N] = 0\n",
    "    Xtri[i + 1*N] = np.loadtxt(\"./triangle_T2/triangle_T2/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 1*N] = 1\n",
    "    Xtri[i + 2*N] = np.loadtxt(\"./triangle_T3/triangle_T3/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 2*N] = 2\n",
    "    Xtri[i + 3*N] = np.loadtxt(\"./triangle_T4/triangle_T4/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 3*N] = 3\n",
    "\n",
    "Xtri_train, Xtri_test, ytri_train, ytri_test = train_test_split(Xtri, ytri, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you know the shape of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:31:05.870456Z",
     "start_time": "2020-02-20T22:31:05.864585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:\n",
      "(800, 32, 32, 1) (800, 32, 32, 1)\n",
      "(800,) (800,)\n",
      "Shape of test data:\n",
      "(200, 32, 32, 1) (200, 32, 32, 1)\n",
      "(200,) (200,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data:\")\n",
    "print(Xsq_train.shape, Xtri_train.shape)\n",
    "print(ysq_train.shape, ytri_train.shape)\n",
    "print(\"Shape of test data:\")\n",
    "print(Xsq_test.shape, Xtri_test.shape)\n",
    "print(ysq_test.shape, ytri_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Train a fully connected neural network to do the classification on both datasets. Then, train  a  convolutional  neural  network  to  do  the  classification,  on  both datasets.   Make  a  table  of  your  performance  numbers  for  both  models  and  upload  these  numbers.   This,  together  with  your code,  should be uploaded to the course website when you turn in your homework.\n",
    "\n",
    "The temperatures for square lattice are $T = 1.5, 2.1, 2.4, 3.5$. $T = 2.5, 3.2, 3.8, 5$ for triangle lattice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class small_FNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(width, height, depth, num_classes, channels_first=False):\n",
    "        model = Sequential()\n",
    "\n",
    "        if channels_first == False:\n",
    "            inputShape = (height, width, depth)\n",
    "            chanDim = -1\n",
    "        elif channels_first == True:\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(256,  activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(128,  activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_model, train_data, train_lbls, test_data, \n",
    "                test_lbls, num_classes, input_shape, hyperparams):\n",
    "    # Ensure data is shaped properly, assumes channels last set up\n",
    "    x_train = train_data\n",
    "    x_test = test_data\n",
    "    \n",
    "    # Create categorical labels\n",
    "    y_train = keras.utils.to_categorical(train_lbls, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_lbls, num_classes)\n",
    "     \n",
    "    # Set hyperparameters\n",
    "    INIT_LR = hyperparams[0]# learning rate\n",
    "    EPOCHS = hyperparams[1] # number of epochs\n",
    "    BS = hyperparams[2] # batch size\n",
    "    OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "    \n",
    "    model = input_model\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=OPT, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    H = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)\n",
    "    \n",
    "    return H, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.8462 - accuracy: 0.2475 - val_loss: 1.3925 - val_accuracy: 0.2950\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.5707 - accuracy: 0.2988 - val_loss: 1.4008 - val_accuracy: 0.2850\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.4859 - accuracy: 0.3338 - val_loss: 1.3785 - val_accuracy: 0.3000\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.3184 - accuracy: 0.4025 - val_loss: 1.3373 - val_accuracy: 0.3100\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.3156 - accuracy: 0.4150 - val_loss: 1.3015 - val_accuracy: 0.3350\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.2500 - accuracy: 0.4162 - val_loss: 1.2567 - val_accuracy: 0.3600\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.1763 - accuracy: 0.4688 - val_loss: 1.2063 - val_accuracy: 0.3700\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.1818 - accuracy: 0.4725 - val_loss: 1.1678 - val_accuracy: 0.4150\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.0988 - accuracy: 0.5050 - val_loss: 1.1259 - val_accuracy: 0.4550\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.0703 - accuracy: 0.4900 - val_loss: 1.0721 - val_accuracy: 0.4700\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.0884 - accuracy: 0.50 - 0s 106us/sample - loss: 1.0728 - accuracy: 0.5163 - val_loss: 1.0508 - val_accuracy: 0.4500\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.0336 - accuracy: 0.5213 - val_loss: 1.0281 - val_accuracy: 0.4700\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.0310 - accuracy: 0.5025 - val_loss: 1.0226 - val_accuracy: 0.4600\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.9813 - accuracy: 0.5525 - val_loss: 0.9942 - val_accuracy: 0.5200\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.9786 - accuracy: 0.5650 - val_loss: 0.9794 - val_accuracy: 0.5200\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.9501 - accuracy: 0.5612 - val_loss: 0.9494 - val_accuracy: 0.5350\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.9173 - accuracy: 0.5975 - val_loss: 0.9303 - val_accuracy: 0.5550\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 0.8908 - accuracy: 0.6125 - val_loss: 0.9219 - val_accuracy: 0.5400\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.8887 - accuracy: 0.6087 - val_loss: 0.9184 - val_accuracy: 0.5400\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.8420 - accuracy: 0.6225 - val_loss: 0.9014 - val_accuracy: 0.5500\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.8468 - accuracy: 0.6125 - val_loss: 0.8959 - val_accuracy: 0.5550\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.8545 - accuracy: 0.6225 - val_loss: 0.8770 - val_accuracy: 0.5750\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.8479 - accuracy: 0.6125 - val_loss: 0.8770 - val_accuracy: 0.5800\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.8403 - accuracy: 0.6288 - val_loss: 0.8793 - val_accuracy: 0.5750\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.7914 - accuracy: 0.6575 - val_loss: 0.8745 - val_accuracy: 0.5800\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.7417 - accuracy: 0.6800 - val_loss: 0.8735 - val_accuracy: 0.6000\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.7572 - accuracy: 0.6400 - val_loss: 0.8482 - val_accuracy: 0.6100\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.7477 - accuracy: 0.6875 - val_loss: 0.8546 - val_accuracy: 0.6050\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.7389 - accuracy: 0.6737 - val_loss: 0.8501 - val_accuracy: 0.6050\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.7120 - accuracy: 0.6963 - val_loss: 0.8375 - val_accuracy: 0.6100\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.6971 - accuracy: 0.7113 - val_loss: 0.8378 - val_accuracy: 0.6000\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.6922 - accuracy: 0.7275 - val_loss: 0.8258 - val_accuracy: 0.6050\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.6496 - accuracy: 0.7400 - val_loss: 0.8208 - val_accuracy: 0.6250\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.6706 - accuracy: 0.7125 - val_loss: 0.8149 - val_accuracy: 0.6250\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.6982 - accuracy: 0.7113 - val_loss: 0.8105 - val_accuracy: 0.6300\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.6317 - accuracy: 0.7412 - val_loss: 0.8054 - val_accuracy: 0.6300\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.6521 - accuracy: 0.7300 - val_loss: 0.8236 - val_accuracy: 0.6050\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.5848 - accuracy: 0.7638 - val_loss: 0.8135 - val_accuracy: 0.6450\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.6133 - accuracy: 0.7487 - val_loss: 0.8075 - val_accuracy: 0.6300\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.6000 - accuracy: 0.7400 - val_loss: 0.7981 - val_accuracy: 0.6450\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.6316 - accuracy: 0.7312 - val_loss: 0.8051 - val_accuracy: 0.6300\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.5943 - accuracy: 0.7638 - val_loss: 0.8038 - val_accuracy: 0.6250\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.5986 - accuracy: 0.7663 - val_loss: 0.8019 - val_accuracy: 0.6400\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.5943 - accuracy: 0.7500 - val_loss: 0.8062 - val_accuracy: 0.6400\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.5888 - accuracy: 0.7750 - val_loss: 0.8024 - val_accuracy: 0.6300\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.5921 - accuracy: 0.7700 - val_loss: 0.8071 - val_accuracy: 0.6150\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.5503 - accuracy: 0.7925 - val_loss: 0.8139 - val_accuracy: 0.6050\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.5378 - accuracy: 0.7788 - val_loss: 0.8192 - val_accuracy: 0.6200\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.5129 - accuracy: 0.8087 - val_loss: 0.8205 - val_accuracy: 0.6150\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.5700 - accuracy: 0.7800 - val_loss: 0.8130 - val_accuracy: 0.6250\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.6937 - accuracy: 0.2713 - val_loss: 1.4062 - val_accuracy: 0.2500\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.5258 - accuracy: 0.3325 - val_loss: 1.4247 - val_accuracy: 0.2400\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.4040 - accuracy: 0.3762 - val_loss: 1.3986 - val_accuracy: 0.2800\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.3692 - accuracy: 0.4000 - val_loss: 1.3500 - val_accuracy: 0.2800\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 1.3063 - accuracy: 0.4062 - val_loss: 1.3244 - val_accuracy: 0.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.2634 - accuracy: 0.4263 - val_loss: 1.3042 - val_accuracy: 0.2950\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.1990 - accuracy: 0.4700 - val_loss: 1.2684 - val_accuracy: 0.3300\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.1816 - accuracy: 0.4737 - val_loss: 1.2466 - val_accuracy: 0.3500\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.1338 - accuracy: 0.4700 - val_loss: 1.2009 - val_accuracy: 0.3750\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.0609 - accuracy: 0.5312 - val_loss: 1.1917 - val_accuracy: 0.3900\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 1.0229 - accuracy: 0.5450 - val_loss: 1.1715 - val_accuracy: 0.3900\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 0.9682 - accuracy: 0.5888 - val_loss: 1.1466 - val_accuracy: 0.4350\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 0.9981 - accuracy: 0.5562 - val_loss: 1.1283 - val_accuracy: 0.4550\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 0.9585 - accuracy: 0.5638 - val_loss: 1.1069 - val_accuracy: 0.4450\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 0.9603 - accuracy: 0.5875 - val_loss: 1.0924 - val_accuracy: 0.4550\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 0.8900 - accuracy: 0.6325 - val_loss: 1.0632 - val_accuracy: 0.4800\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 0.9281 - accuracy: 0.5863 - val_loss: 1.0537 - val_accuracy: 0.4650\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 0.8483 - accuracy: 0.6275 - val_loss: 1.0681 - val_accuracy: 0.4700\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 0.8494 - accuracy: 0.6300 - val_loss: 1.0733 - val_accuracy: 0.4600\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 0.8209 - accuracy: 0.6388 - val_loss: 1.0636 - val_accuracy: 0.4850\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 0.7786 - accuracy: 0.6450 - val_loss: 1.0632 - val_accuracy: 0.4650\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 0.7724 - accuracy: 0.6687 - val_loss: 1.0551 - val_accuracy: 0.4600\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 0.7581 - accuracy: 0.6675 - val_loss: 1.0518 - val_accuracy: 0.4750\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.7172 - accuracy: 0.6812 - val_loss: 1.0705 - val_accuracy: 0.4350\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.7011 - accuracy: 0.6963 - val_loss: 1.0679 - val_accuracy: 0.4500\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.6848 - accuracy: 0.6913 - val_loss: 1.0890 - val_accuracy: 0.4550\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.6756 - accuracy: 0.6975 - val_loss: 1.1072 - val_accuracy: 0.4550\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.6479 - accuracy: 0.7300 - val_loss: 1.1044 - val_accuracy: 0.4600\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.6999 - accuracy: 0.6650 - val_loss: 1.1090 - val_accuracy: 0.4700\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.6328 - accuracy: 0.7237 - val_loss: 1.1175 - val_accuracy: 0.4700\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.6436 - accuracy: 0.7200 - val_loss: 1.1137 - val_accuracy: 0.4550\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 0.6106 - accuracy: 0.7550 - val_loss: 1.1125 - val_accuracy: 0.4450\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.5975 - accuracy: 0.7487 - val_loss: 1.1141 - val_accuracy: 0.4550\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.5850 - accuracy: 0.7450 - val_loss: 1.1217 - val_accuracy: 0.4450\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.5912 - accuracy: 0.7325 - val_loss: 1.1280 - val_accuracy: 0.4750\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.5574 - accuracy: 0.7350 - val_loss: 1.1324 - val_accuracy: 0.4650\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.5404 - accuracy: 0.7638 - val_loss: 1.1443 - val_accuracy: 0.4750\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.5370 - accuracy: 0.7725 - val_loss: 1.1506 - val_accuracy: 0.4700\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 0.5324 - accuracy: 0.7700 - val_loss: 1.1496 - val_accuracy: 0.4850\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.5722 - accuracy: 0.7425 - val_loss: 1.1442 - val_accuracy: 0.4750\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.5360 - accuracy: 0.7738 - val_loss: 1.1694 - val_accuracy: 0.4800\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.4997 - accuracy: 0.7750 - val_loss: 1.1719 - val_accuracy: 0.4750\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.5507 - accuracy: 0.7500 - val_loss: 1.1779 - val_accuracy: 0.4650\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 0.5358 - accuracy: 0.7675 - val_loss: 1.1985 - val_accuracy: 0.4550\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 0.4924 - accuracy: 0.7875 - val_loss: 1.2178 - val_accuracy: 0.4500\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 125us/sample - loss: 0.5155 - accuracy: 0.7588 - val_loss: 1.2239 - val_accuracy: 0.5050\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.5014 - accuracy: 0.7725 - val_loss: 1.2264 - val_accuracy: 0.4700\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.4908 - accuracy: 0.7638 - val_loss: 1.2368 - val_accuracy: 0.4700\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.4779 - accuracy: 0.7875 - val_loss: 1.2378 - val_accuracy: 0.4900\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.5071 - accuracy: 0.7738 - val_loss: 1.2631 - val_accuracy: 0.4800\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "fnn_model_sq = small_FNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "fnn_model_tri = small_FNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "\n",
    "# hyperparams should be a tuple of: INIT_LR, EPOCHS, BS\n",
    "FNN_hyperparams = (0.01, 50, 32)\n",
    "H_sq_FNN, sq_FNN_model = train_model(fnn_model_sq, Xsq_train, ysq_train, Xsq_test, ysq_test, num_classes, input_shape, FNN_hyperparams)\n",
    "H_tri_FNN, tri_FNN_model = train_model(fnn_model_tri, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, FNN_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Train a convolutional neural network to do the classification, on both datasets. Make a table of your performance numbers for (a) and (b). \n",
    "Try to optimize the performance of your models and compare the result.\n",
    "\n",
    "solution to (b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(width, height, depth, num_classes, channels_first=False):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        \n",
    "        if channels_first == False:\n",
    "            inputShape = (height, width, depth)\n",
    "            chanDim = -1\n",
    "        elif channels_first == True:\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        model.add(Conv2D(32, (5, 5), activation='relu', input_shape=inputShape))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "                  \n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 0s 482us/sample - loss: 1.3340 - accuracy: 0.3525 - val_loss: 1.2205 - val_accuracy: 0.3900\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 1.1370 - accuracy: 0.5325 - val_loss: 0.9143 - val_accuracy: 0.6900\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.9296 - accuracy: 0.5962 - val_loss: 0.7645 - val_accuracy: 0.7200\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.8056 - accuracy: 0.6750 - val_loss: 0.6541 - val_accuracy: 0.7350\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.5799 - accuracy: 0.7563 - val_loss: 0.9467 - val_accuracy: 0.5250\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.6365 - accuracy: 0.7125 - val_loss: 0.7360 - val_accuracy: 0.5800\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.4013 - accuracy: 0.8425 - val_loss: 0.5676 - val_accuracy: 0.7550\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.6882 - accuracy: 0.7237 - val_loss: 0.3900 - val_accuracy: 0.8400\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.4195 - accuracy: 0.8350 - val_loss: 0.3534 - val_accuracy: 0.8550\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.4161 - accuracy: 0.8325 - val_loss: 0.7593 - val_accuracy: 0.6600\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.4256 - accuracy: 0.8275 - val_loss: 0.3202 - val_accuracy: 0.8750\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.2994 - accuracy: 0.8875 - val_loss: 0.4774 - val_accuracy: 0.7850\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.2811 - accuracy: 0.8925 - val_loss: 0.4779 - val_accuracy: 0.7850\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.3123 - accuracy: 0.8662 - val_loss: 0.2876 - val_accuracy: 0.8700\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.3312 - accuracy: 0.8650 - val_loss: 0.7559 - val_accuracy: 0.6900\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.3507 - accuracy: 0.8763 - val_loss: 0.3083 - val_accuracy: 0.8700\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.2404 - accuracy: 0.9150 - val_loss: 0.2772 - val_accuracy: 0.8800\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.2324 - accuracy: 0.9150 - val_loss: 0.2769 - val_accuracy: 0.8800\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.2250 - accuracy: 0.9112 - val_loss: 0.2367 - val_accuracy: 0.8950\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.2365 - accuracy: 0.8988 - val_loss: 0.2453 - val_accuracy: 0.8900\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.2477 - accuracy: 0.9087 - val_loss: 0.2876 - val_accuracy: 0.8750\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1803 - accuracy: 0.9362 - val_loss: 0.2645 - val_accuracy: 0.8600\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.2362 - accuracy: 0.9013 - val_loss: 0.7297 - val_accuracy: 0.7300\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.2277 - accuracy: 0.9125 - val_loss: 0.1916 - val_accuracy: 0.9100\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.1650 - accuracy: 0.9350 - val_loss: 0.2766 - val_accuracy: 0.8800\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1575 - accuracy: 0.9463 - val_loss: 0.2408 - val_accuracy: 0.8900\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1477 - accuracy: 0.9513 - val_loss: 0.1829 - val_accuracy: 0.9300\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.1589 - accuracy: 0.9388 - val_loss: 0.1708 - val_accuracy: 0.9400\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.1727 - accuracy: 0.9312 - val_loss: 0.1746 - val_accuracy: 0.9350\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.1585 - accuracy: 0.9388 - val_loss: 0.1635 - val_accuracy: 0.9350\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.1526 - accuracy: 0.9463 - val_loss: 0.1600 - val_accuracy: 0.9200\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.1535 - accuracy: 0.9375 - val_loss: 0.1733 - val_accuracy: 0.9350\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1208 - accuracy: 0.9563 - val_loss: 0.1414 - val_accuracy: 0.9400\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1371 - accuracy: 0.9500 - val_loss: 0.1524 - val_accuracy: 0.9300\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.1484 - accuracy: 0.9463 - val_loss: 0.1568 - val_accuracy: 0.9500\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1516 - accuracy: 0.9287 - val_loss: 0.1600 - val_accuracy: 0.9450\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.1361 - accuracy: 0.9513 - val_loss: 0.2214 - val_accuracy: 0.9000\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1298 - accuracy: 0.9588 - val_loss: 0.1389 - val_accuracy: 0.9550\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.0985 - accuracy: 0.9625 - val_loss: 0.1364 - val_accuracy: 0.9500\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.1481 - accuracy: 0.9513 - val_loss: 0.2450 - val_accuracy: 0.8950\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1190 - accuracy: 0.9588 - val_loss: 0.1862 - val_accuracy: 0.9200\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1296 - accuracy: 0.9538 - val_loss: 0.1329 - val_accuracy: 0.9550\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1238 - accuracy: 0.9500 - val_loss: 0.3356 - val_accuracy: 0.8800\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.1227 - accuracy: 0.9600 - val_loss: 0.1656 - val_accuracy: 0.9250\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 73us/sample - loss: 0.1338 - accuracy: 0.9450 - val_loss: 0.1190 - val_accuracy: 0.9600\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.1127 - accuracy: 0.9613 - val_loss: 0.1613 - val_accuracy: 0.9400\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.1103 - accuracy: 0.9638 - val_loss: 0.1838 - val_accuracy: 0.9000\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.1257 - accuracy: 0.9500 - val_loss: 0.1196 - val_accuracy: 0.9600\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1023 - accuracy: 0.9625 - val_loss: 0.1189 - val_accuracy: 0.9600\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.0997 - accuracy: 0.9663 - val_loss: 0.1473 - val_accuracy: 0.9400\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 0s 487us/sample - loss: 1.3921 - accuracy: 0.2663 - val_loss: 1.3281 - val_accuracy: 0.4100\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 73us/sample - loss: 1.3030 - accuracy: 0.4112 - val_loss: 1.2352 - val_accuracy: 0.4200\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 1.1236 - accuracy: 0.5275 - val_loss: 0.8946 - val_accuracy: 0.6900\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 1.0228 - accuracy: 0.5300 - val_loss: 0.7860 - val_accuracy: 0.6850\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.8056 - accuracy: 0.6137 - val_loss: 1.9280 - val_accuracy: 0.3400\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 85us/sample - loss: 0.7681 - accuracy: 0.7025 - val_loss: 0.8330 - val_accuracy: 0.4900\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.6390 - accuracy: 0.7163 - val_loss: 0.7733 - val_accuracy: 0.6300\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.5582 - accuracy: 0.7387 - val_loss: 0.4406 - val_accuracy: 0.8550\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.4732 - accuracy: 0.7975 - val_loss: 1.1859 - val_accuracy: 0.4650\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.7531 - accuracy: 0.7150 - val_loss: 0.4038 - val_accuracy: 0.9050\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.4569 - accuracy: 0.7987 - val_loss: 0.4363 - val_accuracy: 0.8650\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.4001 - accuracy: 0.8238 - val_loss: 0.4184 - val_accuracy: 0.8600\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.4719 - accuracy: 0.7875 - val_loss: 0.3905 - val_accuracy: 0.7950\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.4676 - accuracy: 0.7937 - val_loss: 0.3219 - val_accuracy: 0.8800\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.3675 - accuracy: 0.8400 - val_loss: 0.7257 - val_accuracy: 0.7250\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.3898 - accuracy: 0.8325 - val_loss: 0.3589 - val_accuracy: 0.8700\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.3209 - accuracy: 0.8763 - val_loss: 0.2969 - val_accuracy: 0.8900\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.3523 - accuracy: 0.8450 - val_loss: 0.6119 - val_accuracy: 0.7000\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.4593 - accuracy: 0.7912 - val_loss: 0.2641 - val_accuracy: 0.9350\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.2836 - accuracy: 0.8863 - val_loss: 0.3025 - val_accuracy: 0.9050\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.3662 - accuracy: 0.8388 - val_loss: 0.4098 - val_accuracy: 0.7900\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.2907 - accuracy: 0.8800 - val_loss: 0.3078 - val_accuracy: 0.8500\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.2774 - accuracy: 0.8775 - val_loss: 0.2113 - val_accuracy: 0.9450\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.2420 - accuracy: 0.9050 - val_loss: 0.2229 - val_accuracy: 0.9450\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.2559 - accuracy: 0.8913 - val_loss: 0.2124 - val_accuracy: 0.9450\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.3391 - accuracy: 0.8450 - val_loss: 0.2160 - val_accuracy: 0.9300\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.2057 - accuracy: 0.9212 - val_loss: 0.2107 - val_accuracy: 0.9350\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.2344 - accuracy: 0.9062 - val_loss: 0.2238 - val_accuracy: 0.9300\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1930 - accuracy: 0.9262 - val_loss: 0.1993 - val_accuracy: 0.9400\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1974 - accuracy: 0.9237 - val_loss: 0.2494 - val_accuracy: 0.9300\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 93us/sample - loss: 0.2134 - accuracy: 0.9137 - val_loss: 1.1194 - val_accuracy: 0.5650\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.5739 - accuracy: 0.8025 - val_loss: 0.1792 - val_accuracy: 0.9550\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1866 - accuracy: 0.9413 - val_loss: 0.1573 - val_accuracy: 0.9650\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1620 - accuracy: 0.9525 - val_loss: 0.1948 - val_accuracy: 0.9450\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1705 - accuracy: 0.9388 - val_loss: 0.1956 - val_accuracy: 0.9250\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.2047 - accuracy: 0.9225 - val_loss: 0.1788 - val_accuracy: 0.9600\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.1611 - accuracy: 0.9362 - val_loss: 0.1703 - val_accuracy: 0.9650\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.1608 - accuracy: 0.9413 - val_loss: 0.1496 - val_accuracy: 0.9700\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1531 - accuracy: 0.9425 - val_loss: 0.2059 - val_accuracy: 0.9150\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1583 - accuracy: 0.9425 - val_loss: 0.1809 - val_accuracy: 0.9600\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1314 - accuracy: 0.9563 - val_loss: 0.2451 - val_accuracy: 0.9250\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.1804 - accuracy: 0.9388 - val_loss: 0.1259 - val_accuracy: 0.9550\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.1293 - accuracy: 0.9563 - val_loss: 0.1301 - val_accuracy: 0.9550\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.1355 - accuracy: 0.9500 - val_loss: 0.1342 - val_accuracy: 0.9650\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.1217 - accuracy: 0.9513 - val_loss: 0.1335 - val_accuracy: 0.9450\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1336 - accuracy: 0.9538 - val_loss: 0.1373 - val_accuracy: 0.9750\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.0918 - accuracy: 0.9675 - val_loss: 0.1372 - val_accuracy: 0.9500\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1134 - accuracy: 0.9475 - val_loss: 0.1200 - val_accuracy: 0.9600\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1264 - accuracy: 0.9600 - val_loss: 0.1170 - val_accuracy: 0.9650\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.1243 - accuracy: 0.9600 - val_loss: 0.1617 - val_accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "cnn_model_sq = SimpleCNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "cnn_model_tri = SimpleCNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "\n",
    "CNN_hyperparams = (0.01, 50, 64)\n",
    "H_sq_CNN, sq_CNN_model = train_model(cnn_model_sq, Xsq_train, ysq_train, Xsq_test, ysq_test, num_classes, input_shape, CNN_hyperparams)\n",
    "H_tri_CNN, tri_CNN_model = train_model(cnn_model_tri, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, CNN_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) We have provided a test set of 10 spins configurations for each of the two problems. Each of the spin configurations is not necessarily at the temperatures of the training sets. Calculate your best estimate of the temperatures of these spin configuration. Upload your results to Kaggle.\n",
    "[Hint: A direct fingerprint of temperature is the distribution of spin up\n",
    "and down, because you can image that the spins fluctuate more violently\n",
    "at higher temperature. Although the mothod you use in homework 2 can also work, you may be interested in trying to take distribution into account when you\n",
    "build the model to estimate temperature and see if you can make use of this extra information. This may help you win the\n",
    "kaggle. It is totally fine if you find that the information of distribution is not helpful. Note also that a CNN kind-of does this. One possibility is that you may want a CNN that captures enough distribution information.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsq_sim_data = []\\ntri_sim_data = []\\nsim_data_temp = []\\n\\nfor temp in temps_vec:    \\n    sq_ising_simu = Ising_sq(32, temp)\\n    tri_ising_simu = Ising_tri(32, temp)\\n    sq_img = sq_ising_simu.simulate()\\n    tri_img = tri_ising_simu.simulate()\\n    \\n    sq_sim_data.append(sq_img)\\n    sq_sim_data.append(sq_img)\\n    \\ncwd = str(os.getcwd())\\nsim_data_path_2 = cwd+\"/sim_data_2.npy\"\\nsim_data_temp_path_2 = cwd+\"/sim_data_temp_2.npy\"\\nsim_data_tot_lv_mean_path = cwd+\"/sim_data_tot_lv_mean.npy\"\\n\\nnp.save(sq_sim_data_path, np.asarray(sq_sim_data))\\nnp.save(tri_sim_data_path, np.asarray(tri_sim_data))\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = str(os.getcwd())\n",
    "sq_sim_data_path = cwd+\"/sq_sim_data.npy\"\n",
    "tri_sim_data_path = cwd+\"/tri_sim_data.npy\"\n",
    "\n",
    "temps_vec = np.linspace(0.01,15,num=1500)\n",
    "\n",
    "\"\"\"\n",
    "sq_sim_data = []\n",
    "tri_sim_data = []\n",
    "sim_data_temp = []\n",
    "\n",
    "for temp in temps_vec:    \n",
    "    sq_ising_simu = Ising_sq(32, temp)\n",
    "    tri_ising_simu = Ising_tri(32, temp)\n",
    "    sq_img = sq_ising_simu.simulate()\n",
    "    tri_img = tri_ising_simu.simulate()\n",
    "    \n",
    "    sq_sim_data.append(sq_img)\n",
    "    tri_sim_data.append(tri_img)\n",
    "    \n",
    "cwd = str(os.getcwd())\n",
    "sim_data_path_2 = cwd+\"/sim_data_2.npy\"\n",
    "sim_data_temp_path_2 = cwd+\"/sim_data_temp_2.npy\"\n",
    "sim_data_tot_lv_mean_path = cwd+\"/sim_data_tot_lv_mean.npy\"\n",
    "\n",
    "np.save(sq_sim_data_path, np.asarray(sq_sim_data))\n",
    "np.save(tri_sim_data_path, np.asarray(tri_sim_data))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data ready to be embedded\n",
    "sq_sim_data = np.load(sq_sim_data_path)[0:1500,:,:].reshape(1500,32,32,-1)\n",
    "tri_sim_data = np.load(tri_sim_data_path).reshape(1500,32,32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_sim_data, tri_sim_data = np.repeat(sq_sim_data, 5, 1), np.repeat(tri_sim_data, 5, 1)\n",
    "sq_sim_data, tri_sim_data = np.repeat(sq_sim_data, 5, 2), np.repeat(tri_sim_data, 5, 2)\n",
    "sq_sim_data, tri_sim_data = np.repeat(sq_sim_data, 3, 3), np.repeat(tri_sim_data, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 160, 160, 0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_sim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pre-trained MobileNetV2 \n",
    "with tf.device('/CPU:0'):\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=(160,160,3),\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')\n",
    "    base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedded data\n",
    "global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "sq_sim_data_emb = global_avg_layer(base_model.predict(sq_sim_data))\n",
    "sq_sim_data_emb = sq_sim_data_emb.numpy()/sq_sim_data_emb.numpy().max()\n",
    "\n",
    "#tri_sim_data_emb = global_avg_layer(base_model.predict(tri_sim_data))\n",
    "#tri_sim_data_emb = tri_sim_data_emb.numpy()/tri_sim_data_emb.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test splits\n",
    "sq_x_train, sq_x_test, sq_y_train, sq_y_test = train_test_split(sq_sim_data_emb, temps_vec, test_size=0.2, random_state=0)\n",
    "#tri_x_train, tri_x_test, tri_y_train, tri_y_test = train_test_split(tri_sim_data_emb, temps_vec, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class RegressionHead:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(channels_first=False):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(num_classes, activation='linear'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "1200/1200 [==============================] - 1s 519us/sample - loss: 56.0420 - MeanSquaredError: 56.0420 - val_loss: 59.5491 - val_MeanSquaredError: 59.5491\n",
      "Epoch 2/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 43.7723 - MeanSquaredError: 43.7723 - val_loss: 49.8098 - val_MeanSquaredError: 49.8098\n",
      "Epoch 3/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 35.8806 - MeanSquaredError: 35.8806 - val_loss: 40.3151 - val_MeanSquaredError: 40.3151\n",
      "Epoch 4/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 28.7519 - MeanSquaredError: 28.7519 - val_loss: 35.4987 - val_MeanSquaredError: 35.4987\n",
      "Epoch 5/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 22.7629 - MeanSquaredError: 22.7629 - val_loss: 31.1740 - val_MeanSquaredError: 31.1740\n",
      "Epoch 6/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 18.0515 - MeanSquaredError: 18.0515 - val_loss: 23.5638 - val_MeanSquaredError: 23.5638\n",
      "Epoch 7/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 14.2786 - MeanSquaredError: 14.2786 - val_loss: 15.7858 - val_MeanSquaredError: 15.7858\n",
      "Epoch 8/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 10.9619 - MeanSquaredError: 10.9619 - val_loss: 12.6562 - val_MeanSquaredError: 12.6562\n",
      "Epoch 9/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 8.9495 - MeanSquaredError: 8.9495 - val_loss: 10.9549 - val_MeanSquaredError: 10.9549\n",
      "Epoch 10/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 7.2554 - MeanSquaredError: 7.2554 - val_loss: 6.1791 - val_MeanSquaredError: 6.1791\n",
      "Epoch 11/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 5.5629 - MeanSquaredError: 5.5629 - val_loss: 6.5372 - val_MeanSquaredError: 6.5372\n",
      "Epoch 12/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 4.9180 - MeanSquaredError: 4.9180 - val_loss: 4.8416 - val_MeanSquaredError: 4.8416\n",
      "Epoch 13/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.1246 - MeanSquaredError: 4.1246 - val_loss: 4.0117 - val_MeanSquaredError: 4.0117\n",
      "Epoch 14/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.8582 - MeanSquaredError: 3.8582 - val_loss: 2.9572 - val_MeanSquaredError: 2.9572\n",
      "Epoch 15/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.3708 - MeanSquaredError: 3.3708 - val_loss: 2.9719 - val_MeanSquaredError: 2.9719\n",
      "Epoch 16/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.2787 - MeanSquaredError: 3.2787 - val_loss: 2.4619 - val_MeanSquaredError: 2.4619\n",
      "Epoch 17/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.0741 - MeanSquaredError: 3.0741 - val_loss: 2.5290 - val_MeanSquaredError: 2.5290\n",
      "Epoch 18/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 3.0462 - MeanSquaredError: 3.0462 - val_loss: 1.7212 - val_MeanSquaredError: 1.7212\n",
      "Epoch 19/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 2.7245 - MeanSquaredError: 2.7245 - val_loss: 1.3806 - val_MeanSquaredError: 1.3806\n",
      "Epoch 20/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 2.7144 - MeanSquaredError: 2.7144 - val_loss: 1.1839 - val_MeanSquaredError: 1.1839\n",
      "Epoch 21/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 2.8711 - MeanSquaredError: 2.8711 - val_loss: 1.2811 - val_MeanSquaredError: 1.2811\n",
      "Epoch 22/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 2.4948 - MeanSquaredError: 2.4948 - val_loss: 1.1097 - val_MeanSquaredError: 1.1097\n",
      "Epoch 23/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 2.8179 - MeanSquaredError: 2.8179 - val_loss: 0.9480 - val_MeanSquaredError: 0.9480\n",
      "Epoch 24/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 2.5674 - MeanSquaredError: 2.5674 - val_loss: 0.9441 - val_MeanSquaredError: 0.9441\n",
      "Epoch 25/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 2.4346 - MeanSquaredError: 2.4346 - val_loss: 1.0923 - val_MeanSquaredError: 1.0923\n",
      "Epoch 26/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 2.4522 - MeanSquaredError: 2.4522 - val_loss: 1.1979 - val_MeanSquaredError: 1.1979\n",
      "Epoch 27/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 2.4772 - MeanSquaredError: 2.4772 - val_loss: 1.3318 - val_MeanSquaredError: 1.3318\n",
      "Epoch 28/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 2.3744 - MeanSquaredError: 2.3744 - val_loss: 1.3166 - val_MeanSquaredError: 1.3166\n",
      "Epoch 29/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 2.4123 - MeanSquaredError: 2.4123 - val_loss: 1.9290 - val_MeanSquaredError: 1.9290\n",
      "Epoch 30/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 2.2445 - MeanSquaredError: 2.2445 - val_loss: 0.9719 - val_MeanSquaredError: 0.9719\n",
      "Epoch 31/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 2.3942 - MeanSquaredError: 2.3942 - val_loss: 0.8373 - val_MeanSquaredError: 0.8373\n",
      "Epoch 32/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 2.5640 - MeanSquaredError: 2.5640 - val_loss: 1.4262 - val_MeanSquaredError: 1.4262\n",
      "Epoch 33/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 2.5212 - MeanSquaredError: 2.5212 - val_loss: 0.8923 - val_MeanSquaredError: 0.8923\n",
      "Epoch 34/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 2.2068 - MeanSquaredError: 2.2068 - val_loss: 1.0836 - val_MeanSquaredError: 1.0836\n",
      "Epoch 35/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 2.2485 - MeanSquaredError: 2.2485 - val_loss: 0.8968 - val_MeanSquaredError: 0.8968\n",
      "Epoch 36/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 2.3286 - MeanSquaredError: 2.3286 - val_loss: 1.4233 - val_MeanSquaredError: 1.4233\n",
      "Epoch 37/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 2.2086 - MeanSquaredError: 2.2086 - val_loss: 1.2699 - val_MeanSquaredError: 1.2699\n",
      "Epoch 38/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 2.2247 - MeanSquaredError: 2.2247 - val_loss: 1.0992 - val_MeanSquaredError: 1.0992\n",
      "Epoch 39/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 2.4931 - MeanSquaredError: 2.4931 - val_loss: 0.8731 - val_MeanSquaredError: 0.8731\n",
      "Epoch 40/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 2.4108 - MeanSquaredError: 2.4108 - val_loss: 0.8503 - val_MeanSquaredError: 0.8503\n",
      "Epoch 41/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 2.1803 - MeanSquaredError: 2.1803 - val_loss: 0.8457 - val_MeanSquaredError: 0.8457\n",
      "Epoch 42/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 2.0818 - MeanSquaredError: 2.0818 - val_loss: 0.8514 - val_MeanSquaredError: 0.8514\n",
      "Epoch 43/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 2.2606 - MeanSquaredError: 2.2606 - val_loss: 0.8741 - val_MeanSquaredError: 0.8741\n",
      "Epoch 44/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 2.2886 - MeanSquaredError: 2.2886 - val_loss: 0.9074 - val_MeanSquaredError: 0.9074\n",
      "Epoch 45/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 2.2933 - MeanSquaredError: 2.2933 - val_loss: 0.8317 - val_MeanSquaredError: 0.8317\n",
      "Epoch 46/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 2.4128 - MeanSquaredError: 2.4128 - val_loss: 0.9505 - val_MeanSquaredError: 0.9505\n",
      "Epoch 47/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 2.0068 - MeanSquaredError: 2.0068 - val_loss: 0.8427 - val_MeanSquaredError: 0.8427\n",
      "Epoch 48/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 2.2000 - MeanSquaredError: 2.2000 - val_loss: 1.0403 - val_MeanSquaredError: 1.0403\n",
      "Epoch 49/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 2.1722 - MeanSquaredError: 2.1722 - val_loss: 1.0253 - val_MeanSquaredError: 1.0253\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 80us/sample - loss: 1.9682 - MeanSquaredError: 1.9682 - val_loss: 0.9610 - val_MeanSquaredError: 0.9610\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.01# learning rate\n",
    "EPOCHS = 50 # number of epochs\n",
    "BS = 32 # batch size\n",
    "OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "LOSS = keras.losses.MeanSquaredError()\n",
    "\n",
    "sq_reg_model = RegressionHead.build()\n",
    "sq_reg_model.compile(optimizer=OPT, loss=LOSS, metrics=['MeanSquaredError'])\n",
    "sq_reg_H = sq_reg_model.fit(sq_x_train, sq_y_train, validation_data=(sq_x_test, sq_y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 0.01# learning rate\n",
    "EPOCHS = 50 # number of epochs\n",
    "BS = 64 # batch size\n",
    "OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "\n",
    "tri_reg_model = RegressionHead.build()\n",
    "tri_reg_model.compile(optimizer=OPT, loss='MeanSquaredError', metrics=['MeanSquaredError'])\n",
    "tri_reg_H = tri_reg_model.fit(tri_x_train, tri_y_train, validation_data=(tri_x_test, tri_y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) *Transfer Learning*.  \n",
    "As we emphasize in class, one can freeze the training of the bottom layers of a network and retrain the top part of the network to adopt to a new situation. Use your CNN that you trained on the squarelattice data to do transfer learning on the triangular lattice data.  How does the performance compare to that of the direct methods?  Add the performance numbers for transfer learning in your table from Part (a). Note that the training time and number of training examples needed for transfer learning is far less than that for the direct  optimization. For  example,  is  50  triangle  example  sufficient  for the re-training process?  Use your transfer learning result to predict the transition temperature of triangle lattice Ising model, as demonstrated in this [Nature Physics](https://www-nature-com.ezp-prod1.hul.harvard.edu/articles/nphys4035.pdf) publication.\n",
    "\n",
    "As a guideline, you may like to just change the last `Dense` layer with `softmax` activation when you do the transfer learning. Other choices are also OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:51:16.394160Z",
     "start_time": "2020-02-24T16:51:16.390887Z"
    }
   },
   "source": [
    "Solution to (d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = sq_CNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = base_model.layers[0:5]\n",
    "trainable_layers = [\n",
    "     Flatten(),\n",
    "     Dropout(0.25),\n",
    "     Dense(32, activation='relu'),\n",
    "     Dense(num_classes, activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_model = keras.Sequential(base_layers+trainable_layers)\n",
    "trans_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "hyperparams = (0.01, 25, 32)\n",
    "H_trans, trained_trans_model = train_model(trans_model, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
