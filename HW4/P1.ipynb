{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:46:46.494051Z",
     "start_time": "2020-02-24T16:46:45.817790Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import gc\n",
    "from numba import cuda\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:46:46.494051Z",
     "start_time": "2020-02-24T16:46:45.817790Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# This is how the triagle lattice data is generated. You may find it helpful to generate some \n",
    "# of your own data\n",
    "class Ising_tri():\n",
    "    ''' Simulating the Ising model '''  \n",
    "    def __init__(self, size, temp):\n",
    "        self.temp = temp\n",
    "        self.N = int(size)\n",
    "    ## monte carlo moves\n",
    "    \n",
    "    def mcmove(self, config, N, beta):\n",
    "        ''' This is to execute the monte carlo moves using \n",
    "        Metropolis algorithm such that detailed\n",
    "        balance condition is satisified'''\n",
    "        for i in range(N):\n",
    "            for j in range(N):            \n",
    "                    a = np.random.randint(0, N) # select a row\n",
    "                    b = np.random.randint(0, N) # select a column\n",
    "                    s =  config[a, b] # current state at (a, b)\n",
    "                    if a%2:\n",
    "                        nb = config[(a+1)%N,b] +config[(a+1)%N,(b+1)%N] + config[a,(b+1)%N] + \\\n",
    "                        config[(a-1)%N,b] + config[(a-1)%N,(b+1)%N] + config[a,(b-1)%N]\n",
    "                    else:\n",
    "                        nb = config[(a+1)%N,b] +config[(a+1)%N,(b-1)%N] + config[a,(b+1)%N] + \\\n",
    "                        config[(a-1)%N,b] + config[(a-1)%N,(b-1)%N] + config[a,(b-1)%N]\n",
    "                    \n",
    "                    \n",
    "                    cost = 2*s*nb\n",
    "                    if cost < 0:\t\n",
    "                        s *= -1\n",
    "                    elif rand() < np.exp(-cost*beta):\n",
    "                        s *= -1\n",
    "                    config[a, b] = s\n",
    "        return config\n",
    "    \n",
    "    def simulate(self):   \n",
    "        ''' This module simulates the Ising model'''\n",
    "        config = 2*np.random.randint(2, size=(self.N,self.N))-1   \n",
    "        msrmnt = 81\n",
    "        for i in range(msrmnt):\n",
    "            self.mcmove(config, self.N, 1.0/self.temp)\n",
    "        return config\n",
    "\n",
    "class Ising_sq():\n",
    "    ''' Simulating the Ising model '''  \n",
    "    def __init__(self, size, temp):\n",
    "        self.temp = temp\n",
    "        self.N = int(size)\n",
    "    ## monte carlo moves\n",
    "\n",
    "    def mcmove(self, config, N, beta):\n",
    "        ''' This is to execute the monte carlo moves using \n",
    "        Metropolis algorithm such that detailed\n",
    "        balance condition is satisified'''\n",
    "        for i in range(N):\n",
    "            for j in range(N):            \n",
    "                    a = np.random.randint(0, N) # select a row\n",
    "                    b = np.random.randint(0, N) # select a column\n",
    "                    s =  config[a, b] # current state at (a, b)\n",
    "                    # periodic boundary condition imposed\n",
    "                    nb = config[(a+1)%N,b] + config[a,(b+1)%N] + config[(a-1)%N,b] + config[a,(b-1)%N]\n",
    "                    cost = 2*s*nb\n",
    "                    if cost < 0:\n",
    "                        s *= -1\n",
    "                    elif rand() < np.exp(-cost*beta):\n",
    "                        s *= -1\n",
    "                    config[a, b] = s\n",
    "        return config\n",
    "    \n",
    "    def simulate(self):   \n",
    "        ''' This module simulates the Ising model'''\n",
    "        config = 2*np.random.randint(2, size=(self.N,self.N))-1   \n",
    "        times = 100\n",
    "        for i in range(times):\n",
    "            self.mcmove(config, self.N, 1.0/self.temp)\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import 4-temp data for square and triangular lattices as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:30:37.313572Z",
     "start_time": "2020-02-20T22:30:34.531268Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "nx, ny = 32, 32\n",
    "\n",
    "Xsq = np.ndarray((4*N,nx,ny,1))\n",
    "ysq = np.ndarray(4*N)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    Xsq[i + 0*N] = np.loadtxt(\"./square_T1/square_T1/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 0*N] = 0\n",
    "    Xsq[i + 1*N] = np.loadtxt(\"./square_T2/square_T2/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 1*N] = 1\n",
    "    Xsq[i + 2*N] = np.loadtxt(\"./square_T3/square_T3/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 2*N] = 2\n",
    "    Xsq[i + 3*N] = np.loadtxt(\"./square_T4/square_T4/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 3*N] = 3\n",
    "\n",
    "Xsq_train, Xsq_test, ysq_train, ysq_test = train_test_split(Xsq, ysq, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:31:02.786262Z",
     "start_time": "2020-02-20T22:31:00.698414Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "nx, ny = 32, 32\n",
    "\n",
    "Xtri = np.ndarray((4*N,nx,ny,1))\n",
    "ytri = np.ndarray(4*N)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    Xtri[i + 0*N] = np.loadtxt(\"./triangle_T1/triangle_T1/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 0*N] = 0\n",
    "    Xtri[i + 1*N] = np.loadtxt(\"./triangle_T2/triangle_T2/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 1*N] = 1\n",
    "    Xtri[i + 2*N] = np.loadtxt(\"./triangle_T3/triangle_T3/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 2*N] = 2\n",
    "    Xtri[i + 3*N] = np.loadtxt(\"./triangle_T4/triangle_T4/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 3*N] = 3\n",
    "\n",
    "Xtri_train, Xtri_test, ytri_train, ytri_test = train_test_split(Xtri, ytri, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you know the shape of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:31:05.870456Z",
     "start_time": "2020-02-20T22:31:05.864585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:\n",
      "(800, 32, 32, 1) (800, 32, 32, 1)\n",
      "(800,) (800,)\n",
      "Shape of test data:\n",
      "(200, 32, 32, 1) (200, 32, 32, 1)\n",
      "(200,) (200,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data:\")\n",
    "print(Xsq_train.shape, Xtri_train.shape)\n",
    "print(ysq_train.shape, ytri_train.shape)\n",
    "print(\"Shape of test data:\")\n",
    "print(Xsq_test.shape, Xtri_test.shape)\n",
    "print(ysq_test.shape, ytri_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Train a fully connected neural network to do the classification on both datasets. Then, train  a  convolutional  neural  network  to  do  the  classification,  on  both datasets.   Make  a  table  of  your  performance  numbers  for  both  models  and  upload  these  numbers.   This,  together  with  your code,  should be uploaded to the course website when you turn in your homework.\n",
    "\n",
    "The temperatures for square lattice are $T = 1.5, 2.1, 2.4, 3.5$. $T = 2.5, 3.2, 3.8, 5$ for triangle lattice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class small_FNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(width, height, depth, num_classes, channels_first=False):\n",
    "        model = Sequential()\n",
    "\n",
    "        if channels_first == False:\n",
    "            inputShape = (height, width, depth)\n",
    "            chanDim = -1\n",
    "        elif channels_first == True:\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(256,  activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(128,  activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_model, train_data, train_lbls, test_data, \n",
    "                test_lbls, num_classes, input_shape, hyperparams):\n",
    "    # Ensure data is shaped properly, assumes channels last set up\n",
    "    x_train = train_data\n",
    "    x_test = test_data\n",
    "    \n",
    "    # Create categorical labels\n",
    "    y_train = keras.utils.to_categorical(train_lbls, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_lbls, num_classes)\n",
    "     \n",
    "    # Set hyperparameters\n",
    "    INIT_LR = hyperparams[0]# learning rate\n",
    "    EPOCHS = hyperparams[1] # number of epochs\n",
    "    BS = hyperparams[2] # batch size\n",
    "    OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "    \n",
    "    model = input_model\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=OPT, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    H = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)\n",
    "    \n",
    "    return H, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.7038 - accuracy: 0.3013 - val_loss: 1.4197 - val_accuracy: 0.2650\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.6188 - accuracy: 0.3200 - val_loss: 1.3636 - val_accuracy: 0.3050\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.4346 - accuracy: 0.3462 - val_loss: 1.3421 - val_accuracy: 0.3100\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.3745 - accuracy: 0.3762 - val_loss: 1.3138 - val_accuracy: 0.3450\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.3112 - accuracy: 0.4075 - val_loss: 1.2906 - val_accuracy: 0.3500\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.2848 - accuracy: 0.4350 - val_loss: 1.2687 - val_accuracy: 0.3700\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.2675 - accuracy: 0.4225 - val_loss: 1.2164 - val_accuracy: 0.3950\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.2416 - accuracy: 0.4375 - val_loss: 1.1849 - val_accuracy: 0.4200\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.2042 - accuracy: 0.4538 - val_loss: 1.1355 - val_accuracy: 0.4300\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.1325 - accuracy: 0.4925 - val_loss: 1.1302 - val_accuracy: 0.4350\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.1033 - accuracy: 0.5000 - val_loss: 1.1084 - val_accuracy: 0.4650\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.0772 - accuracy: 0.4988 - val_loss: 1.0844 - val_accuracy: 0.4700\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.0354 - accuracy: 0.5487 - val_loss: 1.0647 - val_accuracy: 0.5100\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 1.0143 - accuracy: 0.5550 - val_loss: 1.0268 - val_accuracy: 0.5050\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.9845 - accuracy: 0.5775 - val_loss: 1.0123 - val_accuracy: 0.5050\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.9792 - accuracy: 0.5750 - val_loss: 0.9978 - val_accuracy: 0.5150\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 0.9650 - accuracy: 0.5775 - val_loss: 0.9922 - val_accuracy: 0.5050\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 0.9376 - accuracy: 0.5675 - val_loss: 0.9851 - val_accuracy: 0.4950\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 0.9188 - accuracy: 0.5813 - val_loss: 0.9638 - val_accuracy: 0.5650\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 0.8836 - accuracy: 0.5938 - val_loss: 0.9593 - val_accuracy: 0.5400\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 0.8497 - accuracy: 0.6363 - val_loss: 0.9555 - val_accuracy: 0.5700\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.8341 - accuracy: 0.6475 - val_loss: 0.9583 - val_accuracy: 0.5400\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.8342 - accuracy: 0.6263 - val_loss: 0.9532 - val_accuracy: 0.5300\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 0.7990 - accuracy: 0.6562 - val_loss: 0.9474 - val_accuracy: 0.5350\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 0.8114 - accuracy: 0.6712 - val_loss: 0.9458 - val_accuracy: 0.5450\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.7857 - accuracy: 0.6488 - val_loss: 0.9431 - val_accuracy: 0.5600\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 0.7756 - accuracy: 0.6737 - val_loss: 0.9461 - val_accuracy: 0.5550\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 0.7582 - accuracy: 0.6812 - val_loss: 0.9394 - val_accuracy: 0.5350\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.7525 - accuracy: 0.6950 - val_loss: 0.9339 - val_accuracy: 0.5300\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.7407 - accuracy: 0.6950 - val_loss: 0.9361 - val_accuracy: 0.5400\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 0.7340 - accuracy: 0.6837 - val_loss: 0.9326 - val_accuracy: 0.5500\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.6728 - accuracy: 0.7212 - val_loss: 0.9336 - val_accuracy: 0.5550\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.7287 - accuracy: 0.6825 - val_loss: 0.9265 - val_accuracy: 0.5400\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.6453 - accuracy: 0.7175 - val_loss: 0.9205 - val_accuracy: 0.5450\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.6595 - accuracy: 0.7312 - val_loss: 0.9135 - val_accuracy: 0.5450\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.6678 - accuracy: 0.7188 - val_loss: 0.9052 - val_accuracy: 0.5700\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.6402 - accuracy: 0.7287 - val_loss: 0.9117 - val_accuracy: 0.5650\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.6388 - accuracy: 0.7350 - val_loss: 0.9202 - val_accuracy: 0.5650\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.6312 - accuracy: 0.7212 - val_loss: 0.9066 - val_accuracy: 0.5800\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.6406 - accuracy: 0.7362 - val_loss: 0.9098 - val_accuracy: 0.5800\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.6043 - accuracy: 0.7487 - val_loss: 0.9159 - val_accuracy: 0.5650\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.6065 - accuracy: 0.7513 - val_loss: 0.9135 - val_accuracy: 0.5750\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.5580 - accuracy: 0.7713 - val_loss: 0.9241 - val_accuracy: 0.5700\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.6037 - accuracy: 0.7563 - val_loss: 0.9244 - val_accuracy: 0.5300\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.5790 - accuracy: 0.7575 - val_loss: 0.9418 - val_accuracy: 0.5350\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.5572 - accuracy: 0.7788 - val_loss: 0.9446 - val_accuracy: 0.5450\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.5527 - accuracy: 0.7713 - val_loss: 0.9565 - val_accuracy: 0.5300\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.5399 - accuracy: 0.7837 - val_loss: 0.9651 - val_accuracy: 0.5500\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.5256 - accuracy: 0.7987 - val_loss: 0.9771 - val_accuracy: 0.5550\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.5473 - accuracy: 0.7962 - val_loss: 0.9809 - val_accuracy: 0.5350\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.7799 - accuracy: 0.2625 - val_loss: 1.6255 - val_accuracy: 0.2250\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.4538 - accuracy: 0.3663 - val_loss: 1.5993 - val_accuracy: 0.2450\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.4227 - accuracy: 0.3388 - val_loss: 1.5305 - val_accuracy: 0.3100\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.3318 - accuracy: 0.3975 - val_loss: 1.4113 - val_accuracy: 0.3700\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 1.2284 - accuracy: 0.4487 - val_loss: 1.3359 - val_accuracy: 0.4150\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 117us/sample - loss: 1.2239 - accuracy: 0.4450 - val_loss: 1.2353 - val_accuracy: 0.4300\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.1924 - accuracy: 0.4325 - val_loss: 1.1871 - val_accuracy: 0.4700\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.1723 - accuracy: 0.4750 - val_loss: 1.1196 - val_accuracy: 0.4750\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 1.1621 - accuracy: 0.4550 - val_loss: 1.0884 - val_accuracy: 0.4950\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.0999 - accuracy: 0.4950 - val_loss: 1.0564 - val_accuracy: 0.5050\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.0605 - accuracy: 0.5000 - val_loss: 1.0528 - val_accuracy: 0.4900\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.0706 - accuracy: 0.5213 - val_loss: 1.0244 - val_accuracy: 0.5100\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 1.0269 - accuracy: 0.5362 - val_loss: 0.9923 - val_accuracy: 0.5150\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.0477 - accuracy: 0.5075 - val_loss: 0.9836 - val_accuracy: 0.5450\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.0314 - accuracy: 0.5312 - val_loss: 0.9758 - val_accuracy: 0.5050\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.0178 - accuracy: 0.5337 - val_loss: 0.9695 - val_accuracy: 0.5200\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.9923 - accuracy: 0.5512 - val_loss: 0.9664 - val_accuracy: 0.5150\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.9605 - accuracy: 0.5725 - val_loss: 0.9563 - val_accuracy: 0.5300\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.9475 - accuracy: 0.5975 - val_loss: 0.9597 - val_accuracy: 0.5450\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.9533 - accuracy: 0.5788 - val_loss: 0.9676 - val_accuracy: 0.5400\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.9192 - accuracy: 0.5863 - val_loss: 0.9624 - val_accuracy: 0.5450\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.9169 - accuracy: 0.5813 - val_loss: 0.9484 - val_accuracy: 0.5600\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.8722 - accuracy: 0.6263 - val_loss: 0.9376 - val_accuracy: 0.5750\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.8873 - accuracy: 0.6087 - val_loss: 0.9406 - val_accuracy: 0.5350\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.8800 - accuracy: 0.6250 - val_loss: 0.9354 - val_accuracy: 0.5200\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.8238 - accuracy: 0.6313 - val_loss: 0.9343 - val_accuracy: 0.5400\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.8356 - accuracy: 0.6488 - val_loss: 0.9312 - val_accuracy: 0.5500\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.8151 - accuracy: 0.6413 - val_loss: 0.9239 - val_accuracy: 0.5550\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.8129 - accuracy: 0.6475 - val_loss: 0.9240 - val_accuracy: 0.5650\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.8152 - accuracy: 0.6500 - val_loss: 0.9203 - val_accuracy: 0.5700\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.7600 - accuracy: 0.6662 - val_loss: 0.9339 - val_accuracy: 0.5450\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.7994 - accuracy: 0.6250 - val_loss: 0.9126 - val_accuracy: 0.5600\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.7459 - accuracy: 0.6675 - val_loss: 0.9043 - val_accuracy: 0.5600\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.7374 - accuracy: 0.6650 - val_loss: 0.9068 - val_accuracy: 0.5600\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.7330 - accuracy: 0.6737 - val_loss: 0.9102 - val_accuracy: 0.5600\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.7504 - accuracy: 0.6837 - val_loss: 0.9083 - val_accuracy: 0.5600\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.7278 - accuracy: 0.6687 - val_loss: 0.9045 - val_accuracy: 0.5800\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.7092 - accuracy: 0.6875 - val_loss: 0.9007 - val_accuracy: 0.5650\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.6926 - accuracy: 0.7025 - val_loss: 0.8988 - val_accuracy: 0.5650\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.6761 - accuracy: 0.7175 - val_loss: 0.8928 - val_accuracy: 0.5600\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.6963 - accuracy: 0.6925 - val_loss: 0.9066 - val_accuracy: 0.5500\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.6358 - accuracy: 0.7450 - val_loss: 0.8998 - val_accuracy: 0.5500\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.6514 - accuracy: 0.7312 - val_loss: 0.9024 - val_accuracy: 0.5950\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.6312 - accuracy: 0.7425 - val_loss: 0.8958 - val_accuracy: 0.5650\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.6309 - accuracy: 0.7337 - val_loss: 0.8877 - val_accuracy: 0.6000\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.6028 - accuracy: 0.7550 - val_loss: 0.9026 - val_accuracy: 0.5800\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.6155 - accuracy: 0.7475 - val_loss: 0.9042 - val_accuracy: 0.5650\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.5927 - accuracy: 0.7663 - val_loss: 0.8960 - val_accuracy: 0.5750\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.5969 - accuracy: 0.7538 - val_loss: 0.8959 - val_accuracy: 0.5650\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 0.6364 - accuracy: 0.7412 - val_loss: 0.9065 - val_accuracy: 0.5750\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "fnn_model_sq = small_FNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "fnn_model_tri = small_FNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "\n",
    "# hyperparams should be a tuple of: INIT_LR, EPOCHS, BS\n",
    "FNN_hyperparams = (0.01, 50, 32)\n",
    "H_sq_FNN, sq_FNN_model = train_model(fnn_model_sq, Xsq_train, ysq_train, Xsq_test, ysq_test, num_classes, input_shape, FNN_hyperparams)\n",
    "H_tri_FNN, tri_FNN_model = train_model(fnn_model_tri, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, FNN_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Train a convolutional neural network to do the classification, on both datasets. Make a table of your performance numbers for (a) and (b). \n",
    "Try to optimize the performance of your models and compare the result.\n",
    "\n",
    "solution to (b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(width, height, depth, num_classes, channels_first=False):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        \n",
    "        if channels_first == False:\n",
    "            inputShape = (height, width, depth)\n",
    "            chanDim = -1\n",
    "        elif channels_first == True:\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        model.add(Conv2D(32, (5, 5), activation='relu', input_shape=inputShape))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "                  \n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.3770 - accuracy: 0.3025 - val_loss: 1.3355 - val_accuracy: 0.3150\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 1.3090 - accuracy: 0.3625 - val_loss: 1.2403 - val_accuracy: 0.3950\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 1.2031 - accuracy: 0.4425 - val_loss: 1.1016 - val_accuracy: 0.5650\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 1.0361 - accuracy: 0.5288 - val_loss: 0.9425 - val_accuracy: 0.6100\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.8941 - accuracy: 0.5987 - val_loss: 0.9279 - val_accuracy: 0.4950\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.7485 - accuracy: 0.6725 - val_loss: 1.5481 - val_accuracy: 0.2300\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.7559 - accuracy: 0.6750 - val_loss: 0.6461 - val_accuracy: 0.7050\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.6225 - accuracy: 0.7200 - val_loss: 1.5007 - val_accuracy: 0.2400\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.5733 - accuracy: 0.7663 - val_loss: 0.5202 - val_accuracy: 0.7700\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.4232 - accuracy: 0.8388 - val_loss: 0.4119 - val_accuracy: 0.8200\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.7449 - accuracy: 0.6938 - val_loss: 0.5154 - val_accuracy: 0.7700\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.4059 - accuracy: 0.8413 - val_loss: 0.5811 - val_accuracy: 0.7200\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.4154 - accuracy: 0.8288 - val_loss: 0.4093 - val_accuracy: 0.8100\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.3238 - accuracy: 0.8775 - val_loss: 0.4835 - val_accuracy: 0.7750\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.4909 - accuracy: 0.7825 - val_loss: 0.3874 - val_accuracy: 0.8200\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.2930 - accuracy: 0.8850 - val_loss: 0.3188 - val_accuracy: 0.8800\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.3004 - accuracy: 0.8700 - val_loss: 0.2794 - val_accuracy: 0.8900\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.2976 - accuracy: 0.8800 - val_loss: 0.3329 - val_accuracy: 0.8500\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.2446 - accuracy: 0.9175 - val_loss: 0.2384 - val_accuracy: 0.9050\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.3145 - accuracy: 0.8775 - val_loss: 0.2809 - val_accuracy: 0.8900\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.2661 - accuracy: 0.9050 - val_loss: 0.3002 - val_accuracy: 0.8750\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.3178 - accuracy: 0.8763 - val_loss: 0.2264 - val_accuracy: 0.9100\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.2078 - accuracy: 0.9200 - val_loss: 0.2369 - val_accuracy: 0.9000\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.2127 - accuracy: 0.9225 - val_loss: 0.2912 - val_accuracy: 0.8700\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.1906 - accuracy: 0.9312 - val_loss: 0.3412 - val_accuracy: 0.8550\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.2312 - accuracy: 0.9025 - val_loss: 0.2847 - val_accuracy: 0.8800\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.1755 - accuracy: 0.9413 - val_loss: 0.1887 - val_accuracy: 0.9200\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1855 - accuracy: 0.9275 - val_loss: 0.2768 - val_accuracy: 0.8800\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1756 - accuracy: 0.9413 - val_loss: 0.6834 - val_accuracy: 0.7100\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 94us/sample - loss: 0.2123 - accuracy: 0.9187 - val_loss: 0.1768 - val_accuracy: 0.9250\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.2310 - accuracy: 0.9025 - val_loss: 0.2752 - val_accuracy: 0.8900\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1618 - accuracy: 0.9463 - val_loss: 0.1878 - val_accuracy: 0.9200\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.1406 - accuracy: 0.9575 - val_loss: 0.1668 - val_accuracy: 0.9350\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.2393 - accuracy: 0.9150 - val_loss: 0.1873 - val_accuracy: 0.9350\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1648 - accuracy: 0.9362 - val_loss: 0.2117 - val_accuracy: 0.9000\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1416 - accuracy: 0.9500 - val_loss: 0.1631 - val_accuracy: 0.9400\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1278 - accuracy: 0.9588 - val_loss: 0.1750 - val_accuracy: 0.9350\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1427 - accuracy: 0.9425 - val_loss: 0.1943 - val_accuracy: 0.9250\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1612 - accuracy: 0.9388 - val_loss: 0.2269 - val_accuracy: 0.9050\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1438 - accuracy: 0.9362 - val_loss: 0.1779 - val_accuracy: 0.9150\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1479 - accuracy: 0.9488 - val_loss: 0.2061 - val_accuracy: 0.9150\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1314 - accuracy: 0.9575 - val_loss: 0.1974 - val_accuracy: 0.9100\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1134 - accuracy: 0.9650 - val_loss: 0.1665 - val_accuracy: 0.9250\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.1189 - accuracy: 0.9575 - val_loss: 0.1917 - val_accuracy: 0.9050\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 93us/sample - loss: 0.1216 - accuracy: 0.9538 - val_loss: 0.4816 - val_accuracy: 0.7800\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 91us/sample - loss: 0.1803 - accuracy: 0.9275 - val_loss: 0.1393 - val_accuracy: 0.9500\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.1201 - accuracy: 0.9588 - val_loss: 0.1376 - val_accuracy: 0.9400\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.1197 - accuracy: 0.9638 - val_loss: 0.2409 - val_accuracy: 0.9050\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.1535 - accuracy: 0.9400 - val_loss: 0.1113 - val_accuracy: 0.9650\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 92us/sample - loss: 0.1544 - accuracy: 0.9388 - val_loss: 0.1383 - val_accuracy: 0.9600\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 0s 482us/sample - loss: 1.3895 - accuracy: 0.2600 - val_loss: 1.3311 - val_accuracy: 0.3550\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 1.3182 - accuracy: 0.3638 - val_loss: 1.2380 - val_accuracy: 0.4800\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 1.1696 - accuracy: 0.5150 - val_loss: 0.9851 - val_accuracy: 0.5150\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 1.0064 - accuracy: 0.5075 - val_loss: 0.9680 - val_accuracy: 0.4650\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.8583 - accuracy: 0.6012 - val_loss: 1.0624 - val_accuracy: 0.4500\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 82us/sample - loss: 0.7071 - accuracy: 0.6762 - val_loss: 0.9301 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.6910 - accuracy: 0.6875 - val_loss: 0.7662 - val_accuracy: 0.5600\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.7228 - accuracy: 0.6675 - val_loss: 0.5507 - val_accuracy: 0.7350\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 91us/sample - loss: 0.5773 - accuracy: 0.7387 - val_loss: 0.6272 - val_accuracy: 0.6900\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.4746 - accuracy: 0.8037 - val_loss: 0.4981 - val_accuracy: 0.7750\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.5948 - accuracy: 0.7212 - val_loss: 0.4055 - val_accuracy: 0.8650\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.4340 - accuracy: 0.8050 - val_loss: 0.4088 - val_accuracy: 0.8100\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.3778 - accuracy: 0.8462 - val_loss: 1.0321 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 92us/sample - loss: 0.5551 - accuracy: 0.7475 - val_loss: 0.3335 - val_accuracy: 0.9100\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.3872 - accuracy: 0.8250 - val_loss: 0.3009 - val_accuracy: 0.9100\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 93us/sample - loss: 0.4022 - accuracy: 0.8125 - val_loss: 0.4439 - val_accuracy: 0.7800\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 0.3601 - accuracy: 0.8375 - val_loss: 0.4125 - val_accuracy: 0.8250\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 0.3795 - accuracy: 0.8275 - val_loss: 0.4424 - val_accuracy: 0.8050\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.3460 - accuracy: 0.8525 - val_loss: 0.3977 - val_accuracy: 0.8450\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.3135 - accuracy: 0.8600 - val_loss: 0.3242 - val_accuracy: 0.8800\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.3170 - accuracy: 0.8725 - val_loss: 0.4565 - val_accuracy: 0.7850\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.2957 - accuracy: 0.8825 - val_loss: 0.3203 - val_accuracy: 0.8600\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.2540 - accuracy: 0.9025 - val_loss: 0.2755 - val_accuracy: 0.9100\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 0.3535 - accuracy: 0.8462 - val_loss: 0.2518 - val_accuracy: 0.9350\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.2567 - accuracy: 0.8963 - val_loss: 0.4094 - val_accuracy: 0.8000\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.2557 - accuracy: 0.8888 - val_loss: 0.3036 - val_accuracy: 0.8750\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.3090 - accuracy: 0.8637 - val_loss: 0.3539 - val_accuracy: 0.8350\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.2396 - accuracy: 0.9050 - val_loss: 0.2224 - val_accuracy: 0.9300\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.2184 - accuracy: 0.9162 - val_loss: 0.2324 - val_accuracy: 0.9200\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.2181 - accuracy: 0.9125 - val_loss: 0.3041 - val_accuracy: 0.8550\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 92us/sample - loss: 0.2554 - accuracy: 0.9025 - val_loss: 0.2036 - val_accuracy: 0.9400\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1976 - accuracy: 0.9212 - val_loss: 0.2021 - val_accuracy: 0.9350\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.3515 - accuracy: 0.8675 - val_loss: 0.1918 - val_accuracy: 0.9350\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1789 - accuracy: 0.9375 - val_loss: 0.2153 - val_accuracy: 0.9250\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1792 - accuracy: 0.9337 - val_loss: 0.1784 - val_accuracy: 0.9400\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1697 - accuracy: 0.9362 - val_loss: 0.1862 - val_accuracy: 0.9450\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.2077 - accuracy: 0.9212 - val_loss: 0.3349 - val_accuracy: 0.8350\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1847 - accuracy: 0.9250 - val_loss: 0.3627 - val_accuracy: 0.8750\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.1913 - accuracy: 0.9112 - val_loss: 0.1681 - val_accuracy: 0.9550\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1782 - accuracy: 0.9312 - val_loss: 0.1426 - val_accuracy: 0.9500\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.1479 - accuracy: 0.9463 - val_loss: 0.1764 - val_accuracy: 0.9450\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.2001 - accuracy: 0.9100 - val_loss: 0.2224 - val_accuracy: 0.9100\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.1459 - accuracy: 0.9463 - val_loss: 0.1429 - val_accuracy: 0.9550\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1449 - accuracy: 0.9513 - val_loss: 0.1344 - val_accuracy: 0.9550\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.1493 - accuracy: 0.9425 - val_loss: 0.1435 - val_accuracy: 0.9550\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.1338 - accuracy: 0.9475 - val_loss: 0.1261 - val_accuracy: 0.9550\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1147 - accuracy: 0.9650 - val_loss: 0.1296 - val_accuracy: 0.9550\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1157 - accuracy: 0.9613 - val_loss: 0.2211 - val_accuracy: 0.9250\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.2143 - accuracy: 0.9137 - val_loss: 0.1346 - val_accuracy: 0.9500\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1363 - accuracy: 0.9450 - val_loss: 0.1468 - val_accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "cnn_model_sq = SimpleCNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "cnn_model_tri = SimpleCNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "\n",
    "CNN_hyperparams = (0.01, 50, 64)\n",
    "H_sq_CNN, sq_CNN_model = train_model(cnn_model_sq, Xsq_train, ysq_train, Xsq_test, ysq_test, num_classes, input_shape, CNN_hyperparams)\n",
    "H_tri_CNN, tri_CNN_model = train_model(cnn_model_tri, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, CNN_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) We have provided a test set of 10 spins configurations for each of the two problems. Each of the spin configurations is not necessarily at the temperatures of the training sets. Calculate your best estimate of the temperatures of these spin configuration. Upload your results to Kaggle.\n",
    "[Hint: A direct fingerprint of temperature is the distribution of spin up\n",
    "and down, because you can image that the spins fluctuate more violently\n",
    "at higher temperature. Although the mothod you use in homework 2 can also work, you may be interested in trying to take distribution into account when you\n",
    "build the model to estimate temperature and see if you can make use of this extra information. This may help you win the\n",
    "kaggle. It is totally fine if you find that the information of distribution is not helpful. Note also that a CNN kind-of does this. One possibility is that you may want a CNN that captures enough distribution information.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsq_sim_data = []\\ntri_sim_data = []\\nsim_data_temp = []\\n\\nfor temp in temps_vec:    \\n    sq_ising_simu = Ising_sq(32, temp)\\n    tri_ising_simu = Ising_tri(32, temp)\\n    sq_img = sq_ising_simu.simulate()\\n    tri_img = tri_ising_simu.simulate()\\n    \\n    sq_sim_data.append(sq_img)\\n    tri_sim_data.append(tri_img)\\n    \\ncwd = str(os.getcwd())\\nsim_data_path_2 = cwd+\"/sim_data_2.npy\"\\nsim_data_temp_path_2 = cwd+\"/sim_data_temp_2.npy\"\\nsim_data_tot_lv_mean_path = cwd+\"/sim_data_tot_lv_mean.npy\"\\n\\nnp.save(sq_sim_data_path, np.asarray(sq_sim_data))\\nnp.save(tri_sim_data_path, np.asarray(tri_sim_data))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = str(os.getcwd())\n",
    "sq_sim_data_path = cwd+\"/sq_sim_data.npy\"\n",
    "tri_sim_data_path = cwd+\"/tri_sim_data.npy\"\n",
    "\n",
    "temps_vec = np.linspace(0.01,15,num=1500)\n",
    "\n",
    "\"\"\"\n",
    "sq_sim_data = []\n",
    "tri_sim_data = []\n",
    "sim_data_temp = []\n",
    "\n",
    "for temp in temps_vec:    \n",
    "    sq_ising_simu = Ising_sq(32, temp)\n",
    "    tri_ising_simu = Ising_tri(32, temp)\n",
    "    sq_img = sq_ising_simu.simulate()\n",
    "    tri_img = tri_ising_simu.simulate()\n",
    "    \n",
    "    sq_sim_data.append(sq_img)\n",
    "    tri_sim_data.append(tri_img)\n",
    "    \n",
    "cwd = str(os.getcwd())\n",
    "sim_data_path_2 = cwd+\"/sim_data_2.npy\"\n",
    "sim_data_temp_path_2 = cwd+\"/sim_data_temp_2.npy\"\n",
    "sim_data_tot_lv_mean_path = cwd+\"/sim_data_tot_lv_mean.npy\"\n",
    "\n",
    "np.save(sq_sim_data_path, np.asarray(sq_sim_data))\n",
    "np.save(tri_sim_data_path, np.asarray(tri_sim_data))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data ready to be embedded\n",
    "sq_sim_data = np.load(sq_sim_data_path).reshape(1500,32,32,-1)\n",
    "tri_sim_data = np.load(tri_sim_data_path).reshape(1500,32,32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_sim_data, tri_sim_data = np.repeat(sq_sim_data, 5, 1), np.repeat(tri_sim_data, 5, 1)\n",
    "sq_sim_data, tri_sim_data = np.repeat(sq_sim_data, 5, 2), np.repeat(tri_sim_data, 5, 2)\n",
    "sq_sim_data, tri_sim_data = np.repeat(sq_sim_data, 3, 3), np.repeat(tri_sim_data, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 160, 160, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_sim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pre-trained MobileNetV2 \n",
    "with tf.device('/CPU:0'):\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=(160,160,3),\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet',\n",
    "                                                   classes=4)\n",
    "    base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedded data\n",
    "global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "sq_sim_data_emb = global_avg_layer(base_model.predict(sq_sim_data))\n",
    "sq_sim_data_emb = sq_sim_data_emb.numpy()/sq_sim_data_emb.numpy().max()\n",
    "\n",
    "tri_sim_data_emb = global_avg_layer(base_model.predict(tri_sim_data))\n",
    "tri_sim_data_emb = tri_sim_data_emb.numpy()/tri_sim_data_emb.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test splits\n",
    "sq_x_train, sq_x_test, sq_y_train, sq_y_test = train_test_split(sq_sim_data_emb, temps_vec, test_size=0.2, random_state=0)\n",
    "tri_x_train, tri_x_test, tri_y_train, tri_y_test = train_test_split(tri_sim_data_emb, temps_vec, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class RegressionHead:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(channels_first=False):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(612, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "1200/1200 [==============================] - 0s 350us/sample - loss: 54.4427 - MeanSquaredError: 54.4427 - val_loss: 46.3159 - val_MeanSquaredError: 46.3159\n",
      "Epoch 2/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 41.7279 - MeanSquaredError: 41.7279 - val_loss: 37.5618 - val_MeanSquaredError: 37.5618\n",
      "Epoch 3/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 32.5192 - MeanSquaredError: 32.5192 - val_loss: 38.8629 - val_MeanSquaredError: 38.8629\n",
      "Epoch 4/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 24.9197 - MeanSquaredError: 24.9197 - val_loss: 21.5944 - val_MeanSquaredError: 21.5944\n",
      "Epoch 5/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 19.4002 - MeanSquaredError: 19.4002 - val_loss: 20.9087 - val_MeanSquaredError: 20.9087\n",
      "Epoch 6/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 15.3215 - MeanSquaredError: 15.3215 - val_loss: 25.6445 - val_MeanSquaredError: 25.6445\n",
      "Epoch 7/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 12.1364 - MeanSquaredError: 12.1364 - val_loss: 7.6828 - val_MeanSquaredError: 7.6828\n",
      "Epoch 8/50\n",
      "1200/1200 [==============================] - 0s 84us/sample - loss: 9.7471 - MeanSquaredError: 9.7471 - val_loss: 8.6831 - val_MeanSquaredError: 8.6831\n",
      "Epoch 9/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 8.1761 - MeanSquaredError: 8.1761 - val_loss: 7.7585 - val_MeanSquaredError: 7.7585\n",
      "Epoch 10/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 6.5758 - MeanSquaredError: 6.5758 - val_loss: 8.8566 - val_MeanSquaredError: 8.8566\n",
      "Epoch 11/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 6.2853 - MeanSquaredError: 6.2853 - val_loss: 5.0463 - val_MeanSquaredError: 5.0463\n",
      "Epoch 12/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 5.6197 - MeanSquaredError: 5.6197 - val_loss: 16.7878 - val_MeanSquaredError: 16.7878\n",
      "Epoch 13/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 5.3902 - MeanSquaredError: 5.3902 - val_loss: 3.0885 - val_MeanSquaredError: 3.0885\n",
      "Epoch 14/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 5.2067 - MeanSquaredError: 5.2067 - val_loss: 6.0822 - val_MeanSquaredError: 6.0822\n",
      "Epoch 15/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.9504 - MeanSquaredError: 4.9504 - val_loss: 3.3416 - val_MeanSquaredError: 3.3416\n",
      "Epoch 16/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 5.0043 - MeanSquaredError: 5.0043 - val_loss: 3.8482 - val_MeanSquaredError: 3.8482\n",
      "Epoch 17/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 4.5960 - MeanSquaredError: 4.5960 - val_loss: 2.9986 - val_MeanSquaredError: 2.9986\n",
      "Epoch 18/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.4555 - MeanSquaredError: 4.4555 - val_loss: 3.0674 - val_MeanSquaredError: 3.0674\n",
      "Epoch 19/50\n",
      "1200/1200 [==============================] - 0s 88us/sample - loss: 4.6882 - MeanSquaredError: 4.6882 - val_loss: 4.5615 - val_MeanSquaredError: 4.5615\n",
      "Epoch 20/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.2702 - MeanSquaredError: 4.2702 - val_loss: 3.1897 - val_MeanSquaredError: 3.1897\n",
      "Epoch 21/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 4.5780 - MeanSquaredError: 4.5780 - val_loss: 4.0537 - val_MeanSquaredError: 4.0537\n",
      "Epoch 22/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 4.1303 - MeanSquaredError: 4.1303 - val_loss: 4.8368 - val_MeanSquaredError: 4.8368\n",
      "Epoch 23/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 4.5475 - MeanSquaredError: 4.5475 - val_loss: 4.0260 - val_MeanSquaredError: 4.0260\n",
      "Epoch 24/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 4.2798 - MeanSquaredError: 4.2798 - val_loss: 3.7100 - val_MeanSquaredError: 3.7100\n",
      "Epoch 25/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.2037 - MeanSquaredError: 4.2037 - val_loss: 3.2985 - val_MeanSquaredError: 3.2985\n",
      "Epoch 26/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.9964 - MeanSquaredError: 3.9964 - val_loss: 3.4559 - val_MeanSquaredError: 3.4559\n",
      "Epoch 27/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.3887 - MeanSquaredError: 4.3887 - val_loss: 3.1799 - val_MeanSquaredError: 3.1799\n",
      "Epoch 28/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 4.0200 - MeanSquaredError: 4.0200 - val_loss: 6.1322 - val_MeanSquaredError: 6.1322\n",
      "Epoch 29/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.9884 - MeanSquaredError: 3.9884 - val_loss: 4.5786 - val_MeanSquaredError: 4.5786\n",
      "Epoch 30/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 4.1888 - MeanSquaredError: 4.1888 - val_loss: 3.0150 - val_MeanSquaredError: 3.0150\n",
      "Epoch 31/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.4326 - MeanSquaredError: 4.4326 - val_loss: 3.3004 - val_MeanSquaredError: 3.3004\n",
      "Epoch 32/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.8169 - MeanSquaredError: 3.8169 - val_loss: 3.3583 - val_MeanSquaredError: 3.3583\n",
      "Epoch 33/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.7289 - MeanSquaredError: 3.7289 - val_loss: 2.9462 - val_MeanSquaredError: 2.9462\n",
      "Epoch 34/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.2170 - MeanSquaredError: 4.2170 - val_loss: 2.9873 - val_MeanSquaredError: 2.9873\n",
      "Epoch 35/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.7656 - MeanSquaredError: 3.7656 - val_loss: 3.1313 - val_MeanSquaredError: 3.1313\n",
      "Epoch 36/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.9046 - MeanSquaredError: 3.9046 - val_loss: 3.3998 - val_MeanSquaredError: 3.3998\n",
      "Epoch 37/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.9719 - MeanSquaredError: 3.9719 - val_loss: 4.3465 - val_MeanSquaredError: 4.3465\n",
      "Epoch 38/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 4.0969 - MeanSquaredError: 4.0969 - val_loss: 3.2919 - val_MeanSquaredError: 3.2919\n",
      "Epoch 39/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 3.8935 - MeanSquaredError: 3.8935 - val_loss: 3.9759 - val_MeanSquaredError: 3.9759\n",
      "Epoch 40/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.9664 - MeanSquaredError: 3.9664 - val_loss: 3.8760 - val_MeanSquaredError: 3.8760\n",
      "Epoch 41/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 4.0228 - MeanSquaredError: 4.0228 - val_loss: 3.3863 - val_MeanSquaredError: 3.3863\n",
      "Epoch 42/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.0622 - MeanSquaredError: 4.0622 - val_loss: 3.2492 - val_MeanSquaredError: 3.2492\n",
      "Epoch 43/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.8889 - MeanSquaredError: 3.8889 - val_loss: 3.0808 - val_MeanSquaredError: 3.0808\n",
      "Epoch 44/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.7666 - MeanSquaredError: 3.7666 - val_loss: 3.1205 - val_MeanSquaredError: 3.1205\n",
      "Epoch 45/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.9190 - MeanSquaredError: 3.9190 - val_loss: 3.0130 - val_MeanSquaredError: 3.0130\n",
      "Epoch 46/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.8884 - MeanSquaredError: 3.8884 - val_loss: 3.0324 - val_MeanSquaredError: 3.0324\n",
      "Epoch 47/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.8802 - MeanSquaredError: 3.8802 - val_loss: 2.9898 - val_MeanSquaredError: 2.9898\n",
      "Epoch 48/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.7073 - MeanSquaredError: 3.7073 - val_loss: 3.4481 - val_MeanSquaredError: 3.4481\n",
      "Epoch 49/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.7014 - MeanSquaredError: 3.7014 - val_loss: 3.0192 - val_MeanSquaredError: 3.0192\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.9319 - MeanSquaredError: 3.9319 - val_loss: 2.9962 - val_MeanSquaredError: 2.9962\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.01# learning rate\n",
    "EPOCHS = 50 # number of epochs\n",
    "BS = 32 # batch size\n",
    "OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "LOSS = keras.losses.MeanSquaredError()\n",
    "\n",
    "sq_reg_model = RegressionHead.build()\n",
    "sq_reg_model.compile(optimizer=OPT, loss=LOSS, metrics=['MeanSquaredError'])\n",
    "sq_reg_H = sq_reg_model.fit(sq_x_train, sq_y_train, validation_data=(sq_x_test, sq_y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "1200/1200 [==============================] - 0s 361us/sample - loss: 53.4960 - MeanSquaredError: 53.4960 - val_loss: 51.0035 - val_MeanSquaredError: 51.0035\n",
      "Epoch 2/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 40.3129 - MeanSquaredError: 40.3129 - val_loss: 53.8225 - val_MeanSquaredError: 53.8225\n",
      "Epoch 3/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 31.2308 - MeanSquaredError: 31.2308 - val_loss: 38.1518 - val_MeanSquaredError: 38.1518\n",
      "Epoch 4/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 24.0846 - MeanSquaredError: 24.0846 - val_loss: 25.2302 - val_MeanSquaredError: 25.2302\n",
      "Epoch 5/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 18.9162 - MeanSquaredError: 18.9162 - val_loss: 27.5664 - val_MeanSquaredError: 27.5664\n",
      "Epoch 6/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 14.8738 - MeanSquaredError: 14.8738 - val_loss: 13.5919 - val_MeanSquaredError: 13.5919\n",
      "Epoch 7/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 11.8166 - MeanSquaredError: 11.8166 - val_loss: 12.4864 - val_MeanSquaredError: 12.4864\n",
      "Epoch 8/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 9.2564 - MeanSquaredError: 9.2564 - val_loss: 9.7956 - val_MeanSquaredError: 9.7956\n",
      "Epoch 9/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 7.5141 - MeanSquaredError: 7.5141 - val_loss: 5.7672 - val_MeanSquaredError: 5.7672\n",
      "Epoch 10/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 6.4926 - MeanSquaredError: 6.4926 - val_loss: 2.5480 - val_MeanSquaredError: 2.5480\n",
      "Epoch 11/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 5.8711 - MeanSquaredError: 5.8711 - val_loss: 1.9937 - val_MeanSquaredError: 1.9937\n",
      "Epoch 12/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.8742 - MeanSquaredError: 4.8742 - val_loss: 3.2444 - val_MeanSquaredError: 3.2444\n",
      "Epoch 13/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.3897 - MeanSquaredError: 4.3897 - val_loss: 2.7988 - val_MeanSquaredError: 2.7988\n",
      "Epoch 14/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 4.4897 - MeanSquaredError: 4.4897 - val_loss: 2.5287 - val_MeanSquaredError: 2.5287\n",
      "Epoch 15/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.8526 - MeanSquaredError: 3.8526 - val_loss: 2.4621 - val_MeanSquaredError: 2.4621\n",
      "Epoch 16/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 4.0802 - MeanSquaredError: 4.0802 - val_loss: 1.8598 - val_MeanSquaredError: 1.8598\n",
      "Epoch 17/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 3.4820 - MeanSquaredError: 3.4820 - val_loss: 1.9328 - val_MeanSquaredError: 1.9328\n",
      "Epoch 18/50\n",
      "1200/1200 [==============================] - 0s 82us/sample - loss: 3.5104 - MeanSquaredError: 3.5104 - val_loss: 2.7006 - val_MeanSquaredError: 2.7006\n",
      "Epoch 19/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 3.6178 - MeanSquaredError: 3.6178 - val_loss: 2.0860 - val_MeanSquaredError: 2.0860\n",
      "Epoch 20/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 3.5281 - MeanSquaredError: 3.5281 - val_loss: 1.9763 - val_MeanSquaredError: 1.9763\n",
      "Epoch 21/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 3.3349 - MeanSquaredError: 3.3349 - val_loss: 2.3829 - val_MeanSquaredError: 2.3829\n",
      "Epoch 22/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.6369 - MeanSquaredError: 3.6369 - val_loss: 1.8268 - val_MeanSquaredError: 1.8268\n",
      "Epoch 23/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.4683 - MeanSquaredError: 3.4683 - val_loss: 2.0950 - val_MeanSquaredError: 2.0950\n",
      "Epoch 24/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 3.1605 - MeanSquaredError: 3.1605 - val_loss: 2.3099 - val_MeanSquaredError: 2.3099\n",
      "Epoch 25/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 3.4230 - MeanSquaredError: 3.4230 - val_loss: 2.6384 - val_MeanSquaredError: 2.6384\n",
      "Epoch 26/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.3632 - MeanSquaredError: 3.3632 - val_loss: 4.0214 - val_MeanSquaredError: 4.0214\n",
      "Epoch 27/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.5843 - MeanSquaredError: 3.5843 - val_loss: 1.7514 - val_MeanSquaredError: 1.7514\n",
      "Epoch 28/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 3.4722 - MeanSquaredError: 3.4722 - val_loss: 2.7046 - val_MeanSquaredError: 2.7046\n",
      "Epoch 29/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.5716 - MeanSquaredError: 3.5716 - val_loss: 1.7713 - val_MeanSquaredError: 1.7713\n",
      "Epoch 30/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 3.4987 - MeanSquaredError: 3.4987 - val_loss: 1.7112 - val_MeanSquaredError: 1.7112\n",
      "Epoch 31/50\n",
      "1200/1200 [==============================] - 0s 82us/sample - loss: 3.2671 - MeanSquaredError: 3.2671 - val_loss: 2.5977 - val_MeanSquaredError: 2.5977\n",
      "Epoch 32/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 3.4650 - MeanSquaredError: 3.4650 - val_loss: 1.6795 - val_MeanSquaredError: 1.6795\n",
      "Epoch 33/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.0528 - MeanSquaredError: 3.0528 - val_loss: 1.9219 - val_MeanSquaredError: 1.9219\n",
      "Epoch 34/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.3466 - MeanSquaredError: 3.3466 - val_loss: 1.6940 - val_MeanSquaredError: 1.6940\n",
      "Epoch 35/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.1066 - MeanSquaredError: 3.1066 - val_loss: 2.2872 - val_MeanSquaredError: 2.2872\n",
      "Epoch 36/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 3.2774 - MeanSquaredError: 3.2774 - val_loss: 1.8905 - val_MeanSquaredError: 1.8905\n",
      "Epoch 37/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 3.0489 - MeanSquaredError: 3.0489 - val_loss: 1.6110 - val_MeanSquaredError: 1.6110\n",
      "Epoch 38/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.0056 - MeanSquaredError: 3.0056 - val_loss: 1.6350 - val_MeanSquaredError: 1.6350\n",
      "Epoch 39/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.0184 - MeanSquaredError: 3.0184 - val_loss: 1.7263 - val_MeanSquaredError: 1.7263\n",
      "Epoch 40/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 2.9425 - MeanSquaredError: 2.9425 - val_loss: 1.7702 - val_MeanSquaredError: 1.7702\n",
      "Epoch 41/50\n",
      "1200/1200 [==============================] - 0s 91us/sample - loss: 3.1732 - MeanSquaredError: 3.1732 - val_loss: 1.8394 - val_MeanSquaredError: 1.8394\n",
      "Epoch 42/50\n",
      "1200/1200 [==============================] - 0s 87us/sample - loss: 2.8757 - MeanSquaredError: 2.8757 - val_loss: 1.6256 - val_MeanSquaredError: 1.6256\n",
      "Epoch 43/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.2038 - MeanSquaredError: 3.2038 - val_loss: 1.9365 - val_MeanSquaredError: 1.9365\n",
      "Epoch 44/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.6132 - MeanSquaredError: 3.6132 - val_loss: 2.1534 - val_MeanSquaredError: 2.1534\n",
      "Epoch 45/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 2.6852 - MeanSquaredError: 2.6852 - val_loss: 1.8344 - val_MeanSquaredError: 1.8344\n",
      "Epoch 46/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 2.9328 - MeanSquaredError: 2.9328 - val_loss: 1.8819 - val_MeanSquaredError: 1.8819\n",
      "Epoch 47/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.1374 - MeanSquaredError: 3.1374 - val_loss: 1.5956 - val_MeanSquaredError: 1.5956\n",
      "Epoch 48/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 2.9741 - MeanSquaredError: 2.9741 - val_loss: 2.1125 - val_MeanSquaredError: 2.1125\n",
      "Epoch 49/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 2.9964 - MeanSquaredError: 2.9964 - val_loss: 1.8497 - val_MeanSquaredError: 1.8497\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.0290 - MeanSquaredError: 3.0290 - val_loss: 1.6168 - val_MeanSquaredError: 1.6168\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.01# learning rate\n",
    "EPOCHS = 50 # number of epochs\n",
    "BS = 32 # batch size\n",
    "OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "LOSS = keras.losses.MeanSquaredError()\n",
    "\n",
    "tri_reg_model = RegressionHead.build()\n",
    "tri_reg_model.compile(optimizer=OPT, loss=LOSS, metrics=['MeanSquaredError'])\n",
    "tri_reg_H = tri_reg_model.fit(tri_x_train, tri_y_train, validation_data=(tri_x_test, tri_y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_import(dir):\n",
    "    temp = []\n",
    "    for i in range(len(os.listdir(dir+'/'))):\n",
    "        temp.append(np.loadtxt((dir+'/')+str(i).zfill(3), delimiter=','))\n",
    "    return np.array(temp)\n",
    "\n",
    "def T10_import(dir):\n",
    "    temp = []\n",
    "    for i in range(0,10):\n",
    "        for j in range(0,10):\n",
    "            temp.append(np.loadtxt((dir+'/T0'+str(i)+'#')+str(j).zfill(2), delimiter=','))\n",
    "    return np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_t10_img = T10_import(cwd+'/triangle_10T').reshape(100,32,32,-1)\n",
    "square_t10_img = T10_import(cwd+'/square_10T').reshape(100,32,32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_t10_data, tri_t10_data = np.repeat(square_t10_img, 5, 1), np.repeat(tri_t10_img, 5, 1)\n",
    "square_t10_data, tri_t10_data = np.repeat(square_t10_data, 5, 2), np.repeat(tri_t10_data, 5, 2)\n",
    "square_t10_data, tri_t10_data = np.repeat(square_t10_data, 3, 3), np.repeat(tri_t10_data, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32, 32, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square_t10_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_t10_data_emb = global_avg_layer(base_model.predict(square_t10_data))\n",
    "square_t10_data_emb = square_t10_data_emb.numpy()/square_t10_data_emb.numpy().max()\n",
    "\n",
    "tri_t10_data_emb = global_avg_layer(base_model.predict(tri_t10_data))\n",
    "tri_t10_data_emb = tri_t10_data_emb.numpy()/tri_t10_data_emb.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.6601267 ],\n",
       "       [5.2140226 ],\n",
       "       [6.00695   ],\n",
       "       [5.011929  ],\n",
       "       [4.138114  ],\n",
       "       [4.750653  ],\n",
       "       [6.067241  ],\n",
       "       [4.378121  ],\n",
       "       [4.9823513 ],\n",
       "       [4.6362586 ],\n",
       "       [4.02925   ],\n",
       "       [3.3392138 ],\n",
       "       [4.7100606 ],\n",
       "       [3.914775  ],\n",
       "       [4.248056  ],\n",
       "       [3.382172  ],\n",
       "       [3.2406697 ],\n",
       "       [3.9389744 ],\n",
       "       [3.1664176 ],\n",
       "       [4.5941057 ],\n",
       "       [4.496542  ],\n",
       "       [4.328278  ],\n",
       "       [4.00007   ],\n",
       "       [5.01751   ],\n",
       "       [4.306549  ],\n",
       "       [4.09298   ],\n",
       "       [4.9168344 ],\n",
       "       [4.5878954 ],\n",
       "       [4.1558933 ],\n",
       "       [4.2527957 ],\n",
       "       [4.22478   ],\n",
       "       [4.769401  ],\n",
       "       [3.7035217 ],\n",
       "       [4.065798  ],\n",
       "       [4.195915  ],\n",
       "       [3.9596224 ],\n",
       "       [3.9306293 ],\n",
       "       [4.050641  ],\n",
       "       [4.3738756 ],\n",
       "       [3.6717181 ],\n",
       "       [4.711441  ],\n",
       "       [4.6812267 ],\n",
       "       [4.0168576 ],\n",
       "       [4.095689  ],\n",
       "       [4.2343287 ],\n",
       "       [4.2828107 ],\n",
       "       [4.172458  ],\n",
       "       [4.990421  ],\n",
       "       [4.0858955 ],\n",
       "       [4.895487  ],\n",
       "       [5.230031  ],\n",
       "       [6.344587  ],\n",
       "       [5.5798755 ],\n",
       "       [4.423042  ],\n",
       "       [4.9698615 ],\n",
       "       [5.3891206 ],\n",
       "       [4.9618716 ],\n",
       "       [5.1351676 ],\n",
       "       [4.897898  ],\n",
       "       [4.9205966 ],\n",
       "       [3.9900298 ],\n",
       "       [4.8158207 ],\n",
       "       [4.1855774 ],\n",
       "       [4.2149806 ],\n",
       "       [4.6270847 ],\n",
       "       [4.4177895 ],\n",
       "       [4.230074  ],\n",
       "       [4.809138  ],\n",
       "       [4.638615  ],\n",
       "       [4.1288066 ],\n",
       "       [1.6369854 ],\n",
       "       [1.5562027 ],\n",
       "       [1.4175817 ],\n",
       "       [0.98975587],\n",
       "       [1.9062428 ],\n",
       "       [2.5905905 ],\n",
       "       [2.0650458 ],\n",
       "       [2.4689355 ],\n",
       "       [2.1003652 ],\n",
       "       [2.318708  ],\n",
       "       [5.2595086 ],\n",
       "       [5.6956644 ],\n",
       "       [5.149246  ],\n",
       "       [5.8421435 ],\n",
       "       [6.2016115 ],\n",
       "       [6.1880383 ],\n",
       "       [5.0449514 ],\n",
       "       [5.4291673 ],\n",
       "       [5.4448586 ],\n",
       "       [4.248476  ],\n",
       "       [2.604601  ],\n",
       "       [3.0734448 ],\n",
       "       [3.0865917 ],\n",
       "       [2.9564137 ],\n",
       "       [2.863184  ],\n",
       "       [2.1287932 ],\n",
       "       [2.7813945 ],\n",
       "       [2.9327855 ],\n",
       "       [2.699916  ],\n",
       "       [3.5799026 ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_reg_model.predict(tri_t10_data_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.65633900e-16, 5.51906005e-08, 8.69663358e-02, 9.13033545e-01],\n",
       "       [1.51543430e-19, 1.59032631e-11, 4.33319074e-04, 9.99566615e-01],\n",
       "       [2.51819547e-21, 1.06018200e-12, 9.55850192e-05, 9.99904394e-01],\n",
       "       [4.15801195e-17, 1.48430834e-09, 4.47533885e-03, 9.95524645e-01],\n",
       "       [2.39680969e-13, 8.61846218e-07, 3.34095895e-01, 6.65903270e-01],\n",
       "       [5.23150172e-19, 7.62910290e-11, 1.10261468e-03, 9.98897314e-01],\n",
       "       [1.13710011e-19, 1.06397356e-11, 6.70520414e-04, 9.99329448e-01],\n",
       "       [3.12760166e-17, 8.93384122e-09, 2.69313660e-02, 9.73068655e-01],\n",
       "       [1.68044439e-20, 1.35628774e-11, 3.78648721e-04, 9.99621272e-01],\n",
       "       [1.79048455e-18, 1.53093399e-10, 2.12737708e-03, 9.97872591e-01],\n",
       "       [9.51254964e-10, 2.04438320e-03, 9.97881472e-01, 7.40964388e-05],\n",
       "       [4.68881357e-11, 3.83405160e-04, 9.99500513e-01, 1.16107396e-04],\n",
       "       [1.98213191e-12, 1.26059003e-05, 8.72999251e-01, 1.26988113e-01],\n",
       "       [3.48983754e-12, 3.11443982e-05, 9.92305577e-01, 7.66335893e-03],\n",
       "       [2.40370640e-11, 3.93721530e-05, 9.71868813e-01, 2.80917455e-02],\n",
       "       [2.62479677e-10, 1.95326618e-04, 9.99329686e-01, 4.74942324e-04],\n",
       "       [1.31565321e-06, 1.16581187e-01, 8.83405924e-01, 1.16072615e-05],\n",
       "       [5.29298862e-11, 1.28369968e-04, 9.95063722e-01, 4.80791181e-03],\n",
       "       [2.70124332e-07, 4.79436256e-02, 9.52045619e-01, 1.05670597e-05],\n",
       "       [2.59562261e-10, 5.98661718e-04, 9.99251425e-01, 1.49849395e-04],\n",
       "       [7.19259382e-15, 2.71323444e-07, 3.01714897e-01, 6.98284864e-01],\n",
       "       [1.68865996e-14, 3.23234531e-07, 2.29005963e-01, 7.70993650e-01],\n",
       "       [4.01588781e-15, 1.51707226e-07, 1.94018424e-01, 8.05981338e-01],\n",
       "       [9.56026235e-18, 3.91143506e-10, 4.85697482e-03, 9.95143056e-01],\n",
       "       [5.85400977e-13, 5.56606165e-06, 6.35989010e-01, 3.64005446e-01],\n",
       "       [1.55671634e-12, 1.81940577e-05, 9.96669471e-01, 3.31235421e-03],\n",
       "       [3.39571174e-17, 5.87324367e-09, 2.04528701e-02, 9.79547083e-01],\n",
       "       [8.32768438e-14, 1.63567358e-06, 4.81426865e-01, 5.18571556e-01],\n",
       "       [1.63581874e-11, 6.49295034e-05, 9.98943508e-01, 9.91640380e-04],\n",
       "       [6.65686581e-13, 1.31403840e-06, 5.81311643e-01, 4.18687075e-01],\n",
       "       [9.49260948e-11, 3.56397533e-04, 9.99546468e-01, 9.71551635e-05],\n",
       "       [4.35937848e-11, 6.86963685e-05, 9.91551280e-01, 8.38008430e-03],\n",
       "       [7.99182682e-12, 2.76771734e-05, 9.07473683e-01, 9.24985930e-02],\n",
       "       [9.22053336e-11, 2.26664371e-04, 9.99116480e-01, 6.56801567e-04],\n",
       "       [6.61172173e-10, 5.07682969e-04, 9.98779953e-01, 7.12381559e-04],\n",
       "       [4.04368927e-10, 2.99393345e-04, 9.99329925e-01, 3.70736467e-04],\n",
       "       [4.24123764e-10, 9.52801842e-04, 9.99025583e-01, 2.16447479e-05],\n",
       "       [7.94890582e-07, 1.66331813e-01, 8.33660603e-01, 6.71388034e-06],\n",
       "       [8.13825896e-10, 9.17542842e-04, 9.98830378e-01, 2.52093974e-04],\n",
       "       [1.33389841e-08, 9.97015741e-03, 9.89977062e-01, 5.27775701e-05],\n",
       "       [1.06800013e-14, 4.81568691e-07, 2.76243061e-01, 7.23756433e-01],\n",
       "       [3.48443435e-15, 2.18575465e-07, 1.52989849e-01, 8.47009957e-01],\n",
       "       [3.14773685e-12, 1.76579906e-05, 9.45055664e-01, 5.49266636e-02],\n",
       "       [2.47431437e-12, 2.19227295e-05, 9.39181745e-01, 6.07962906e-02],\n",
       "       [5.74909857e-13, 6.52525569e-06, 8.01371753e-01, 1.98621735e-01],\n",
       "       [6.61067287e-15, 7.35637542e-08, 8.56449530e-02, 9.14354980e-01],\n",
       "       [6.54721740e-15, 1.00807434e-07, 8.36038813e-02, 9.16395962e-01],\n",
       "       [1.01915147e-13, 2.74204194e-06, 9.56709623e-01, 4.32876498e-02],\n",
       "       [7.47146160e-13, 6.47382058e-06, 7.03456581e-01, 2.96536922e-01],\n",
       "       [2.57979046e-14, 3.44254033e-07, 1.18446991e-01, 8.81552696e-01],\n",
       "       [6.65999559e-19, 6.61096733e-11, 1.12970767e-03, 9.98870313e-01],\n",
       "       [2.31932457e-21, 8.66632612e-13, 7.14316848e-05, 9.99928594e-01],\n",
       "       [8.93936570e-19, 2.55877070e-10, 5.32362564e-03, 9.94676352e-01],\n",
       "       [1.17125208e-18, 2.02164382e-10, 2.84788152e-03, 9.97152090e-01],\n",
       "       [2.44530810e-20, 7.76403334e-12, 1.70653759e-04, 9.99829292e-01],\n",
       "       [9.44076500e-21, 3.99829822e-12, 1.95366534e-04, 9.99804676e-01],\n",
       "       [2.04939852e-19, 3.99841028e-11, 7.47326820e-04, 9.99252617e-01],\n",
       "       [7.34427981e-14, 8.85128884e-07, 2.17231110e-01, 7.82768011e-01],\n",
       "       [8.82365736e-17, 1.76732562e-09, 2.96657509e-03, 9.97033477e-01],\n",
       "       [5.33500104e-18, 2.14621187e-09, 1.31488759e-02, 9.86851156e-01],\n",
       "       [3.60409836e-13, 6.68459279e-06, 9.49291289e-01, 5.07019907e-02],\n",
       "       [3.59876652e-14, 2.69618909e-07, 3.76445681e-01, 6.23553991e-01],\n",
       "       [1.71057728e-14, 1.61846373e-07, 2.85432011e-01, 7.14567840e-01],\n",
       "       [1.81552025e-13, 2.44538933e-06, 5.49332142e-01, 4.50665414e-01],\n",
       "       [1.40214135e-18, 3.66517927e-10, 2.55174492e-03, 9.97448325e-01],\n",
       "       [2.99862766e-16, 9.21971743e-09, 2.89022364e-02, 9.71097827e-01],\n",
       "       [2.93500987e-11, 4.48478604e-05, 9.83827233e-01, 1.61279794e-02],\n",
       "       [4.66486561e-16, 4.48715092e-08, 1.48391068e-01, 8.51608932e-01],\n",
       "       [1.22000720e-17, 1.06759823e-09, 6.59133913e-03, 9.93408680e-01],\n",
       "       [6.99750045e-14, 6.59697321e-07, 2.66296148e-01, 7.33703136e-01],\n",
       "       [9.24506307e-01, 7.54883364e-02, 5.26408394e-06, 4.85628794e-11],\n",
       "       [9.91569877e-01, 8.43007118e-03, 2.07158131e-08, 3.01100934e-14],\n",
       "       [9.51371610e-01, 4.86279503e-02, 4.64710098e-07, 1.43208362e-12],\n",
       "       [9.98114228e-01, 1.88581471e-03, 1.13808685e-09, 8.39752858e-16],\n",
       "       [8.17963481e-01, 1.82029605e-01, 6.88662476e-06, 1.33995072e-11],\n",
       "       [9.80094016e-01, 1.99059118e-02, 1.08983876e-07, 3.60378204e-13],\n",
       "       [9.79518831e-01, 2.04809923e-02, 2.28640729e-07, 8.54228960e-13],\n",
       "       [9.54985201e-01, 4.50136140e-02, 1.29768500e-06, 4.82594173e-12],\n",
       "       [9.74720359e-01, 2.52794493e-02, 1.74470770e-07, 4.35889280e-13],\n",
       "       [8.35169613e-01, 1.64826453e-01, 4.02808200e-06, 1.50441118e-11],\n",
       "       [1.27366820e-18, 2.64708561e-10, 1.54244155e-03, 9.98457551e-01],\n",
       "       [4.57331493e-18, 1.44965384e-09, 7.93433096e-03, 9.92065728e-01],\n",
       "       [2.13387410e-18, 3.64086122e-10, 4.63694008e-03, 9.95362997e-01],\n",
       "       [3.59409728e-18, 2.06935663e-10, 1.60335761e-03, 9.98396575e-01],\n",
       "       [3.72754852e-22, 2.68431165e-13, 6.44799438e-05, 9.99935508e-01],\n",
       "       [2.90443389e-21, 9.34816677e-13, 6.17697151e-05, 9.99938250e-01],\n",
       "       [8.77019946e-15, 2.73234434e-07, 3.46542716e-01, 6.53456986e-01],\n",
       "       [2.52890155e-19, 4.12698313e-11, 9.04336455e-04, 9.99095678e-01],\n",
       "       [3.70444333e-19, 7.29504443e-11, 1.32753991e-03, 9.98672485e-01],\n",
       "       [1.29778693e-19, 4.14970454e-11, 9.29886184e-04, 9.99070108e-01],\n",
       "       [4.85254526e-02, 9.50101316e-01, 1.37321558e-03, 6.50119825e-09],\n",
       "       [3.44777033e-02, 9.65206206e-01, 3.16037942e-04, 7.21280313e-10],\n",
       "       [9.47840691e-01, 5.21589592e-02, 3.19687416e-07, 2.17793611e-13],\n",
       "       [1.66722927e-02, 9.82908607e-01, 4.19098214e-04, 1.40134371e-09],\n",
       "       [7.21673906e-01, 2.78301835e-01, 2.42262176e-05, 2.41404979e-10],\n",
       "       [2.58044392e-01, 7.41739154e-01, 2.16449713e-04, 3.36816935e-10],\n",
       "       [7.89434612e-01, 2.10551932e-01, 1.34605762e-05, 3.83588786e-11],\n",
       "       [7.97704160e-01, 2.02290952e-01, 4.91897345e-06, 3.75058535e-12],\n",
       "       [8.08736444e-01, 1.91253439e-01, 1.01061623e-05, 3.09665245e-11],\n",
       "       [5.95142424e-01, 4.04781729e-01, 7.58152964e-05, 7.39037664e-10]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_CNN_model.predict_proba(tri_t10_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) *Transfer Learning*.  \n",
    "As we emphasize in class, one can freeze the training of the bottom layers of a network and retrain the top part of the network to adopt to a new situation. Use your CNN that you trained on the squarelattice data to do transfer learning on the triangular lattice data.  How does the performance compare to that of the direct methods?  Add the performance numbers for transfer learning in your table from Part (a). Note that the training time and number of training examples needed for transfer learning is far less than that for the direct  optimization. For  example,  is  50  triangle  example  sufficient  for the re-training process?  Use your transfer learning result to predict the transition temperature of triangle lattice Ising model, as demonstrated in this [Nature Physics](https://www-nature-com.ezp-prod1.hul.harvard.edu/articles/nphys4035.pdf) publication.\n",
    "\n",
    "As a guideline, you may like to just change the last `Dense` layer with `softmax` activation when you do the transfer learning. Other choices are also OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:51:16.394160Z",
     "start_time": "2020-02-24T16:51:16.390887Z"
    }
   },
   "source": [
    "Solution to (d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = sq_CNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = base_model.layers[0:5]\n",
    "trainable_layers = [\n",
    "     Flatten(),\n",
    "     Dropout(0.25),\n",
    "     Dense(32, activation='relu'),\n",
    "     Dense(num_classes, activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_model = keras.Sequential(base_layers+trainable_layers)\n",
    "trans_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "hyperparams = (0.01, 25, 32)\n",
    "H_trans, trained_trans_model = train_model(trans_model, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
