{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:46:46.494051Z",
     "start_time": "2020-02-24T16:46:45.817790Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import gc\n",
    "from numba import cuda\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:46:46.494051Z",
     "start_time": "2020-02-24T16:46:45.817790Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# This is how the triagle lattice data is generated. You may find it helpful to generate some \n",
    "# of your own data\n",
    "class Ising_tri():\n",
    "    ''' Simulating the Ising model '''  \n",
    "    def __init__(self, size, temp):\n",
    "        self.temp = temp\n",
    "        self.N = int(size)\n",
    "    ## monte carlo moves\n",
    "    \n",
    "    def mcmove(self, config, N, beta):\n",
    "        ''' This is to execute the monte carlo moves using \n",
    "        Metropolis algorithm such that detailed\n",
    "        balance condition is satisified'''\n",
    "        for i in range(N):\n",
    "            for j in range(N):            \n",
    "                    a = np.random.randint(0, N) # select a row\n",
    "                    b = np.random.randint(0, N) # select a column\n",
    "                    s =  config[a, b] # current state at (a, b)\n",
    "                    if a%2:\n",
    "                        nb = config[(a+1)%N,b] +config[(a+1)%N,(b+1)%N] + config[a,(b+1)%N] + \\\n",
    "                        config[(a-1)%N,b] + config[(a-1)%N,(b+1)%N] + config[a,(b-1)%N]\n",
    "                    else:\n",
    "                        nb = config[(a+1)%N,b] +config[(a+1)%N,(b-1)%N] + config[a,(b+1)%N] + \\\n",
    "                        config[(a-1)%N,b] + config[(a-1)%N,(b-1)%N] + config[a,(b-1)%N]\n",
    "                    \n",
    "                    \n",
    "                    cost = 2*s*nb\n",
    "                    if cost < 0:\t\n",
    "                        s *= -1\n",
    "                    elif rand() < np.exp(-cost*beta):\n",
    "                        s *= -1\n",
    "                    config[a, b] = s\n",
    "        return config\n",
    "    \n",
    "    def simulate(self):   \n",
    "        ''' This module simulates the Ising model'''\n",
    "        config = 2*np.random.randint(2, size=(self.N,self.N))-1   \n",
    "        msrmnt = 81\n",
    "        for i in range(msrmnt):\n",
    "            self.mcmove(config, self.N, 1.0/self.temp)\n",
    "        return config\n",
    "\n",
    "class Ising_sq():\n",
    "    ''' Simulating the Ising model '''  \n",
    "    def __init__(self, size, temp):\n",
    "        self.temp = temp\n",
    "        self.N = int(size)\n",
    "    ## monte carlo moves\n",
    "\n",
    "    def mcmove(self, config, N, beta):\n",
    "        ''' This is to execute the monte carlo moves using \n",
    "        Metropolis algorithm such that detailed\n",
    "        balance condition is satisified'''\n",
    "        for i in range(N):\n",
    "            for j in range(N):            \n",
    "                    a = np.random.randint(0, N) # select a row\n",
    "                    b = np.random.randint(0, N) # select a column\n",
    "                    s =  config[a, b] # current state at (a, b)\n",
    "                    # periodic boundary condition imposed\n",
    "                    nb = config[(a+1)%N,b] + config[a,(b+1)%N] + config[(a-1)%N,b] + config[a,(b-1)%N]\n",
    "                    cost = 2*s*nb\n",
    "                    if cost < 0:\n",
    "                        s *= -1\n",
    "                    elif rand() < np.exp(-cost*beta):\n",
    "                        s *= -1\n",
    "                    config[a, b] = s\n",
    "        return config\n",
    "    \n",
    "    def simulate(self):   \n",
    "        ''' This module simulates the Ising model'''\n",
    "        config = 2*np.random.randint(2, size=(self.N,self.N))-1   \n",
    "        times = 100\n",
    "        for i in range(times):\n",
    "            self.mcmove(config, self.N, 1.0/self.temp)\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import 4-temp data for square and triangular lattices as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:30:37.313572Z",
     "start_time": "2020-02-20T22:30:34.531268Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "nx, ny = 32, 32\n",
    "\n",
    "Xsq = np.ndarray((4*N,nx,ny,1))\n",
    "ysq = np.ndarray(4*N)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    Xsq[i + 0*N] = np.loadtxt(\"./square_T1/square_T1/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 0*N] = 0\n",
    "    Xsq[i + 1*N] = np.loadtxt(\"./square_T2/square_T2/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 1*N] = 1\n",
    "    Xsq[i + 2*N] = np.loadtxt(\"./square_T3/square_T3/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 2*N] = 2\n",
    "    Xsq[i + 3*N] = np.loadtxt(\"./square_T4/square_T4/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 3*N] = 3\n",
    "\n",
    "Xsq_train, Xsq_test, ysq_train, ysq_test = train_test_split(Xsq, ysq, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:31:02.786262Z",
     "start_time": "2020-02-20T22:31:00.698414Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "nx, ny = 32, 32\n",
    "\n",
    "Xtri = np.ndarray((4*N,nx,ny,1))\n",
    "ytri = np.ndarray(4*N)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    Xtri[i + 0*N] = np.loadtxt(\"./triangle_T1/triangle_T1/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 0*N] = 0\n",
    "    Xtri[i + 1*N] = np.loadtxt(\"./triangle_T2/triangle_T2/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 1*N] = 1\n",
    "    Xtri[i + 2*N] = np.loadtxt(\"./triangle_T3/triangle_T3/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 2*N] = 2\n",
    "    Xtri[i + 3*N] = np.loadtxt(\"./triangle_T4/triangle_T4/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 3*N] = 3\n",
    "\n",
    "Xtri_train, Xtri_test, ytri_train, ytri_test = train_test_split(Xtri, ytri, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you know the shape of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:31:05.870456Z",
     "start_time": "2020-02-20T22:31:05.864585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:\n",
      "(800, 32, 32, 1) (800, 32, 32, 1)\n",
      "(800,) (800,)\n",
      "Shape of test data:\n",
      "(200, 32, 32, 1) (200, 32, 32, 1)\n",
      "(200,) (200,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data:\")\n",
    "print(Xsq_train.shape, Xtri_train.shape)\n",
    "print(ysq_train.shape, ytri_train.shape)\n",
    "print(\"Shape of test data:\")\n",
    "print(Xsq_test.shape, Xtri_test.shape)\n",
    "print(ysq_test.shape, ytri_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Train a fully connected neural network to do the classification on both datasets. Then, train  a  convolutional  neural  network  to  do  the  classification,  on  both datasets.   Make  a  table  of  your  performance  numbers  for  both  models  and  upload  these  numbers.   This,  together  with  your code,  should be uploaded to the course website when you turn in your homework.\n",
    "\n",
    "The temperatures for square lattice are $T = 1.5, 2.1, 2.4, 3.5$. $T = 2.5, 3.2, 3.8, 5$ for triangle lattice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class small_FNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(width, height, depth, num_classes, channels_first=False):\n",
    "        model = Sequential()\n",
    "\n",
    "        if channels_first == False:\n",
    "            inputShape = (height, width, depth)\n",
    "            chanDim = -1\n",
    "        elif channels_first == True:\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(256,  activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(128,  activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_model, train_data, train_lbls, test_data, \n",
    "                test_lbls, num_classes, input_shape, hyperparams):\n",
    "    # Ensure data is shaped properly, assumes channels last set up\n",
    "    x_train = train_data\n",
    "    x_test = test_data\n",
    "    \n",
    "    # Create categorical labels\n",
    "    y_train = keras.utils.to_categorical(train_lbls, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_lbls, num_classes)\n",
    "     \n",
    "    # Set hyperparameters\n",
    "    INIT_LR = hyperparams[0]# learning rate\n",
    "    EPOCHS = hyperparams[1] # number of epochs\n",
    "    BS = hyperparams[2] # batch size\n",
    "    OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "    \n",
    "    model = input_model\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=OPT, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    H = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)\n",
    "    \n",
    "    return H, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.8019 - accuracy: 0.2763 - val_loss: 1.3965 - val_accuracy: 0.3150\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.5497 - accuracy: 0.3275 - val_loss: 1.3893 - val_accuracy: 0.3000\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 1.4317 - accuracy: 0.3550 - val_loss: 1.4009 - val_accuracy: 0.3200\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 1.3357 - accuracy: 0.4025 - val_loss: 1.3634 - val_accuracy: 0.3150\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 1.2563 - accuracy: 0.4350 - val_loss: 1.3089 - val_accuracy: 0.3550\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.2139 - accuracy: 0.4525 - val_loss: 1.2589 - val_accuracy: 0.3650\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 1.1843 - accuracy: 0.4850 - val_loss: 1.2248 - val_accuracy: 0.3850\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 1.1575 - accuracy: 0.4963 - val_loss: 1.1903 - val_accuracy: 0.4150\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 1.1021 - accuracy: 0.5113 - val_loss: 1.1394 - val_accuracy: 0.4500\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.0682 - accuracy: 0.5300 - val_loss: 1.1259 - val_accuracy: 0.4800\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.0130 - accuracy: 0.5550 - val_loss: 1.0776 - val_accuracy: 0.4650\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.0041 - accuracy: 0.5412 - val_loss: 1.0454 - val_accuracy: 0.4850\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.9948 - accuracy: 0.5562 - val_loss: 1.0197 - val_accuracy: 0.4950\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.9475 - accuracy: 0.5950 - val_loss: 0.9965 - val_accuracy: 0.5250\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.9506 - accuracy: 0.5750 - val_loss: 0.9713 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 0.9047 - accuracy: 0.6037 - val_loss: 0.9713 - val_accuracy: 0.5100\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.8917 - accuracy: 0.6112 - val_loss: 0.9669 - val_accuracy: 0.5150\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 0.9199 - accuracy: 0.5888 - val_loss: 0.9594 - val_accuracy: 0.5050\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.8390 - accuracy: 0.6338 - val_loss: 0.9497 - val_accuracy: 0.5400\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.8356 - accuracy: 0.6463 - val_loss: 0.9349 - val_accuracy: 0.5350\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 0.7973 - accuracy: 0.6538 - val_loss: 0.9373 - val_accuracy: 0.5150\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.8159 - accuracy: 0.6513 - val_loss: 0.9401 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 0.7923 - accuracy: 0.6562 - val_loss: 0.9448 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.7842 - accuracy: 0.6600 - val_loss: 0.9267 - val_accuracy: 0.5200\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.7760 - accuracy: 0.6575 - val_loss: 0.9187 - val_accuracy: 0.5100\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.7757 - accuracy: 0.6800 - val_loss: 0.9192 - val_accuracy: 0.5200\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 0.7751 - accuracy: 0.6488 - val_loss: 0.9249 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 0.7221 - accuracy: 0.6862 - val_loss: 0.9140 - val_accuracy: 0.5150\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 140us/sample - loss: 0.6743 - accuracy: 0.7250 - val_loss: 0.9112 - val_accuracy: 0.5200\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 0.7152 - accuracy: 0.6913 - val_loss: 0.9181 - val_accuracy: 0.5300\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 0.7041 - accuracy: 0.7063 - val_loss: 0.9073 - val_accuracy: 0.5250\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 0.6946 - accuracy: 0.6988 - val_loss: 0.9072 - val_accuracy: 0.5250\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 132us/sample - loss: 0.6819 - accuracy: 0.7275 - val_loss: 0.9004 - val_accuracy: 0.5500\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 165us/sample - loss: 0.6678 - accuracy: 0.7138 - val_loss: 0.9035 - val_accuracy: 0.5300\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 0.6776 - accuracy: 0.7138 - val_loss: 0.8982 - val_accuracy: 0.5400\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 180us/sample - loss: 0.6307 - accuracy: 0.7350 - val_loss: 0.8854 - val_accuracy: 0.5500\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 174us/sample - loss: 0.6560 - accuracy: 0.7275 - val_loss: 0.8947 - val_accuracy: 0.5350\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 184us/sample - loss: 0.5842 - accuracy: 0.7487 - val_loss: 0.8955 - val_accuracy: 0.5400\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 0.6096 - accuracy: 0.7600 - val_loss: 0.8885 - val_accuracy: 0.5750\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 0.6050 - accuracy: 0.7375 - val_loss: 0.9025 - val_accuracy: 0.5650\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 0.6614 - accuracy: 0.7262 - val_loss: 0.8951 - val_accuracy: 0.5450\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 0.6005 - accuracy: 0.7550 - val_loss: 0.9313 - val_accuracy: 0.5450\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 126us/sample - loss: 0.5889 - accuracy: 0.7588 - val_loss: 0.9356 - val_accuracy: 0.5250\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 0.5585 - accuracy: 0.7763 - val_loss: 0.9245 - val_accuracy: 0.5300\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 95us/sample - loss: 0.5945 - accuracy: 0.7425 - val_loss: 0.9056 - val_accuracy: 0.5400\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.5732 - accuracy: 0.7663 - val_loss: 0.9120 - val_accuracy: 0.5450\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 0.5615 - accuracy: 0.7750 - val_loss: 0.9273 - val_accuracy: 0.5500\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.5409 - accuracy: 0.7950 - val_loss: 0.9247 - val_accuracy: 0.5450\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.5166 - accuracy: 0.8012 - val_loss: 0.9161 - val_accuracy: 0.5450\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.5409 - accuracy: 0.7675 - val_loss: 0.9210 - val_accuracy: 0.5600\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.5909 - accuracy: 0.2937 - val_loss: 1.3946 - val_accuracy: 0.2650\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.4140 - accuracy: 0.3363 - val_loss: 1.3545 - val_accuracy: 0.2950\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 135us/sample - loss: 1.3056 - accuracy: 0.4175 - val_loss: 1.3236 - val_accuracy: 0.3200\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 1.3345 - accuracy: 0.3713 - val_loss: 1.2760 - val_accuracy: 0.3650\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 1.2602 - accuracy: 0.3862 - val_loss: 1.2479 - val_accuracy: 0.3850\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 158us/sample - loss: 1.2487 - accuracy: 0.3837 - val_loss: 1.2139 - val_accuracy: 0.4100\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 179us/sample - loss: 1.1880 - accuracy: 0.4288 - val_loss: 1.1756 - val_accuracy: 0.4600\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 155us/sample - loss: 1.1586 - accuracy: 0.4613 - val_loss: 1.1501 - val_accuracy: 0.4950\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 1.0675 - accuracy: 0.5163 - val_loss: 1.1197 - val_accuracy: 0.5150\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 143us/sample - loss: 1.1313 - accuracy: 0.4625 - val_loss: 1.1074 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 1.0596 - accuracy: 0.5075 - val_loss: 1.0745 - val_accuracy: 0.5550\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 1.0559 - accuracy: 0.5050 - val_loss: 1.0640 - val_accuracy: 0.5450\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.0172 - accuracy: 0.5138 - val_loss: 1.0486 - val_accuracy: 0.5400\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 1.0196 - accuracy: 0.4950 - val_loss: 1.0296 - val_accuracy: 0.5700\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 0.9953 - accuracy: 0.5250 - val_loss: 1.0154 - val_accuracy: 0.5750\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 204us/sample - loss: 0.9583 - accuracy: 0.5412 - val_loss: 1.0020 - val_accuracy: 0.5450\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 202us/sample - loss: 0.9648 - accuracy: 0.5163 - val_loss: 0.9921 - val_accuracy: 0.5550\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 203us/sample - loss: 0.9571 - accuracy: 0.5512 - val_loss: 0.9931 - val_accuracy: 0.5250\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 206us/sample - loss: 0.9148 - accuracy: 0.5863 - val_loss: 0.9855 - val_accuracy: 0.5150\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 240us/sample - loss: 0.9028 - accuracy: 0.5838 - val_loss: 0.9806 - val_accuracy: 0.5150\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 240us/sample - loss: 0.8926 - accuracy: 0.5788 - val_loss: 0.9777 - val_accuracy: 0.4900\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 0.8657 - accuracy: 0.5950 - val_loss: 0.9762 - val_accuracy: 0.5150\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 0.8652 - accuracy: 0.5987 - val_loss: 0.9732 - val_accuracy: 0.5200\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 0.8572 - accuracy: 0.5838 - val_loss: 0.9674 - val_accuracy: 0.5250\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 0.8273 - accuracy: 0.6363 - val_loss: 0.9521 - val_accuracy: 0.5200\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 95us/sample - loss: 0.8239 - accuracy: 0.6263 - val_loss: 0.9478 - val_accuracy: 0.5400\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.8212 - accuracy: 0.6237 - val_loss: 0.9477 - val_accuracy: 0.5500\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 0.8031 - accuracy: 0.6225 - val_loss: 0.9423 - val_accuracy: 0.5250\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 147us/sample - loss: 0.7860 - accuracy: 0.6562 - val_loss: 0.9286 - val_accuracy: 0.5600\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 0.7778 - accuracy: 0.6325 - val_loss: 0.9390 - val_accuracy: 0.5150\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 194us/sample - loss: 0.7678 - accuracy: 0.6350 - val_loss: 0.9328 - val_accuracy: 0.5400\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 0.7495 - accuracy: 0.6288 - val_loss: 0.9529 - val_accuracy: 0.5250\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 163us/sample - loss: 0.7339 - accuracy: 0.6612 - val_loss: 0.9360 - val_accuracy: 0.5250\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 161us/sample - loss: 0.7180 - accuracy: 0.6825 - val_loss: 0.9397 - val_accuracy: 0.5300\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 166us/sample - loss: 0.7198 - accuracy: 0.6725 - val_loss: 0.9305 - val_accuracy: 0.5350\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 193us/sample - loss: 0.6965 - accuracy: 0.6787 - val_loss: 0.9357 - val_accuracy: 0.5400\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 226us/sample - loss: 0.6969 - accuracy: 0.6875 - val_loss: 0.9388 - val_accuracy: 0.5250\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 231us/sample - loss: 0.6747 - accuracy: 0.7075 - val_loss: 0.9442 - val_accuracy: 0.5250\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 186us/sample - loss: 0.6631 - accuracy: 0.7125 - val_loss: 0.9444 - val_accuracy: 0.5400\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 182us/sample - loss: 0.6210 - accuracy: 0.7450 - val_loss: 0.9462 - val_accuracy: 0.5250\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 159us/sample - loss: 0.6116 - accuracy: 0.7588 - val_loss: 0.9329 - val_accuracy: 0.5450\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 0.6267 - accuracy: 0.7250 - val_loss: 0.9374 - val_accuracy: 0.5350\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 152us/sample - loss: 0.6266 - accuracy: 0.7150 - val_loss: 0.9531 - val_accuracy: 0.5150\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 0.5761 - accuracy: 0.7675 - val_loss: 0.9519 - val_accuracy: 0.5050\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 151us/sample - loss: 0.5746 - accuracy: 0.7412 - val_loss: 0.9382 - val_accuracy: 0.5100\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 142us/sample - loss: 0.5954 - accuracy: 0.7487 - val_loss: 0.9402 - val_accuracy: 0.5100\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 0.5853 - accuracy: 0.7450 - val_loss: 0.9454 - val_accuracy: 0.5150\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 0.5781 - accuracy: 0.7500 - val_loss: 0.9500 - val_accuracy: 0.5050\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 197us/sample - loss: 0.5533 - accuracy: 0.7475 - val_loss: 0.9488 - val_accuracy: 0.5200\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 198us/sample - loss: 0.5514 - accuracy: 0.7538 - val_loss: 0.9521 - val_accuracy: 0.5250\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "fnn_model_sq = small_FNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "fnn_model_tri = small_FNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "\n",
    "# hyperparams should be a tuple of: INIT_LR, EPOCHS, BS\n",
    "FNN_hyperparams = (0.01, 50, 32)\n",
    "H_sq_FNN, sq_FNN_model = train_model(fnn_model_sq, Xsq_train, ysq_train, Xsq_test, ysq_test, num_classes, input_shape, FNN_hyperparams)\n",
    "H_tri_FNN, tri_FNN_model = train_model(fnn_model_tri, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, FNN_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Train a convolutional neural network to do the classification, on both datasets. Make a table of your performance numbers for (a) and (b). \n",
    "Try to optimize the performance of your models and compare the result.\n",
    "\n",
    "solution to (b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(width, height, depth, num_classes, channels_first=False):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        \n",
    "        if channels_first == False:\n",
    "            inputShape = (height, width, depth)\n",
    "            chanDim = -1\n",
    "        elif channels_first == True:\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        model.add(Conv2D(32, (5, 5), activation='relu', input_shape=inputShape))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "                  \n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.3825 - accuracy: 0.2625 - val_loss: 1.3227 - val_accuracy: 0.2900\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 73us/sample - loss: 1.3326 - accuracy: 0.3113 - val_loss: 1.2643 - val_accuracy: 0.3900\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 1.2112 - accuracy: 0.4700 - val_loss: 1.0898 - val_accuracy: 0.5150\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.9718 - accuracy: 0.6012 - val_loss: 0.8724 - val_accuracy: 0.6100\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 71us/sample - loss: 0.8864 - accuracy: 0.6012 - val_loss: 0.7543 - val_accuracy: 0.6400\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.7727 - accuracy: 0.6325 - val_loss: 0.5539 - val_accuracy: 0.8100\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.7192 - accuracy: 0.6562 - val_loss: 0.5361 - val_accuracy: 0.7700\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.4982 - accuracy: 0.7975 - val_loss: 0.8148 - val_accuracy: 0.5850\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 92us/sample - loss: 0.4921 - accuracy: 0.7987 - val_loss: 0.9216 - val_accuracy: 0.5300\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.5247 - accuracy: 0.7600 - val_loss: 0.4505 - val_accuracy: 0.7900\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.3952 - accuracy: 0.8450 - val_loss: 0.4527 - val_accuracy: 0.7850\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.6336 - accuracy: 0.7163 - val_loss: 0.3847 - val_accuracy: 0.8450\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.3424 - accuracy: 0.8662 - val_loss: 0.3532 - val_accuracy: 0.8550\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.3195 - accuracy: 0.8925 - val_loss: 0.3262 - val_accuracy: 0.8250\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 72us/sample - loss: 0.2985 - accuracy: 0.8850 - val_loss: 0.3277 - val_accuracy: 0.8550\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.3195 - accuracy: 0.8725 - val_loss: 0.3901 - val_accuracy: 0.8050\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.2568 - accuracy: 0.9100 - val_loss: 0.3322 - val_accuracy: 0.8500\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.2581 - accuracy: 0.8963 - val_loss: 0.4050 - val_accuracy: 0.8250\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.2655 - accuracy: 0.9000 - val_loss: 0.3561 - val_accuracy: 0.8350\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.3613 - accuracy: 0.8537 - val_loss: 1.3510 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.3673 - accuracy: 0.8637 - val_loss: 0.2652 - val_accuracy: 0.8950\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.2249 - accuracy: 0.9125 - val_loss: 0.2650 - val_accuracy: 0.8800\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.2248 - accuracy: 0.9162 - val_loss: 0.2899 - val_accuracy: 0.8850\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.2233 - accuracy: 0.9200 - val_loss: 0.2527 - val_accuracy: 0.8800\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 94us/sample - loss: 0.2107 - accuracy: 0.9175 - val_loss: 0.2285 - val_accuracy: 0.9050\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.2109 - accuracy: 0.9150 - val_loss: 0.3377 - val_accuracy: 0.8650\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.1905 - accuracy: 0.9325 - val_loss: 0.2035 - val_accuracy: 0.9300\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.1943 - accuracy: 0.9300 - val_loss: 0.2154 - val_accuracy: 0.9050\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.1698 - accuracy: 0.9400 - val_loss: 0.2113 - val_accuracy: 0.9100\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1734 - accuracy: 0.9375 - val_loss: 0.2712 - val_accuracy: 0.8800\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 94us/sample - loss: 0.1653 - accuracy: 0.9438 - val_loss: 0.2051 - val_accuracy: 0.9100\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 0.1805 - accuracy: 0.9300 - val_loss: 0.2087 - val_accuracy: 0.9150\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 0.1920 - accuracy: 0.9275 - val_loss: 0.2279 - val_accuracy: 0.9250\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 0.1999 - accuracy: 0.9250 - val_loss: 0.1879 - val_accuracy: 0.9150\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 128us/sample - loss: 0.1416 - accuracy: 0.9550 - val_loss: 0.2469 - val_accuracy: 0.8900\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 0.1527 - accuracy: 0.9463 - val_loss: 0.2026 - val_accuracy: 0.9050\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 0.1502 - accuracy: 0.9475 - val_loss: 0.1652 - val_accuracy: 0.9400\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 0.1558 - accuracy: 0.9450 - val_loss: 0.3401 - val_accuracy: 0.8450\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.1440 - accuracy: 0.9450 - val_loss: 0.1832 - val_accuracy: 0.9050\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.1397 - accuracy: 0.9500 - val_loss: 0.1466 - val_accuracy: 0.9350\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 73us/sample - loss: 0.1188 - accuracy: 0.9563 - val_loss: 0.2150 - val_accuracy: 0.9100\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.1534 - accuracy: 0.9475 - val_loss: 0.1598 - val_accuracy: 0.9250\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 91us/sample - loss: 0.1539 - accuracy: 0.9475 - val_loss: 0.2309 - val_accuracy: 0.8900\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.1253 - accuracy: 0.9613 - val_loss: 0.1543 - val_accuracy: 0.9150\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.1365 - accuracy: 0.9475 - val_loss: 0.5929 - val_accuracy: 0.7600\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 94us/sample - loss: 0.1332 - accuracy: 0.9475 - val_loss: 0.6328 - val_accuracy: 0.7550\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.1945 - accuracy: 0.9300 - val_loss: 0.2251 - val_accuracy: 0.9100\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.1392 - accuracy: 0.9500 - val_loss: 0.1289 - val_accuracy: 0.9300\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.1221 - accuracy: 0.9488 - val_loss: 0.1267 - val_accuracy: 0.9550\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 0.1115 - accuracy: 0.9688 - val_loss: 0.1171 - val_accuracy: 0.9400\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 0s 470us/sample - loss: 1.3526 - accuracy: 0.2875 - val_loss: 1.2628 - val_accuracy: 0.4800\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 70us/sample - loss: 1.2228 - accuracy: 0.4375 - val_loss: 1.0744 - val_accuracy: 0.7100\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 1.0082 - accuracy: 0.5575 - val_loss: 1.3164 - val_accuracy: 0.1900\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.8988 - accuracy: 0.5938 - val_loss: 0.7026 - val_accuracy: 0.7900\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 144us/sample - loss: 0.8114 - accuracy: 0.6250 - val_loss: 1.0334 - val_accuracy: 0.4450\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 0.6728 - accuracy: 0.7063 - val_loss: 0.5464 - val_accuracy: 0.7950\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 150us/sample - loss: 0.6813 - accuracy: 0.6888 - val_loss: 0.5103 - val_accuracy: 0.7850\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 145us/sample - loss: 0.6171 - accuracy: 0.7063 - val_loss: 0.7029 - val_accuracy: 0.6300\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 94us/sample - loss: 0.5225 - accuracy: 0.7788 - val_loss: 0.4956 - val_accuracy: 0.7900\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.4716 - accuracy: 0.7937 - val_loss: 0.4009 - val_accuracy: 0.8550\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 93us/sample - loss: 0.5178 - accuracy: 0.7638 - val_loss: 0.7486 - val_accuracy: 0.6200\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.5196 - accuracy: 0.7638 - val_loss: 0.4097 - val_accuracy: 0.8250\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.3532 - accuracy: 0.8612 - val_loss: 0.3605 - val_accuracy: 0.8700\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.4737 - accuracy: 0.7775 - val_loss: 0.6655 - val_accuracy: 0.6700\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.3661 - accuracy: 0.8475 - val_loss: 0.3831 - val_accuracy: 0.8250\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.3989 - accuracy: 0.8188 - val_loss: 0.2889 - val_accuracy: 0.9000\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 94us/sample - loss: 0.3110 - accuracy: 0.8800 - val_loss: 0.2821 - val_accuracy: 0.9200\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 0.3239 - accuracy: 0.8687 - val_loss: 0.6848 - val_accuracy: 0.6850\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.4951 - accuracy: 0.7725 - val_loss: 0.3954 - val_accuracy: 0.8250\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 0.2952 - accuracy: 0.8788 - val_loss: 0.2870 - val_accuracy: 0.8650\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 0.3390 - accuracy: 0.8413 - val_loss: 0.4449 - val_accuracy: 0.8000\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.4121 - accuracy: 0.81 - 0s 87us/sample - loss: 0.2709 - accuracy: 0.8913 - val_loss: 0.2211 - val_accuracy: 0.9350\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.2480 - accuracy: 0.9013 - val_loss: 0.4705 - val_accuracy: 0.8100\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 0.2598 - accuracy: 0.8925 - val_loss: 0.2286 - val_accuracy: 0.9250\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 0.2760 - accuracy: 0.8888 - val_loss: 0.2977 - val_accuracy: 0.8900\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 0.2589 - accuracy: 0.8988 - val_loss: 0.2505 - val_accuracy: 0.9100\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 0.3417 - accuracy: 0.8600 - val_loss: 0.3893 - val_accuracy: 0.8950\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 0.2452 - accuracy: 0.9000 - val_loss: 0.2853 - val_accuracy: 0.8900\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 139us/sample - loss: 0.2611 - accuracy: 0.8825 - val_loss: 0.2121 - val_accuracy: 0.9350\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 0.2096 - accuracy: 0.9212 - val_loss: 0.2119 - val_accuracy: 0.9300\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 137us/sample - loss: 0.1942 - accuracy: 0.9262 - val_loss: 0.1757 - val_accuracy: 0.9350\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 0.3054 - accuracy: 0.8700 - val_loss: 0.1789 - val_accuracy: 0.9450\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.1823 - accuracy: 0.9350 - val_loss: 0.4665 - val_accuracy: 0.7850\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.2454 - accuracy: 0.8963 - val_loss: 0.1614 - val_accuracy: 0.9450\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.1678 - accuracy: 0.9300 - val_loss: 0.1525 - val_accuracy: 0.9650\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 0.1972 - accuracy: 0.9275 - val_loss: 0.2168 - val_accuracy: 0.9500\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.1921 - accuracy: 0.9262 - val_loss: 0.1377 - val_accuracy: 0.9650\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 0.1626 - accuracy: 0.9413 - val_loss: 0.3511 - val_accuracy: 0.8800\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 0.1973 - accuracy: 0.9287 - val_loss: 0.1593 - val_accuracy: 0.9550\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 123us/sample - loss: 0.1505 - accuracy: 0.9438 - val_loss: 0.1631 - val_accuracy: 0.9350\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 0.1700 - accuracy: 0.9225 - val_loss: 0.1521 - val_accuracy: 0.9700\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.1723 - accuracy: 0.9337 - val_loss: 0.1298 - val_accuracy: 0.9650\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.1354 - accuracy: 0.9575 - val_loss: 0.1887 - val_accuracy: 0.9500\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.1902 - accuracy: 0.9237 - val_loss: 0.1483 - val_accuracy: 0.9650\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.1361 - accuracy: 0.9513 - val_loss: 0.1251 - val_accuracy: 0.9450\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.1350 - accuracy: 0.9563 - val_loss: 0.2278 - val_accuracy: 0.9350\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.1259 - accuracy: 0.9525 - val_loss: 0.1284 - val_accuracy: 0.9650\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 0.1767 - accuracy: 0.9300 - val_loss: 0.1276 - val_accuracy: 0.9700\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 0.1269 - accuracy: 0.9538 - val_loss: 0.1655 - val_accuracy: 0.9550\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 0.1320 - accuracy: 0.9550 - val_loss: 0.1646 - val_accuracy: 0.9450\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "cnn_model_sq = SimpleCNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "cnn_model_tri = SimpleCNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "\n",
    "CNN_hyperparams = (0.01, 50, 64)\n",
    "H_sq_CNN, sq_CNN_model = train_model(cnn_model_sq, Xsq_train, ysq_train, Xsq_test, ysq_test, num_classes, input_shape, CNN_hyperparams)\n",
    "H_tri_CNN, tri_CNN_model = train_model(cnn_model_tri, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, CNN_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) We have provided a test set of 10 spins configurations for each of the two problems. Each of the spin configurations is not necessarily at the temperatures of the training sets. Calculate your best estimate of the temperatures of these spin configuration. Upload your results to Kaggle.\n",
    "[Hint: A direct fingerprint of temperature is the distribution of spin up\n",
    "and down, because you can image that the spins fluctuate more violently\n",
    "at higher temperature. Although the mothod you use in homework 2 can also work, you may be interested in trying to take distribution into account when you\n",
    "build the model to estimate temperature and see if you can make use of this extra information. This may help you win the\n",
    "kaggle. It is totally fine if you find that the information of distribution is not helpful. Note also that a CNN kind-of does this. One possibility is that you may want a CNN that captures enough distribution information.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsq_sim_data = []\\ntri_sim_data = []\\nsim_data_temp = []\\n\\nfor temp in temps_vec:    \\n    sq_ising_simu = Ising_sq(32, temp)\\n    tri_ising_simu = Ising_tri(32, temp)\\n    sq_img = sq_ising_simu.simulate()\\n    tri_img = tri_ising_simu.simulate()\\n    \\n    sq_sim_data.append(sq_img)\\n    tri_sim_data.append(tri_img)\\n    \\ncwd = str(os.getcwd())\\nsim_data_path_2 = cwd+\"/sim_data_2.npy\"\\nsim_data_temp_path_2 = cwd+\"/sim_data_temp_2.npy\"\\nsim_data_tot_lv_mean_path = cwd+\"/sim_data_tot_lv_mean.npy\"\\n\\nnp.save(sq_sim_data_path, np.asarray(sq_sim_data))\\nnp.save(tri_sim_data_path, np.asarray(tri_sim_data))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = str(os.getcwd())\n",
    "sq_sim_data_path = cwd+\"/sq_sim_data.npy\"\n",
    "tri_sim_data_path = cwd+\"/tri_sim_data.npy\"\n",
    "\n",
    "temps_vec = np.linspace(0.01,15,num=1500)\n",
    "\n",
    "\"\"\"\n",
    "sq_sim_data = []\n",
    "tri_sim_data = []\n",
    "sim_data_temp = []\n",
    "\n",
    "for temp in temps_vec:    \n",
    "    sq_ising_simu = Ising_sq(32, temp)\n",
    "    tri_ising_simu = Ising_tri(32, temp)\n",
    "    sq_img = sq_ising_simu.simulate()\n",
    "    tri_img = tri_ising_simu.simulate()\n",
    "    \n",
    "    sq_sim_data.append(sq_img)\n",
    "    tri_sim_data.append(tri_img)\n",
    "    \n",
    "cwd = str(os.getcwd())\n",
    "sim_data_path_2 = cwd+\"/sim_data_2.npy\"\n",
    "sim_data_temp_path_2 = cwd+\"/sim_data_temp_2.npy\"\n",
    "sim_data_tot_lv_mean_path = cwd+\"/sim_data_tot_lv_mean.npy\"\n",
    "\n",
    "np.save(sq_sim_data_path, np.asarray(sq_sim_data))\n",
    "np.save(tri_sim_data_path, np.asarray(tri_sim_data))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data ready to be embedded\n",
    "sq_sim_img = np.load(sq_sim_data_path).reshape(1500,32,32,-1)\n",
    "tri_sim_img = np.load(tri_sim_data_path).reshape(1500,32,32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_sim_img_rs, tri_sim_img_rs = np.repeat(sq_sim_img, 5, 1), np.repeat(tri_sim_img, 5, 1)\n",
    "sq_sim_img_rs, tri_sim_img_rs = np.repeat(sq_sim_img_rs, 5, 2), np.repeat(tri_sim_img_rs, 5, 2)\n",
    "sq_sim_img_rs, tri_sim_img_rs = np.repeat(sq_sim_img_rs, 3, 3), np.repeat(tri_sim_img_rs, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pre-trained MobileNetV2 \n",
    "with tf.device('/CPU:0'):\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=(160,160,3),\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet',\n",
    "                                                   classes=4)\n",
    "    base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedded data\n",
    "global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "sq_sim_data_emb = global_avg_layer(base_model.predict(sq_sim_img_rs))\n",
    "sq_sim_data_emb = sq_sim_data_emb.numpy()/sq_sim_data_emb.numpy().max()\n",
    "\n",
    "tri_sim_data_emb = global_avg_layer(base_model.predict(tri_sim_img_rs))\n",
    "tri_sim_data_emb = tri_sim_data_emb.numpy()/tri_sim_data_emb.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test splits\n",
    "sq_x_train, sq_x_test, sq_y_train, sq_y_test = train_test_split(sq_sim_data_emb, temps_vec, test_size=0.2, random_state=0)\n",
    "tri_x_train, tri_x_test, tri_y_train, tri_y_test = train_test_split(tri_sim_data_emb, temps_vec, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class RegressionHead:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(channels_first=False):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(612, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "1200/1200 [==============================] - 0s 344us/sample - loss: 54.7610 - MeanSquaredError: 54.7610 - val_loss: 73.1355 - val_MeanSquaredError: 73.1355\n",
      "Epoch 2/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 41.3751 - MeanSquaredError: 41.3751 - val_loss: 42.0638 - val_MeanSquaredError: 42.0638\n",
      "Epoch 3/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 32.7214 - MeanSquaredError: 32.7214 - val_loss: 42.1137 - val_MeanSquaredError: 42.1137\n",
      "Epoch 4/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 25.4869 - MeanSquaredError: 25.4869 - val_loss: 30.4425 - val_MeanSquaredError: 30.4425\n",
      "Epoch 5/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 19.7150 - MeanSquaredError: 19.7150 - val_loss: 21.5422 - val_MeanSquaredError: 21.5422\n",
      "Epoch 6/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 15.7074 - MeanSquaredError: 15.7074 - val_loss: 14.0032 - val_MeanSquaredError: 14.0032\n",
      "Epoch 7/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 12.0298 - MeanSquaredError: 12.0298 - val_loss: 8.7096 - val_MeanSquaredError: 8.7096\n",
      "Epoch 8/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 9.8045 - MeanSquaredError: 9.8045 - val_loss: 8.5081 - val_MeanSquaredError: 8.5081\n",
      "Epoch 9/50\n",
      "1200/1200 [==============================] - 0s 67us/sample - loss: 8.3100 - MeanSquaredError: 8.3100 - val_loss: 5.1005 - val_MeanSquaredError: 5.1005\n",
      "Epoch 10/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 7.3018 - MeanSquaredError: 7.3018 - val_loss: 4.4327 - val_MeanSquaredError: 4.4327\n",
      "Epoch 11/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 6.5196 - MeanSquaredError: 6.5196 - val_loss: 5.3024 - val_MeanSquaredError: 5.3024\n",
      "Epoch 12/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 5.8566 - MeanSquaredError: 5.8566 - val_loss: 3.6974 - val_MeanSquaredError: 3.6974\n",
      "Epoch 13/50\n",
      "1200/1200 [==============================] - 0s 82us/sample - loss: 5.0267 - MeanSquaredError: 5.0267 - val_loss: 3.7570 - val_MeanSquaredError: 3.7570\n",
      "Epoch 14/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 4.7641 - MeanSquaredError: 4.7641 - val_loss: 4.9740 - val_MeanSquaredError: 4.9740\n",
      "Epoch 15/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 4.5853 - MeanSquaredError: 4.5853 - val_loss: 4.7055 - val_MeanSquaredError: 4.7055\n",
      "Epoch 16/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 4.6869 - MeanSquaredError: 4.6869 - val_loss: 4.1263 - val_MeanSquaredError: 4.1263\n",
      "Epoch 17/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 4.4340 - MeanSquaredError: 4.4340 - val_loss: 5.7938 - val_MeanSquaredError: 5.7938\n",
      "Epoch 18/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 4.2057 - MeanSquaredError: 4.2057 - val_loss: 3.1438 - val_MeanSquaredError: 3.1438\n",
      "Epoch 19/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 4.0957 - MeanSquaredError: 4.0957 - val_loss: 3.5672 - val_MeanSquaredError: 3.5672\n",
      "Epoch 20/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 4.5741 - MeanSquaredError: 4.5741 - val_loss: 3.2279 - val_MeanSquaredError: 3.2279\n",
      "Epoch 21/50\n",
      "1200/1200 [==============================] - 0s 83us/sample - loss: 4.2786 - MeanSquaredError: 4.2786 - val_loss: 3.3153 - val_MeanSquaredError: 3.3153\n",
      "Epoch 22/50\n",
      "1200/1200 [==============================] - 0s 87us/sample - loss: 4.3827 - MeanSquaredError: 4.3827 - val_loss: 3.1400 - val_MeanSquaredError: 3.1400\n",
      "Epoch 23/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 4.2659 - MeanSquaredError: 4.2659 - val_loss: 3.7384 - val_MeanSquaredError: 3.7384\n",
      "Epoch 24/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.8785 - MeanSquaredError: 3.8785 - val_loss: 3.2704 - val_MeanSquaredError: 3.2704\n",
      "Epoch 25/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 4.1267 - MeanSquaredError: 4.1267 - val_loss: 3.0630 - val_MeanSquaredError: 3.0630\n",
      "Epoch 26/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.0444 - MeanSquaredError: 4.0444 - val_loss: 3.1919 - val_MeanSquaredError: 3.1919\n",
      "Epoch 27/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.9647 - MeanSquaredError: 3.9647 - val_loss: 3.0886 - val_MeanSquaredError: 3.0886\n",
      "Epoch 28/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 4.1420 - MeanSquaredError: 4.1420 - val_loss: 3.1495 - val_MeanSquaredError: 3.1495\n",
      "Epoch 29/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 4.0421 - MeanSquaredError: 4.0421 - val_loss: 3.1748 - val_MeanSquaredError: 3.1748\n",
      "Epoch 30/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 4.2596 - MeanSquaredError: 4.2596 - val_loss: 3.2913 - val_MeanSquaredError: 3.2913\n",
      "Epoch 31/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.2382 - MeanSquaredError: 4.2382 - val_loss: 3.2308 - val_MeanSquaredError: 3.2308\n",
      "Epoch 32/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.8137 - MeanSquaredError: 3.8137 - val_loss: 3.8275 - val_MeanSquaredError: 3.8275\n",
      "Epoch 33/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.7256 - MeanSquaredError: 3.7256 - val_loss: 3.2995 - val_MeanSquaredError: 3.2995\n",
      "Epoch 34/50\n",
      "1200/1200 [==============================] - 0s 90us/sample - loss: 3.9185 - MeanSquaredError: 3.9185 - val_loss: 3.4063 - val_MeanSquaredError: 3.4063\n",
      "Epoch 35/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 3.9651 - MeanSquaredError: 3.9651 - val_loss: 3.2318 - val_MeanSquaredError: 3.2318\n",
      "Epoch 36/50\n",
      "1200/1200 [==============================] - 0s 68us/sample - loss: 3.9658 - MeanSquaredError: 3.9658 - val_loss: 3.1272 - val_MeanSquaredError: 3.1272\n",
      "Epoch 37/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.6946 - MeanSquaredError: 3.6946 - val_loss: 3.1660 - val_MeanSquaredError: 3.1660\n",
      "Epoch 38/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 4.0699 - MeanSquaredError: 4.0699 - val_loss: 3.3820 - val_MeanSquaredError: 3.3820\n",
      "Epoch 39/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.7012 - MeanSquaredError: 3.7012 - val_loss: 3.8815 - val_MeanSquaredError: 3.8815\n",
      "Epoch 40/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 4.0698 - MeanSquaredError: 4.0698 - val_loss: 3.4681 - val_MeanSquaredError: 3.4681\n",
      "Epoch 41/50\n",
      "1200/1200 [==============================] - 0s 68us/sample - loss: 3.8264 - MeanSquaredError: 3.8264 - val_loss: 3.1326 - val_MeanSquaredError: 3.1326\n",
      "Epoch 42/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 3.7388 - MeanSquaredError: 3.7388 - val_loss: 3.2476 - val_MeanSquaredError: 3.2476\n",
      "Epoch 43/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 3.9349 - MeanSquaredError: 3.9349 - val_loss: 4.6513 - val_MeanSquaredError: 4.6513\n",
      "Epoch 44/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.8160 - MeanSquaredError: 3.8160 - val_loss: 3.3612 - val_MeanSquaredError: 3.3612\n",
      "Epoch 45/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.8655 - MeanSquaredError: 3.8655 - val_loss: 3.1255 - val_MeanSquaredError: 3.1255\n",
      "Epoch 46/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 4.0377 - MeanSquaredError: 4.0377 - val_loss: 3.2353 - val_MeanSquaredError: 3.2353\n",
      "Epoch 47/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.9655 - MeanSquaredError: 3.9655 - val_loss: 3.4882 - val_MeanSquaredError: 3.4882\n",
      "Epoch 48/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.5793 - MeanSquaredError: 3.5793 - val_loss: 3.2497 - val_MeanSquaredError: 3.2497\n",
      "Epoch 49/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 3.7387 - MeanSquaredError: 3.7387 - val_loss: 3.2330 - val_MeanSquaredError: 3.2330\n",
      "Epoch 50/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.9802 - MeanSquaredError: 3.9802 - val_loss: 3.1609 - val_MeanSquaredError: 3.1609\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.01# learning rate\n",
    "EPOCHS = 50 # number of epochs\n",
    "BS = 32 # batch size\n",
    "OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "LOSS = keras.losses.MeanSquaredError()\n",
    "\n",
    "sq_reg_model = RegressionHead.build()\n",
    "sq_reg_model.compile(optimizer=OPT, loss=LOSS, metrics=['MeanSquaredError'])\n",
    "sq_reg_H = sq_reg_model.fit(sq_x_train, sq_y_train, validation_data=(sq_x_test, sq_y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "1200/1200 [==============================] - 0s 357us/sample - loss: 52.7057 - MeanSquaredError: 52.7057 - val_loss: 47.9506 - val_MeanSquaredError: 47.9506\n",
      "Epoch 2/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 39.3660 - MeanSquaredError: 39.3660 - val_loss: 34.1387 - val_MeanSquaredError: 34.1387\n",
      "Epoch 3/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 30.2303 - MeanSquaredError: 30.2303 - val_loss: 37.1038 - val_MeanSquaredError: 37.1038\n",
      "Epoch 4/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 23.0994 - MeanSquaredError: 23.0994 - val_loss: 23.5275 - val_MeanSquaredError: 23.5275\n",
      "Epoch 5/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 17.6534 - MeanSquaredError: 17.6534 - val_loss: 14.9548 - val_MeanSquaredError: 14.9548\n",
      "Epoch 6/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 13.1963 - MeanSquaredError: 13.1963 - val_loss: 12.1166 - val_MeanSquaredError: 12.1166\n",
      "Epoch 7/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 10.3081 - MeanSquaredError: 10.3081 - val_loss: 7.6466 - val_MeanSquaredError: 7.6466\n",
      "Epoch 8/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 8.1903 - MeanSquaredError: 8.1903 - val_loss: 6.5202 - val_MeanSquaredError: 6.5202\n",
      "Epoch 9/50\n",
      "1200/1200 [==============================] - 0s 68us/sample - loss: 6.7329 - MeanSquaredError: 6.7329 - val_loss: 2.8152 - val_MeanSquaredError: 2.8152\n",
      "Epoch 10/50\n",
      "1200/1200 [==============================] - 0s 67us/sample - loss: 5.8988 - MeanSquaredError: 5.8988 - val_loss: 3.3508 - val_MeanSquaredError: 3.3508\n",
      "Epoch 11/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 5.1518 - MeanSquaredError: 5.1518 - val_loss: 2.8613 - val_MeanSquaredError: 2.8613\n",
      "Epoch 12/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 4.5299 - MeanSquaredError: 4.5299 - val_loss: 5.5622 - val_MeanSquaredError: 5.5622\n",
      "Epoch 13/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 4.5055 - MeanSquaredError: 4.5055 - val_loss: 3.1978 - val_MeanSquaredError: 3.1978\n",
      "Epoch 14/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 3.7247 - MeanSquaredError: 3.7247 - val_loss: 2.6410 - val_MeanSquaredError: 2.6410\n",
      "Epoch 15/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 3.9521 - MeanSquaredError: 3.9521 - val_loss: 3.0465 - val_MeanSquaredError: 3.0465\n",
      "Epoch 16/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.2835 - MeanSquaredError: 3.2835 - val_loss: 3.4399 - val_MeanSquaredError: 3.4399\n",
      "Epoch 17/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.8171 - MeanSquaredError: 3.8171 - val_loss: 1.7176 - val_MeanSquaredError: 1.7176\n",
      "Epoch 18/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.5643 - MeanSquaredError: 3.5643 - val_loss: 1.9650 - val_MeanSquaredError: 1.9650\n",
      "Epoch 19/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.2800 - MeanSquaredError: 3.2800 - val_loss: 1.8565 - val_MeanSquaredError: 1.8565\n",
      "Epoch 20/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.5503 - MeanSquaredError: 3.5503 - val_loss: 1.8843 - val_MeanSquaredError: 1.8843\n",
      "Epoch 21/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.4558 - MeanSquaredError: 3.4558 - val_loss: 2.7854 - val_MeanSquaredError: 2.7854\n",
      "Epoch 22/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 3.2186 - MeanSquaredError: 3.2186 - val_loss: 1.7193 - val_MeanSquaredError: 1.7193\n",
      "Epoch 23/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 3.2255 - MeanSquaredError: 3.2255 - val_loss: 2.2888 - val_MeanSquaredError: 2.2888\n",
      "Epoch 24/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 3.2339 - MeanSquaredError: 3.2339 - val_loss: 1.8252 - val_MeanSquaredError: 1.8252\n",
      "Epoch 25/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 3.1753 - MeanSquaredError: 3.1753 - val_loss: 1.6355 - val_MeanSquaredError: 1.6355\n",
      "Epoch 26/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.3002 - MeanSquaredError: 3.3002 - val_loss: 1.6720 - val_MeanSquaredError: 1.6720\n",
      "Epoch 27/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.2218 - MeanSquaredError: 3.2218 - val_loss: 1.7345 - val_MeanSquaredError: 1.7345\n",
      "Epoch 28/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 2.7058 - MeanSquaredError: 2.7058 - val_loss: 1.9616 - val_MeanSquaredError: 1.9616\n",
      "Epoch 29/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 2.9533 - MeanSquaredError: 2.9533 - val_loss: 1.7238 - val_MeanSquaredError: 1.7238\n",
      "Epoch 30/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 3.0552 - MeanSquaredError: 3.0552 - val_loss: 1.5863 - val_MeanSquaredError: 1.5863\n",
      "Epoch 31/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.0769 - MeanSquaredError: 3.0769 - val_loss: 1.7804 - val_MeanSquaredError: 1.7804\n",
      "Epoch 32/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.0355 - MeanSquaredError: 3.0355 - val_loss: 2.2984 - val_MeanSquaredError: 2.2984\n",
      "Epoch 33/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 2.9610 - MeanSquaredError: 2.9610 - val_loss: 1.8051 - val_MeanSquaredError: 1.8051\n",
      "Epoch 34/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.1039 - MeanSquaredError: 3.1039 - val_loss: 1.6080 - val_MeanSquaredError: 1.6080\n",
      "Epoch 35/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 3.1656 - MeanSquaredError: 3.1656 - val_loss: 1.8044 - val_MeanSquaredError: 1.8044\n",
      "Epoch 36/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 2.8203 - MeanSquaredError: 2.8203 - val_loss: 1.9672 - val_MeanSquaredError: 1.9672\n",
      "Epoch 37/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 2.8408 - MeanSquaredError: 2.8408 - val_loss: 1.8197 - val_MeanSquaredError: 1.8197\n",
      "Epoch 38/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 2.9619 - MeanSquaredError: 2.9619 - val_loss: 1.9367 - val_MeanSquaredError: 1.9367\n",
      "Epoch 39/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.0586 - MeanSquaredError: 3.0586 - val_loss: 1.5525 - val_MeanSquaredError: 1.5525\n",
      "Epoch 40/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 2.8224 - MeanSquaredError: 2.8224 - val_loss: 2.3589 - val_MeanSquaredError: 2.3589\n",
      "Epoch 41/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 2.8676 - MeanSquaredError: 2.8676 - val_loss: 1.7988 - val_MeanSquaredError: 1.7988\n",
      "Epoch 42/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 2.7617 - MeanSquaredError: 2.7617 - val_loss: 1.6663 - val_MeanSquaredError: 1.6663\n",
      "Epoch 43/50\n",
      "1200/1200 [==============================] - 0s 68us/sample - loss: 3.2545 - MeanSquaredError: 3.2545 - val_loss: 1.8239 - val_MeanSquaredError: 1.8239\n",
      "Epoch 44/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 2.9440 - MeanSquaredError: 2.9440 - val_loss: 1.6831 - val_MeanSquaredError: 1.6831\n",
      "Epoch 45/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 2.7532 - MeanSquaredError: 2.7532 - val_loss: 1.6561 - val_MeanSquaredError: 1.6561\n",
      "Epoch 46/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 2.9069 - MeanSquaredError: 2.9069 - val_loss: 1.6480 - val_MeanSquaredError: 1.6480\n",
      "Epoch 47/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 3.0146 - MeanSquaredError: 3.0146 - val_loss: 1.8473 - val_MeanSquaredError: 1.8473\n",
      "Epoch 48/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 2.7226 - MeanSquaredError: 2.7226 - val_loss: 1.5456 - val_MeanSquaredError: 1.5456\n",
      "Epoch 49/50\n",
      "1200/1200 [==============================] - 0s 68us/sample - loss: 2.8417 - MeanSquaredError: 2.8417 - val_loss: 3.1401 - val_MeanSquaredError: 3.1401\n",
      "Epoch 50/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 3.1019 - MeanSquaredError: 3.1019 - val_loss: 1.5665 - val_MeanSquaredError: 1.5665\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.01# learning rate\n",
    "EPOCHS = 50 # number of epochs\n",
    "BS = 32 # batch size\n",
    "OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "LOSS = keras.losses.MeanSquaredError()\n",
    "\n",
    "tri_reg_model = RegressionHead.build()\n",
    "tri_reg_model.compile(optimizer=OPT, loss=LOSS, metrics=['MeanSquaredError'])\n",
    "tri_reg_H = tri_reg_model.fit(tri_x_train, tri_y_train, validation_data=(tri_x_test, tri_y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_sim_data_probas = sq_CNN_model.predict_proba(sq_sim_img)\n",
    "tri_sim_data_probas = sq_CNN_model.predict_proba(tri_sim_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_import(dir):\n",
    "    temp = []\n",
    "    for i in range(len(os.listdir(dir+'/'))):\n",
    "        temp.append(np.loadtxt((dir+'/')+str(i).zfill(3), delimiter=','))\n",
    "    return np.array(temp)\n",
    "\n",
    "def T10_import(dir):\n",
    "    temp = []\n",
    "    for i in range(0,10):\n",
    "        for j in range(0,10):\n",
    "            temp.append(np.loadtxt((dir+'/T0'+str(i)+'#')+str(j).zfill(2), delimiter=','))\n",
    "    return np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_t10_img = T10_import(cwd+'/triangle_10T').reshape(100,32,32,-1)\n",
    "sq_t10_img = T10_import(cwd+'/square_10T').reshape(100,32,32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_t10_img_rs, tri_t10_img_rs = np.repeat(sq_t10_img, 5, 1), np.repeat(tri_t10_img, 5, 1)\n",
    "sq_t10_img_rs, tri_t10_img_rs = np.repeat(sq_t10_img_rs, 5, 2), np.repeat(tri_t10_img_rs, 5, 2)\n",
    "sq_t10_img_rs, tri_t10_img_rs = np.repeat(sq_t10_img_rs, 3, 3), np.repeat(tri_t10_img_rs, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_t10_data_emb = global_avg_layer(base_model.predict(sq_t10_img_rs))\n",
    "sq_t10_data_emb = sq_t10_data_emb.numpy()/sq_t10_data_emb.numpy().max()\n",
    "\n",
    "tri_t10_data_emb = global_avg_layer(base_model.predict(tri_t10_img_rs))\n",
    "tri_t10_data_emb = tri_t10_data_emb.numpy()/tri_t10_data_emb.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "pca = PCA(n_components=2)\n",
    "tsne_img_emb = tsne.fit_transform(tri_t10_data_emb)\n",
    "pca_img_emb = pca.fit_transform(tri_t10_data_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    plt.plot(tsne_img_emb.transpose()[0],tsne_img_emb.transpose()[1],'.',markersize=12)\n",
    "plt.title(\"TSNE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEICAYAAABCnX+uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZkUlEQVR4nO3dfYxcZ3XH8d9ZO45DHNbBu85GsU0iGbWN3JJ4dwMRSKU0EobAprypRGp4a+utCiWoyCkvJl43USsRKUK0SLUFNK2aQimQZktAaVCBKFIC63VN5GBSbaHVurDejR0bu0kc2Xv6x8w4u7P3zsyduTP3Pvd+P9IK7+zM3WfYzJlnznOe85i7CwAQrr6sBwAA6AyBHAACRyAHgMARyAEgcARyAAgcgRwAAkcgB4DAEchRCmb232b2vJmdMbNjZva3Zrau+rM3mdmjZnbazBbM7PtmNlb3+DeYmZvZHdk8AyAegRxl8jZ3Xydpu6RRSbvN7F2S/lnS30vaJOkKSXdKelvdY98n6UT1f4FcIZCjdNz9fyV9W9KvS7pX0l3u/gV3P+Xui+7+fXf/w9r9zexlkt4l6UOSXmVmI5kMHIhBIEfpmNlmSW+R9JykzZK+1uQh75R0RpWZ+8OS3tvVAQIJEchRJv9iZiclPSbp+5I+W739F00e9z5J/+Tu5yX9o6Rbzeyi7g0TSIZAjjL5HXdf7+6vdPc/lnS8evuVcQ+ozt5/S9L91ZselLRW0s1dHSmQAIEcZfa0pFlVUidxblPldfKvZjYn6aeqBHLSK8gNAjlKyys9nP9U0qfN7ANm9nIz6zOz15vZ/urd3itpr6Trlny9U9LNZrYhk4EDdQjkKDV3/5qk35X0QUk/l3RM0t2SHjSz10q6WtLn3X1uydekpBlJt2Y0bGAZ42AJAAgbM3IACByBHAAC13EgN7O1ZvZDM/uRmT1lZnvTGBgAoDUd58jNzCRd6u5nqpskHpN0u7s/kcYAAQCNre70AtUSrjPVby+qfjV8dxgYGPCrr766018NAKUyPT39jLsP1t/ecSCXJDNbJWla0lZVSrV+EHGfnZJ2StKWLVt04MCBNH41AJSGmf1P1O2pLHa6+3l3v06VNqA3mNm2iPvsd/cRdx8ZHFzxhgIAaFOqVSvuflLS9yTtSPO6AIB4aVStDJrZ+uq/L5F0k6SfdHpdAEBr0siRXynp76p58j5JX3X3b6ZwXQBAC9KoWnlS0vUpjAUA0IZUqlaA0ExN7tPmg/dooy9o3gY1u32XRsfGsx4W0BYCOUpnanKftk3v1iX2omTSkBbUP71bUxLBHEGi1wpKZ/PBeypBfIlL7EVtPnhPRiMCOkMgR+ls9IWY25/p8UiAdBDIUTrzFr0hbd4GejwSIB0EcpTO7PZdet7XLLvteV+j2e27MhoR0BkWO1E6o2PjmpKqVSvPaN4GNDuc/6oVKm0QJ5Oj3kZGRpymWUDrllXaVD3va3R4+O6uBHPeNPLJzKbdfaT+dlIrQAB6WWlTe9MY0oL6quWZ26Z3a2pyX+q/C+kgkCMVU5P7NDexVYt7+jU3sZUXfcp6WWlDeWZ4COToGDO47utlpQ3lmeEhkKNjzOC6r5eVNpRnhodAjo4xg+u+0bFxHR6+W3Ma1KKb5jTYtYVOyjPDQ/khOjZvgxrSymA+bwMaymA8RTU6Ni5VA/dQ9atbvyfE8swyI5CjY7Pbd6k/ojRudngXgTxQvXrTQDpIraBjvfzYD2AlNgQBQCDYEAQABUUgB4DAEcgBIHAEcgAIHOWHJUAnO6DYCOQFx0HDQPGRWik4+qAAxUcgLzj6oADFRyAvODrZAcVHIC84OtkBxcdiZ8F1u5MdFTFA9ui1grb1+kBgoOzotYLUURED5AOBHG2Lq4i5whc4hBnoIQI52hZXEWMmDmEGeohAjrZFVcTUI9UCdB+BHG2rPxkobt2czUdAd3UcyM1ss5l918yOmNlTZnZ7GgNDGEbHxjU0MaO+vSd10tZF3ueUXdrjUQHlkkYd+TlJH3P3g2Z2maRpM3vE3X+cwrURFEt4O4A0dDwjd/dfuPvB6r9PSzoi6apOr4vw9PvpmNvP9HgkQLmkmiM3s6slXS/pBxE/22lmB8zswMJCdNkawpa0r8vU5D7NTWylVBHoUGqB3MzWSfq6pI+6+y/rf+7u+919xN1HBgejX/AIW5K+LrVdoUNaoFQR6FAqgdzMLlIliN/v7t9I45oIT30Vy5wGY7frsysUSE/Hi51mZpK+KOmIu9/b+ZAQstGxcakauIeqX1E2+kLkGiilikByaczIXyfpNklvNLND1a+3pHBdFBh90oH0dDwjd/fHRH0ZEprdvkv9EZ0TZ4d3xc7iAURjZycykSSfDqAx+pEDQCDoRw4ABUUgB4DAEcgBIHAcvpxjHGwMoBUE8pxadrBxdQt7//RuTUkEcwDLkFrJKbawA2gVgTyn4g42Zgs7gHoE8pxiCzuAVhHIcypJS1gA5UYgzym2sANoFVv0C4JSRaD44rboU35YAJQqAuVGaqUAKFUEyo1AXgCUKgLlRiAvAEoVgXIjkBcApYpAuRHIC4BSRaDcKD8EgEBwQhAAFBSBHAACx4agkmEHKFA8BPIuyGuwbGUHaF7HDiAeqZWU1YLlkBbUVw2W26Z3a2pyX+bjun764w13gOZ17AAaI5CnrNl2+anJfZqb2KrFPf2am9jakyBZC9CrbTHy57UdoGz1B8JEaiVlG31Bsqjbn8msuVVUgF7K5Jqb2KorGowdQH4xI09Zo+3yWc1443qx1Fj1TSVuRwFb/YF8I5CnrNF2+ayaW8W9udTrM2mxLpqz1R/IPwJ5yhptl8+quVXUm0ujDb1xW/2zyO8DaI4t+j20LEde9byv6UlflJfKCp/RvA3oYr2gy3V6xf3mNKihiZmWxu4unbR1mtl+JyWKQA+wRT8HsmxuNTo2rqGJGfXtPamhiRnNbP90oo6JUfl9M+lynaFEEcgYM/ISq5+lN9r8s7inX30RFS01cTN5AOnp6pmdZvYlSW+VNO/u29K4JrpvdGxcqgbuoepXnHkb1JDiq18oUQQa6+au6bRSK/dJ2pHStZBDUQumS1GiCMTr9q7pVAK5uz8q6UQa10I+1fL7z+qyFRUvlCgCjXV7DwmLnWjZ6Ni4Lp84qgPDn+E0IiCBbu8h6dkWfTPbKWmnJG3ZsqVXvxZdkCS3DiB+jWneBlJ5/fRsRu7u+919xN1HBgdb22mI1rFZB8ivbh+QTtOsAsiqGRcQoix67o+OjWtKWl7uO5ze702ljtzMvizpDZIGJB2TtMfdvxh3f+rI0zU3sTXyYxu13cByWe6uTkNXd3a6+63ufqW7X+TumxoFcaQvq2ZcQGiK2nOfqpWE8piLzqoZFxCaok56COQJ5PUotG4vpABFUdRJD4E8gbx+LMuyGRcQkqJOeqhaSaDRMW5Zo7YbaK7b1SNZIZAn0O2ifgDdV8RJD6mVBPLwsSyPi60AssWMPIGsP5ax8QdAFA6WCAgbf4By46i3AihqDSyAzhDIA1LUGlgAnSGQByQPi60A8odAHhA2/gCIwmJnA1m0uwSAOHGLnZQfxqDUD0AoSK3EaLWvCht0AGSNGXmMVvqqMGsHkAfMyGO0UuqX126IAMqFQB6jlVI/NugAyAMCeYxWSv3a3aBDXh1AmsiR11lacri5WnI4NDYe2e5ydvsu9Ucc5Do7vCu2NSZ5dQBpI5AvkTTIttMNsVFe/aVrUbcOoHVsCFoirrvgs7pMZ7V2WYCV2gu6i3v61RdRDeMuvaA1K2b37NwEUMOGoBbElRyu99MyO31hlv6K6U/K5brYzidOj8SdMnReffEVMARyAA2w2LlE3OKl1QX3NXauEsSXaLXsMK4apk+LkfenAgZAM6UO5PXVIz97xetXBNkkmadWgm5cNQwtagG0q7SplciFzeMP6dCGm3XNiccuLF5ebM/rcp1p6ZqtHsIcdfjrlJS4AgYApBIH8rjqkWtOPHbh2LQhVQL+8PQdkQuUS3UadLM+DxRAuEobyJv1UqnVkw/7QtTdljnnfalUl0TN1AGgmdIG8rjqkXkb0Gxd2qWZPjkzZwCZKe1iZ6NeKlFpl0ZYkASQpdIG8ka9VOKaYUXhzEygnPLUM6m0qRUpPicdl3ZZyl06ZoMNFyQ5Kg7oXB5fR3nrmVTYGXkn75ZRaZd6x2xQQxMzDYP4tundGtKC+qp/6G3Tu+l0CCSQ19dR3s4iKGQg7/SPX0u7PKt1kRuCzvqqpumUvP2hgRDl9XWUt7MIUgnkZrbDzJ42sxkz+3ga1+xEGn/80bFxndUlK7bnS9Jz9rKmH5/y9ocGQpTX11HedmJ3HMjNbJWkz0t6s6RrJd1qZtd2et1ONPrjJ0m5xF2n30/HXqN2/biqRSpcgNblLWDWtHKCWC+lsdh5g6QZd/+pJJnZVyTdIunHKVy7LXGLlads3YoFio3Td2hx+o7IRZRGi55DWlixyCGpYf05W+6BZNo5vKUX8rYTO41AfpWk2SXfH5X0mvo7mdlOSTslacuWLSn82nhxf3yZr0i51LbeR606R11n0bViu/7StE1U/XkrFS4AVspbwKwfW152Ynd8sISZvVvSm9z9D6rf3ybpBnf/k7jH9OJgicc/936NHn9Qq7So8+rT1IZb9JrjDzTtmTKnwQu9VqSlpU+V/4iu8IXIvPmimySPvP6im/r2nuzsCQEovbiDJdJY7DwqafOS7zdJ+nkK123b1OQ+XXf8Ia22RZlJq21R1x1/SKdsXdPHXuELy/Leo2PjGpqYUd/ekxqamNGxBjm7vObzABRbGoF8StKrzOwaM1sj6T2SJlO4btviqlYka1ofbk3KFRstcuRtAQRAOXQcyN39nKQPS3pY0hFJX3X3pzq9bifiqk3W+2kdHr5b57z5044rV2y0tb/RzwCgW1LZou/u35L0rTSulYZTti7yMIjaakDcsWr14mpVGy1y5GkBBEA5FG5n59TkPl3qL0T+rM8qaZe4XHY9ctsAQlC4QL754D1aY+dif77Rn4nMZdcX75DbBhCKwgXyZi1o520gMpf9xIa3k9sGEKTCtbFttBtz6Y6w2kYDVdtj6sRjmt2+S0Nj4+S2AQSlcIE8ajdmLW2yVpVKlKjt9Fn3EwaAdhUukNdm2lsP3qX1flpmWrYTsxawX7A18R0SCeQAGmh22EWvD8MoXI5cqrWgXRu5lV6qBOz1vrI8Ucq+PSaAfGt23kEWh2EUMpBLzRc944L8Kbu0C6MBUBTNzjvI4jCM4FMrcR9hWjl3M9rKCJ/HMwMBZGOjL0S2qa59mm/2824Iekbe6CNMK+duRumvS7nk9cxAANlo1hwvi+Z5QQfyRh9hlteKr9zwE6f+/+y8nhkIIBvNmuNl0Twv6EDe7Dy/l1rQnmrpelGHKuf1zEAA2WjWHC+L5nlB58jj8uDzNtDWhh6LSGyl/TsA9FY31riaNcfrdfO8oGfkST7CtNLvcI2dW5Eyocc4EK6yrHEFHciTfIRpcsLbBfUpk9GxcR3acLPOeZ/cpXPep0MbbqZqBQhAWda4gk6tSK1/hGm1HPE5rdHSA+GWHhsnSatVOTZuanIfwRzIuSxKAbMQ9Iw8iVZa10rSpTqrxz/3/gvfl+UdHSiispyjW6hAPjW5T3MTW7W4p3/ZAcpSdBomqiLRTBo9/uCF76laAcJVljWu4FMrNbVFjUbdDOvTML6nP/Jaq5YsjVK1AoSr1kSvUrXyjOZtQLPDxduZXZhA3jAFEvNHO68+rY6oZ6ncXhHVFndpX3MA+VaGc3QLk1ppJwUyteGWFXly98rtNVkU9wNAEoWZkbeTArnxI/fp8c9VcuKrtKjz6tPUhlt040fuW3a/MryjAwhXsIG8frfWz17xevUffyhxCmRp0F4t6caujRgAuiO4QD41uU9bD/65RvxMpad4bWHz+EM6tOFmXXPisUIvagBAvaBy5LXKlMt1ZsXBEJfYi/rVE49Uv2ux1SEAFEBQM/KoypSl1vsZmZ3hMGUApRJUII/bblsTNUvfPn2HfPoOSdJJu0wz2z9NYAdQKMGkVqYm92mxjeGuskqAN5Mu12m9evqThet8BqDcggjktdx4rXFVJ6Ja1QJAyIJIrTTLjSdV2yTEocoAiiCIQB6XG3dfmRdvxbwNaLaF3iwAEIIgUitxrShP2roVnc2aedFXa3b7LtrTAiiMIAJ5XCvKme136vDw3Trn8U/D/aWvZ3WZfjT8FxodG6c9LVBAjVpZF1kQqZVmrSgXq+WFUcyk817JzJzV2gu3054WKJZWWlkXVUczcjN7t5k9ZWaLZjaS1qCijI6Na3b7Ls3bgDb6gjYfvOfCu21c6qVmlenCwau18sOyNJwHiqLZbLvM6dJOZ+SHJb1DUtc/vzR6t1VEz/A4a+ycth68S5dPHC1Fw3mgCFqZbZflfM4oHQVydz8iSdZO6UhCce+2Ww/epbNaq4v1os67qU/etJJlvZ+WRHtaIBStHBxT5nRpzxY7zWynmR0wswMLC81Ps693Rczi5Ho/rSEtqM+kVUazLKCIWilOKHO6tGkgN7PvmNnhiK9bmj12KXff7+4j7j4yONg4px3lfMxQ62ffZtJik3h+0tYl/v1AI2WtluiVuHWweRu48O8yn+bVNLXi7jf1YiDN9EWcrdnInAYvzOKXBvuzvkozw3dqNM3BodTKXC2RRLOd1I1+3urZuWVNlwZRRy41r0ypv+/QxIxs7ykdGP7MsnfoJ4f/khcXUlXmaolW1d7samnQIS1o2/TuC59cmv28zLPtVpjXnz6c5MFmb5f0V5IGJZ2UdMjd39TscSMjI37gwIFEv2vZrKeB530Nf2D01OKefvVFLLAvuqlv78neDyiH5ia2Ri5Ezqky6Wr283pl7ZNkZtPuvqLUu6MZubs/4O6b3P1id7+ilSDerto7ctz7jrt4l0YmWsnfll2zxcokO62bzd7LKJjUilQJ5sdiXjTHqukUgjh6rczVEq1q9maX5M2QVNZKQQVyiRcN8of8bXPNXrdJXtf0SVopiF4rSzXruwJkoazVEq1q9rpN8rou88afOB0tdrarncVOAJCiCx/KUuQQt9gZ3IwcQLm1+6m8yJUuzMgBFF5RZvFdKT8EgLR1o91B0StdSK0AyI1utTsoeotbZuQAcqNbM+eib9oikAPIjW7ViBd9/wmBHEBbupHL7tbMueibtsiRA0isW7nsVtvVtqPIm7aYkQNIrFu57KQzZw70qGBGDiCxblaBtDpz5kCPlzAjB5BYHqpAil4bngSBHEBieagCoQviSwjkABLLQxVIHj4V5AU5cgBtyboKpJsVLqEhkAOBKnI3v1ZwNsFL6H4IBKgo3fyQDN0PgQKhYgNLkVoBApRGHXfZUzNFwowcCFCnFRu11MyQFtRX3UyzbXp3aXdGho5ADgSo0zpuUjPFQiAHAtRpHTebaYqFHDkQqE7quOdtUENaGcznbaB0NdhFwIwcKKE8bLFHepiRAyWUp800VM90jg1BADLDxqZk2BAEIHeonkkHgRxAZqieSQeBHEBmaEWbDgI5gMxQPZMOAjmAzOThgIoi6KhqxczukfQ2SS9K+i9JH3D3k80eR9UKACTXraqVRyRtc/ffkPSfkj7R4fUAAAl1FMjd/d/c/Vz12yckbep8SACAJNLMkX9Q0rfjfmhmO83sgJkdWFiILjkCACTXdIu+mX1H0f14PuXuD1bv8ylJ5yTdH3cdd98vab9UyZG3NVoAwApNA7m739To52b2PklvlfTbnsV+fwAouU6rVnZIulfSb7rHbNGKftyCpP+TFPr2rQGF/RxCH7/Ec8gLnkNvvNLdV+yi6jSQz0i6WNLx6k1PuPsftfjYA1FlNCEJ/TmEPn6J55AXPIdsddTG1t23pjUQAEB72NkJAIHLMpDvz/B3pyX05xD6+CWeQ17wHDKUycESAID0kFoBgMARyAEgcJkFcjO7x8x+YmZPmtkDZrY+q7G0y8zebWZPmdmimQVVtmRmO8zsaTObMbOPZz2epMzsS2Y2b2aHsx5LO8xss5l918yOVP8buj3rMSVlZmvN7Idm9qPqc9ib9ZjaZWarzOw/zOybWY+lHVnOyIvQOfGwpHdIejTrgSRhZqskfV7SmyVdK+lWM7s221Eldp+kHVkPogPnJH3M3X9N0mslfSjAv8FZSW9091dLuk7SDjN7bcZjatftko5kPYh2ZRbIi9A50d2PuPvTWY+jDTdImnH3n7r7i5K+IumWjMeUiLs/KulE1uNol7v/wt0PVv99WpUgclW2o0rGK85Uv72o+hVc9YSZbZJ0s6QvZD2WduUlR96wcyJSd5Wk2SXfH1VgQaRIzOxqSddL+kG2I0mumpI4JGle0iPuHtxzkPRZSXdIWsx6IO3qaGdnM2l1TsxSK88hQBZxW3AzqSIws3WSvi7po+7+y6zHk5S7n5d0XXWN6wEz2+buwaxbmNlbJc27+7SZvSHr8bSrq4G8CJ0Tmz2HQB2VtHnJ95sk/TyjsZSWmV2kShC/392/kfV4OuHuJ83se6qsWwQTyCW9TtKYmb1F0lpJLzezf3D338t4XIlkWbWyQ9KfSRpz9+eyGkdJTUl6lZldY2ZrJL1H0mTGYyoVMzNJX5R0xN3vzXo87TCzwVq1mZldIukmST/JdlTJuPsn3H2Tu1+tyuvg30ML4lK2OfK/lnSZpEfM7JCZ/U2GY2mLmb3dzI5KulHSQ2b2cNZjakV1kfnDkh5WZZHtq+7+VLajSsbMvizpcUm/YmZHzez3sx5TQq+TdJukN1b/+z9UnRWG5EpJ3zWzJ1WZHDzi7kGW74WOLfoAELi8VK0AANpEIAeAwBHIASBwBHIACByBHAACRyAHgMARyAEgcP8P6dBV+xueXTwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    plt.plot(pca_img_emb.transpose()[0], pca_img_emb.transpose()[1],'.',markersize=12)\n",
    "plt.title(\"PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_pred_temps = tri_reg_model.predict(tri_t10_data_emb).reshape(10,-1)\n",
    "sq_pred_temps = tri_reg_model.predict(sq_t10_data_emb).reshape(10,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tri_class_probas = tri_CNN_model.predict_proba(tri_t10_img).reshape(10,-1,4)\n",
    "all_sq_class_probas = sq_CNN_model.predict_proba(sq_t10_img).reshape(10,-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tri_class_probas = np.mean(all_tri_class_probas, axis=1)\n",
    "avg_sq_class_probas = np.mean(all_sq_class_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "tri_temp_list = []\n",
    "sq_temp_list = []\n",
    "\n",
    "for proba in avg_tri_class_probas:\n",
    "    dist_list = []\n",
    "    for proba_2 in tri_sim_data_probas:\n",
    "        dist_list.append(distance.euclidean(proba, proba_2))\n",
    "    max_idx = dist_list.index(min(dist_list))\n",
    "    tri_temp_list.append(temps_vec[max_idx])\n",
    "    \n",
    "for proba in avg_sq_class_probas:\n",
    "    dist_list = []\n",
    "    for proba_2 in sq_sim_data_probas:\n",
    "        dist_list.append(distance.euclidean(proba, proba_2))\n",
    "    max_idx = dist_list.index(min(dist_list))\n",
    "    sq_temp_list.append(temps_vec[max_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.9299999999999997,\n",
       " 2.53,\n",
       " 2.1199999999999997,\n",
       " 1.46,\n",
       " 1.4100000000000001,\n",
       " 3.3,\n",
       " 2.9699999999999998,\n",
       " 1.9100000000000001,\n",
       " 3.8899999999999997,\n",
       " 0.86]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.29,\n",
       " 4.109999999999999,\n",
       " 4.7299999999999995,\n",
       " 4.06,\n",
       " 4.2299999999999995,\n",
       " 5.29,\n",
       " 4.24,\n",
       " 2.1999999999999997,\n",
       " 5.17,\n",
       " 3.01]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) *Transfer Learning*.  \n",
    "As we emphasize in class, one can freeze the training of the bottom layers of a network and retrain the top part of the network to adopt to a new situation. Use your CNN that you trained on the squarelattice data to do transfer learning on the triangular lattice data.  How does the performance compare to that of the direct methods?  Add the performance numbers for transfer learning in your table from Part (a). Note that the training time and number of training examples needed for transfer learning is far less than that for the direct  optimization. For  example,  is  50  triangle  example  sufficient  for the re-training process?  Use your transfer learning result to predict the transition temperature of triangle lattice Ising model, as demonstrated in this [Nature Physics](https://www-nature-com.ezp-prod1.hul.harvard.edu/articles/nphys4035.pdf) publication.\n",
    "\n",
    "As a guideline, you may like to just change the last `Dense` layer with `softmax` activation when you do the transfer learning. Other choices are also OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:51:16.394160Z",
     "start_time": "2020-02-24T16:51:16.390887Z"
    }
   },
   "source": [
    "Solution to (d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = sq_CNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = base_model.layers[0:5]\n",
    "trainable_layers = [\n",
    "     Flatten(),\n",
    "     Dropout(0.25),\n",
    "     Dense(32, activation='relu'),\n",
    "     Dense(num_classes, activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_model = keras.Sequential(base_layers+trainable_layers)\n",
    "trans_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "hyperparams = (0.01, 25, 32)\n",
    "H_trans, trained_trans_model = train_model(trans_model, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_ests = trained_trans_model.predict_proba(tri_sim_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(proba_ests, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(proba_ests[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(proba_ests[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(proba_ests[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(proba_ests[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
