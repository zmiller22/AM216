{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:46:46.494051Z",
     "start_time": "2020-02-24T16:46:45.817790Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import gc\n",
    "from numba import cuda\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:46:46.494051Z",
     "start_time": "2020-02-24T16:46:45.817790Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# This is how the triagle lattice data is generated. You may find it helpful to generate some \n",
    "# of your own data\n",
    "class Ising_tri():\n",
    "    ''' Simulating the Ising model '''  \n",
    "    def __init__(self, size, temp):\n",
    "        self.temp = temp\n",
    "        self.N = int(size)\n",
    "    ## monte carlo moves\n",
    "    \n",
    "    def mcmove(self, config, N, beta):\n",
    "        ''' This is to execute the monte carlo moves using \n",
    "        Metropolis algorithm such that detailed\n",
    "        balance condition is satisified'''\n",
    "        for i in range(N):\n",
    "            for j in range(N):            \n",
    "                    a = np.random.randint(0, N) # select a row\n",
    "                    b = np.random.randint(0, N) # select a column\n",
    "                    s =  config[a, b] # current state at (a, b)\n",
    "                    if a%2:\n",
    "                        nb = config[(a+1)%N,b] +config[(a+1)%N,(b+1)%N] + config[a,(b+1)%N] + \\\n",
    "                        config[(a-1)%N,b] + config[(a-1)%N,(b+1)%N] + config[a,(b-1)%N]\n",
    "                    else:\n",
    "                        nb = config[(a+1)%N,b] +config[(a+1)%N,(b-1)%N] + config[a,(b+1)%N] + \\\n",
    "                        config[(a-1)%N,b] + config[(a-1)%N,(b-1)%N] + config[a,(b-1)%N]\n",
    "                    \n",
    "                    \n",
    "                    cost = 2*s*nb\n",
    "                    if cost < 0:\t\n",
    "                        s *= -1\n",
    "                    elif rand() < np.exp(-cost*beta):\n",
    "                        s *= -1\n",
    "                    config[a, b] = s\n",
    "        return config\n",
    "    \n",
    "    def simulate(self):   \n",
    "        ''' This module simulates the Ising model'''\n",
    "        config = 2*np.random.randint(2, size=(self.N,self.N))-1   \n",
    "        msrmnt = 81\n",
    "        for i in range(msrmnt):\n",
    "            self.mcmove(config, self.N, 1.0/self.temp)\n",
    "        return config\n",
    "\n",
    "class Ising_sq():\n",
    "    ''' Simulating the Ising model '''  \n",
    "    def __init__(self, size, temp):\n",
    "        self.temp = temp\n",
    "        self.N = int(size)\n",
    "    ## monte carlo moves\n",
    "\n",
    "    def mcmove(self, config, N, beta):\n",
    "        ''' This is to execute the monte carlo moves using \n",
    "        Metropolis algorithm such that detailed\n",
    "        balance condition is satisified'''\n",
    "        for i in range(N):\n",
    "            for j in range(N):            \n",
    "                    a = np.random.randint(0, N) # select a row\n",
    "                    b = np.random.randint(0, N) # select a column\n",
    "                    s =  config[a, b] # current state at (a, b)\n",
    "                    # periodic boundary condition imposed\n",
    "                    nb = config[(a+1)%N,b] + config[a,(b+1)%N] + config[(a-1)%N,b] + config[a,(b-1)%N]\n",
    "                    cost = 2*s*nb\n",
    "                    if cost < 0:\n",
    "                        s *= -1\n",
    "                    elif rand() < np.exp(-cost*beta):\n",
    "                        s *= -1\n",
    "                    config[a, b] = s\n",
    "        return config\n",
    "    \n",
    "    def simulate(self):   \n",
    "        ''' This module simulates the Ising model'''\n",
    "        config = 2*np.random.randint(2, size=(self.N,self.N))-1   \n",
    "        times = 100\n",
    "        for i in range(times):\n",
    "            self.mcmove(config, self.N, 1.0/self.temp)\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import 4-temp data for square and triangular lattices as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:30:37.313572Z",
     "start_time": "2020-02-20T22:30:34.531268Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "nx, ny = 32, 32\n",
    "\n",
    "Xsq = np.ndarray((4*N,nx,ny,1))\n",
    "ysq = np.ndarray(4*N)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    Xsq[i + 0*N] = np.loadtxt(\"./square_T1/square_T1/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 0*N] = 0\n",
    "    Xsq[i + 1*N] = np.loadtxt(\"./square_T2/square_T2/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 1*N] = 1\n",
    "    Xsq[i + 2*N] = np.loadtxt(\"./square_T3/square_T3/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 2*N] = 2\n",
    "    Xsq[i + 3*N] = np.loadtxt(\"./square_T4/square_T4/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 3*N] = 3\n",
    "\n",
    "Xsq_train, Xsq_test, ysq_train, ysq_test = train_test_split(Xsq, ysq, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:31:02.786262Z",
     "start_time": "2020-02-20T22:31:00.698414Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "nx, ny = 32, 32\n",
    "\n",
    "Xtri = np.ndarray((4*N,nx,ny,1))\n",
    "ytri = np.ndarray(4*N)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    Xtri[i + 0*N] = np.loadtxt(\"./triangle_T1/triangle_T1/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 0*N] = 0\n",
    "    Xtri[i + 1*N] = np.loadtxt(\"./triangle_T2/triangle_T2/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 1*N] = 1\n",
    "    Xtri[i + 2*N] = np.loadtxt(\"./triangle_T3/triangle_T3/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 2*N] = 2\n",
    "    Xtri[i + 3*N] = np.loadtxt(\"./triangle_T4/triangle_T4/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 3*N] = 3\n",
    "\n",
    "Xtri_train, Xtri_test, ytri_train, ytri_test = train_test_split(Xtri, ytri, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you know the shape of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:31:05.870456Z",
     "start_time": "2020-02-20T22:31:05.864585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:\n",
      "(800, 32, 32, 1) (800, 32, 32, 1)\n",
      "(800,) (800,)\n",
      "Shape of test data:\n",
      "(200, 32, 32, 1) (200, 32, 32, 1)\n",
      "(200,) (200,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data:\")\n",
    "print(Xsq_train.shape, Xtri_train.shape)\n",
    "print(ysq_train.shape, ytri_train.shape)\n",
    "print(\"Shape of test data:\")\n",
    "print(Xsq_test.shape, Xtri_test.shape)\n",
    "print(ysq_test.shape, ytri_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Train a fully connected neural network to do the classification on both datasets. Then, train  a  convolutional  neural  network  to  do  the  classification,  on  both datasets.   Make  a  table  of  your  performance  numbers  for  both  models  and  upload  these  numbers.   This,  together  with  your code,  should be uploaded to the course website when you turn in your homework.\n",
    "\n",
    "The temperatures for square lattice are $T = 1.5, 2.1, 2.4, 3.5$. $T = 2.5, 3.2, 3.8, 5$ for triangle lattice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class small_FNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(width, height, depth, num_classes, channels_first=False):\n",
    "        model = Sequential()\n",
    "\n",
    "        if channels_first == False:\n",
    "            inputShape = (height, width, depth)\n",
    "            chanDim = -1\n",
    "        elif channels_first == True:\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(256,  activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(128,  activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_model, train_data, train_lbls, test_data, \n",
    "                test_lbls, num_classes, input_shape, hyperparams):\n",
    "    # Ensure data is shaped properly, assumes channels last set up\n",
    "    x_train = train_data\n",
    "    x_test = test_data\n",
    "    \n",
    "    # Create categorical labels\n",
    "    y_train = keras.utils.to_categorical(train_lbls, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_lbls, num_classes)\n",
    "     \n",
    "    # Set hyperparameters\n",
    "    INIT_LR = hyperparams[0]# learning rate\n",
    "    EPOCHS = hyperparams[1] # number of epochs\n",
    "    BS = hyperparams[2] # batch size\n",
    "    OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "    \n",
    "    model = input_model\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=OPT, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    H = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)\n",
    "    \n",
    "    return H, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.7555 - accuracy: 0.2488 - val_loss: 1.4371 - val_accuracy: 0.2350\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.5301 - accuracy: 0.3150 - val_loss: 1.4052 - val_accuracy: 0.2500\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 1.4472 - accuracy: 0.3562 - val_loss: 1.3562 - val_accuracy: 0.2650\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.3842 - accuracy: 0.3675 - val_loss: 1.3101 - val_accuracy: 0.2850\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.2905 - accuracy: 0.4062 - val_loss: 1.2757 - val_accuracy: 0.3350\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 1.2801 - accuracy: 0.4000 - val_loss: 1.2307 - val_accuracy: 0.3950\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 1.2320 - accuracy: 0.4387 - val_loss: 1.2007 - val_accuracy: 0.4050\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.1941 - accuracy: 0.4700 - val_loss: 1.1601 - val_accuracy: 0.4750\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.1809 - accuracy: 0.4650 - val_loss: 1.1305 - val_accuracy: 0.5150\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.1429 - accuracy: 0.4863 - val_loss: 1.0990 - val_accuracy: 0.5150\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.0642 - accuracy: 0.5213 - val_loss: 1.0710 - val_accuracy: 0.5150\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.0580 - accuracy: 0.5425 - val_loss: 1.0406 - val_accuracy: 0.5250\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.0399 - accuracy: 0.5163 - val_loss: 1.0097 - val_accuracy: 0.5750\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 1.0081 - accuracy: 0.5650 - val_loss: 0.9876 - val_accuracy: 0.5650\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 0.9697 - accuracy: 0.5663 - val_loss: 0.9727 - val_accuracy: 0.5800\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 0.9643 - accuracy: 0.5713 - val_loss: 0.9648 - val_accuracy: 0.5800\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.9136 - accuracy: 0.5838 - val_loss: 0.9437 - val_accuracy: 0.6050\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.9186 - accuracy: 0.6112 - val_loss: 0.9408 - val_accuracy: 0.5750\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.8797 - accuracy: 0.6187 - val_loss: 0.9259 - val_accuracy: 0.5850\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.8871 - accuracy: 0.6037 - val_loss: 0.9236 - val_accuracy: 0.6000\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 0.8439 - accuracy: 0.6463 - val_loss: 0.9111 - val_accuracy: 0.5900\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.8482 - accuracy: 0.6250 - val_loss: 0.9115 - val_accuracy: 0.6000\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 0.8242 - accuracy: 0.6438 - val_loss: 0.9184 - val_accuracy: 0.5850\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 0.7888 - accuracy: 0.6475 - val_loss: 0.9049 - val_accuracy: 0.6100\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.7836 - accuracy: 0.6725 - val_loss: 0.8999 - val_accuracy: 0.6100\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 0.7771 - accuracy: 0.6775 - val_loss: 0.8899 - val_accuracy: 0.6150\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 0.7490 - accuracy: 0.6925 - val_loss: 0.8743 - val_accuracy: 0.6200\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.7481 - accuracy: 0.6963 - val_loss: 0.8980 - val_accuracy: 0.6000\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.7227 - accuracy: 0.7200 - val_loss: 0.9009 - val_accuracy: 0.5650\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.7143 - accuracy: 0.6938 - val_loss: 0.8976 - val_accuracy: 0.5800\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 0.6947 - accuracy: 0.7075 - val_loss: 0.8972 - val_accuracy: 0.6050\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.6621 - accuracy: 0.7275 - val_loss: 0.9045 - val_accuracy: 0.5850\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 0.6416 - accuracy: 0.7400 - val_loss: 0.9202 - val_accuracy: 0.5800\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.6547 - accuracy: 0.7175 - val_loss: 0.9099 - val_accuracy: 0.5750\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.6116 - accuracy: 0.7450 - val_loss: 0.9035 - val_accuracy: 0.5500\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.5966 - accuracy: 0.7525 - val_loss: 0.9057 - val_accuracy: 0.5600\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.6690 - accuracy: 0.7188 - val_loss: 0.9108 - val_accuracy: 0.5500\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.5919 - accuracy: 0.7613 - val_loss: 0.9093 - val_accuracy: 0.5750\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.6016 - accuracy: 0.7525 - val_loss: 0.9001 - val_accuracy: 0.5600\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.5774 - accuracy: 0.7675 - val_loss: 0.8940 - val_accuracy: 0.5700\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.6136 - accuracy: 0.7450 - val_loss: 0.9082 - val_accuracy: 0.5600\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.5440 - accuracy: 0.7850 - val_loss: 0.9031 - val_accuracy: 0.5800\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 0.5471 - accuracy: 0.7675 - val_loss: 0.9105 - val_accuracy: 0.5750\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.5424 - accuracy: 0.7788 - val_loss: 0.9102 - val_accuracy: 0.5750\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.5478 - accuracy: 0.7600 - val_loss: 0.9247 - val_accuracy: 0.5900\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.5341 - accuracy: 0.7713 - val_loss: 0.9262 - val_accuracy: 0.5600\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.5354 - accuracy: 0.7862 - val_loss: 0.9412 - val_accuracy: 0.5550\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.5160 - accuracy: 0.7713 - val_loss: 0.9373 - val_accuracy: 0.5600\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.5316 - accuracy: 0.7738 - val_loss: 0.9405 - val_accuracy: 0.5450\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.5349 - accuracy: 0.7675 - val_loss: 0.9422 - val_accuracy: 0.5400\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.8006 - accuracy: 0.2688 - val_loss: 1.4253 - val_accuracy: 0.2700\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 1.5750 - accuracy: 0.3250 - val_loss: 1.4192 - val_accuracy: 0.2450\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.4127 - accuracy: 0.3713 - val_loss: 1.3858 - val_accuracy: 0.2800\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 1.3400 - accuracy: 0.3913 - val_loss: 1.3508 - val_accuracy: 0.3050\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.2802 - accuracy: 0.3975 - val_loss: 1.3008 - val_accuracy: 0.4000\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 105us/sample - loss: 1.2341 - accuracy: 0.4288 - val_loss: 1.1907 - val_accuracy: 0.4500\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.2079 - accuracy: 0.4412 - val_loss: 1.1440 - val_accuracy: 0.4750\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.1429 - accuracy: 0.4538 - val_loss: 1.0961 - val_accuracy: 0.4850\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 1.1157 - accuracy: 0.4950 - val_loss: 1.0348 - val_accuracy: 0.5150\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.0825 - accuracy: 0.5075 - val_loss: 0.9940 - val_accuracy: 0.5300\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.0250 - accuracy: 0.5225 - val_loss: 0.9661 - val_accuracy: 0.5600\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.0343 - accuracy: 0.5350 - val_loss: 0.9346 - val_accuracy: 0.5650\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.9620 - accuracy: 0.5700 - val_loss: 0.9214 - val_accuracy: 0.5600\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.9741 - accuracy: 0.5537 - val_loss: 0.8960 - val_accuracy: 0.5600\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.9265 - accuracy: 0.5688 - val_loss: 0.8821 - val_accuracy: 0.5800\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 0.9041 - accuracy: 0.5838 - val_loss: 0.8791 - val_accuracy: 0.5650\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 156us/sample - loss: 0.9285 - accuracy: 0.5763 - val_loss: 0.8648 - val_accuracy: 0.5950\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 162us/sample - loss: 0.9120 - accuracy: 0.5863 - val_loss: 0.8524 - val_accuracy: 0.6000\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 154us/sample - loss: 0.8909 - accuracy: 0.5987 - val_loss: 0.8522 - val_accuracy: 0.5950\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 0.8768 - accuracy: 0.6075 - val_loss: 0.8446 - val_accuracy: 0.6150\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.8576 - accuracy: 0.5913 - val_loss: 0.8388 - val_accuracy: 0.6200\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 0.8298 - accuracy: 0.6237 - val_loss: 0.8304 - val_accuracy: 0.6250\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 119us/sample - loss: 0.8325 - accuracy: 0.6237 - val_loss: 0.8131 - val_accuracy: 0.6300\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.8479 - accuracy: 0.6025 - val_loss: 0.8137 - val_accuracy: 0.6050\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.8069 - accuracy: 0.6325 - val_loss: 0.8131 - val_accuracy: 0.6200\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.7843 - accuracy: 0.6450 - val_loss: 0.8080 - val_accuracy: 0.6200\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.7878 - accuracy: 0.6350 - val_loss: 0.8112 - val_accuracy: 0.6450\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.7257 - accuracy: 0.6725 - val_loss: 0.8109 - val_accuracy: 0.6150\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.7680 - accuracy: 0.6650 - val_loss: 0.8004 - val_accuracy: 0.6250\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.7703 - accuracy: 0.6388 - val_loss: 0.7973 - val_accuracy: 0.6250\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 0.7392 - accuracy: 0.6637 - val_loss: 0.8037 - val_accuracy: 0.6150\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.7364 - accuracy: 0.6775 - val_loss: 0.7917 - val_accuracy: 0.6250\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.7148 - accuracy: 0.6862 - val_loss: 0.7872 - val_accuracy: 0.6600\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.6738 - accuracy: 0.7125 - val_loss: 0.7880 - val_accuracy: 0.6450\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.6755 - accuracy: 0.6938 - val_loss: 0.7900 - val_accuracy: 0.6650\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.7239 - accuracy: 0.6662 - val_loss: 0.7924 - val_accuracy: 0.6750\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.6938 - accuracy: 0.6775 - val_loss: 0.8053 - val_accuracy: 0.6700\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.6512 - accuracy: 0.7100 - val_loss: 0.8039 - val_accuracy: 0.6800\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.7136 - accuracy: 0.6762 - val_loss: 0.7980 - val_accuracy: 0.6450\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.6658 - accuracy: 0.7200 - val_loss: 0.7884 - val_accuracy: 0.6550\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.6680 - accuracy: 0.6963 - val_loss: 0.7886 - val_accuracy: 0.6450\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.6593 - accuracy: 0.7038 - val_loss: 0.7795 - val_accuracy: 0.6450\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 0.6114 - accuracy: 0.7300 - val_loss: 0.7751 - val_accuracy: 0.6700\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 0.6262 - accuracy: 0.7075 - val_loss: 0.7811 - val_accuracy: 0.6550\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 122us/sample - loss: 0.6034 - accuracy: 0.7175 - val_loss: 0.7804 - val_accuracy: 0.6550\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 134us/sample - loss: 0.6257 - accuracy: 0.7262 - val_loss: 0.7886 - val_accuracy: 0.6400\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 133us/sample - loss: 0.6234 - accuracy: 0.7175 - val_loss: 0.7947 - val_accuracy: 0.6400\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 138us/sample - loss: 0.6077 - accuracy: 0.7237 - val_loss: 0.7869 - val_accuracy: 0.6600\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 141us/sample - loss: 0.5767 - accuracy: 0.7513 - val_loss: 0.7804 - val_accuracy: 0.6800\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 136us/sample - loss: 0.5996 - accuracy: 0.7212 - val_loss: 0.7778 - val_accuracy: 0.6750\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "fnn_model_sq = small_FNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "fnn_model_tri = small_FNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "\n",
    "# hyperparams should be a tuple of: INIT_LR, EPOCHS, BS\n",
    "FNN_hyperparams = (0.01, 50, 32)\n",
    "H_sq_FNN, sq_FNN_model = train_model(fnn_model_sq, Xsq_train, ysq_train, Xsq_test, ysq_test, num_classes, input_shape, FNN_hyperparams)\n",
    "H_tri_FNN, tri_FNN_model = train_model(fnn_model_tri, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, FNN_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Train a convolutional neural network to do the classification, on both datasets. Make a table of your performance numbers for (a) and (b). \n",
    "Try to optimize the performance of your models and compare the result.\n",
    "\n",
    "solution to (b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(width, height, depth, num_classes, channels_first=False):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        \n",
    "        if channels_first == False:\n",
    "            inputShape = (height, width, depth)\n",
    "            chanDim = -1\n",
    "        elif channels_first == True:\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        model.add(Conv2D(32, (5, 5), activation='relu', input_shape=inputShape))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "                  \n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.3780 - accuracy: 0.2475 - val_loss: 1.2947 - val_accuracy: 0.3450\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 1.3038 - accuracy: 0.3262 - val_loss: 1.2100 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 1.1429 - accuracy: 0.5412 - val_loss: 0.9591 - val_accuracy: 0.5600\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.9492 - accuracy: 0.5487 - val_loss: 0.7605 - val_accuracy: 0.6850\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.8694 - accuracy: 0.6237 - val_loss: 0.6766 - val_accuracy: 0.6850\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.7335 - accuracy: 0.6675 - val_loss: 0.6444 - val_accuracy: 0.7400\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.6604 - accuracy: 0.7100 - val_loss: 0.7573 - val_accuracy: 0.6150\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.5636 - accuracy: 0.7350 - val_loss: 0.4964 - val_accuracy: 0.7800\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.5287 - accuracy: 0.7638 - val_loss: 1.0314 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.5053 - accuracy: 0.7937 - val_loss: 0.4100 - val_accuracy: 0.8300\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.4284 - accuracy: 0.8150 - val_loss: 0.5212 - val_accuracy: 0.7400\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.3918 - accuracy: 0.8375 - val_loss: 0.3721 - val_accuracy: 0.8550\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 0.3845 - accuracy: 0.8400 - val_loss: 0.3196 - val_accuracy: 0.8550\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.3243 - accuracy: 0.8737 - val_loss: 0.6700 - val_accuracy: 0.6800\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.3116 - accuracy: 0.8687 - val_loss: 0.8127 - val_accuracy: 0.6300\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.4326 - accuracy: 0.8213 - val_loss: 0.2808 - val_accuracy: 0.8900\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.2730 - accuracy: 0.9038 - val_loss: 0.3642 - val_accuracy: 0.8550\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.2607 - accuracy: 0.9038 - val_loss: 0.3114 - val_accuracy: 0.8650\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.2212 - accuracy: 0.9162 - val_loss: 0.2591 - val_accuracy: 0.8850\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.3729 - accuracy: 0.8388 - val_loss: 0.8849 - val_accuracy: 0.6600\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.2831 - accuracy: 0.8950 - val_loss: 0.3097 - val_accuracy: 0.8850\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.2112 - accuracy: 0.9250 - val_loss: 0.2378 - val_accuracy: 0.8850\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.1824 - accuracy: 0.9362 - val_loss: 0.2685 - val_accuracy: 0.8800\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.3048 - accuracy: 0.8988 - val_loss: 0.2420 - val_accuracy: 0.8900\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1711 - accuracy: 0.9450 - val_loss: 0.2317 - val_accuracy: 0.8900\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.1959 - accuracy: 0.9237 - val_loss: 0.5253 - val_accuracy: 0.7850\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.2049 - accuracy: 0.9125 - val_loss: 0.2944 - val_accuracy: 0.8700\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 93us/sample - loss: 0.3049 - accuracy: 0.8850 - val_loss: 0.2133 - val_accuracy: 0.9100\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1761 - accuracy: 0.9388 - val_loss: 0.1825 - val_accuracy: 0.9100\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1686 - accuracy: 0.9325 - val_loss: 0.1956 - val_accuracy: 0.9150\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.1595 - accuracy: 0.9413 - val_loss: 0.1838 - val_accuracy: 0.9200\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1429 - accuracy: 0.9500 - val_loss: 0.1641 - val_accuracy: 0.9250\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1395 - accuracy: 0.9500 - val_loss: 0.1632 - val_accuracy: 0.9400\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1582 - accuracy: 0.9438 - val_loss: 0.1501 - val_accuracy: 0.9450\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.1219 - accuracy: 0.9525 - val_loss: 0.1807 - val_accuracy: 0.9100\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1322 - accuracy: 0.9563 - val_loss: 0.1402 - val_accuracy: 0.9450\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1465 - accuracy: 0.9475 - val_loss: 0.2003 - val_accuracy: 0.9250\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.1401 - accuracy: 0.9488 - val_loss: 0.1730 - val_accuracy: 0.9300\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1306 - accuracy: 0.9500 - val_loss: 0.1264 - val_accuracy: 0.9600\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.1305 - accuracy: 0.9488 - val_loss: 0.1294 - val_accuracy: 0.9600\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.1296 - accuracy: 0.9550 - val_loss: 0.1367 - val_accuracy: 0.9550\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 72us/sample - loss: 0.1311 - accuracy: 0.9538 - val_loss: 0.2129 - val_accuracy: 0.8950\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1382 - accuracy: 0.9438 - val_loss: 0.3118 - val_accuracy: 0.8650\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.1609 - accuracy: 0.9400 - val_loss: 0.1717 - val_accuracy: 0.9250\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 71us/sample - loss: 0.1402 - accuracy: 0.9513 - val_loss: 0.1582 - val_accuracy: 0.9500\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.1137 - accuracy: 0.9638 - val_loss: 0.1290 - val_accuracy: 0.9300\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 72us/sample - loss: 0.1084 - accuracy: 0.9650 - val_loss: 0.2099 - val_accuracy: 0.9150\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.1170 - accuracy: 0.9563 - val_loss: 0.1590 - val_accuracy: 0.9350\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1082 - accuracy: 0.9663 - val_loss: 0.1199 - val_accuracy: 0.9450\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.0985 - accuracy: 0.9663 - val_loss: 0.1203 - val_accuracy: 0.9450\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 0s 491us/sample - loss: 1.3774 - accuracy: 0.2637 - val_loss: 1.3126 - val_accuracy: 0.3550\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 1.2855 - accuracy: 0.3800 - val_loss: 1.2458 - val_accuracy: 0.2800\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 92us/sample - loss: 1.1393 - accuracy: 0.4787 - val_loss: 0.9918 - val_accuracy: 0.4900\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.9650 - accuracy: 0.5387 - val_loss: 0.7842 - val_accuracy: 0.7050\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.9111 - accuracy: 0.6075 - val_loss: 0.7319 - val_accuracy: 0.6500\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 84us/sample - loss: 0.7443 - accuracy: 0.6275 - val_loss: 0.5879 - val_accuracy: 0.8000\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.6331 - accuracy: 0.7000 - val_loss: 0.5114 - val_accuracy: 0.8150\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.7066 - accuracy: 0.6612 - val_loss: 0.5398 - val_accuracy: 0.7250\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.6433 - accuracy: 0.7113 - val_loss: 0.5016 - val_accuracy: 0.7850\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.5078 - accuracy: 0.7738 - val_loss: 0.4684 - val_accuracy: 0.7850\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.4364 - accuracy: 0.8163 - val_loss: 0.5029 - val_accuracy: 0.7600\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.3997 - accuracy: 0.8275 - val_loss: 0.3502 - val_accuracy: 0.8900\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.3731 - accuracy: 0.8313 - val_loss: 0.7169 - val_accuracy: 0.6900\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.6043 - accuracy: 0.7775 - val_loss: 0.3333 - val_accuracy: 0.9000\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.3901 - accuracy: 0.8438 - val_loss: 0.7050 - val_accuracy: 0.7000\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.5594 - accuracy: 0.7525 - val_loss: 0.3852 - val_accuracy: 0.8400\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.3648 - accuracy: 0.8537 - val_loss: 0.2845 - val_accuracy: 0.8900\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.3322 - accuracy: 0.8475 - val_loss: 0.4279 - val_accuracy: 0.7900\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 0.3400 - accuracy: 0.8687 - val_loss: 0.3691 - val_accuracy: 0.8500\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.2843 - accuracy: 0.8788 - val_loss: 0.3295 - val_accuracy: 0.8350\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.3422 - accuracy: 0.8750 - val_loss: 0.2596 - val_accuracy: 0.9000\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.2872 - accuracy: 0.8788 - val_loss: 0.3358 - val_accuracy: 0.8600\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.3295 - accuracy: 0.8425 - val_loss: 0.2224 - val_accuracy: 0.9300\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.2450 - accuracy: 0.9038 - val_loss: 0.3026 - val_accuracy: 0.8950\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.2635 - accuracy: 0.8875 - val_loss: 0.4539 - val_accuracy: 0.7900\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.3723 - accuracy: 0.8400 - val_loss: 0.2819 - val_accuracy: 0.9000\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.2825 - accuracy: 0.9000 - val_loss: 0.2345 - val_accuracy: 0.9250\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.2262 - accuracy: 0.9125 - val_loss: 0.2423 - val_accuracy: 0.9450\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.2139 - accuracy: 0.9150 - val_loss: 0.2068 - val_accuracy: 0.9400\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.2231 - accuracy: 0.9112 - val_loss: 0.4045 - val_accuracy: 0.8150\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 91us/sample - loss: 0.3067 - accuracy: 0.8675 - val_loss: 0.1824 - val_accuracy: 0.9550\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 91us/sample - loss: 0.2752 - accuracy: 0.8788 - val_loss: 0.1951 - val_accuracy: 0.9600\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 94us/sample - loss: 0.2279 - accuracy: 0.9062 - val_loss: 0.2452 - val_accuracy: 0.9250\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.2137 - accuracy: 0.9125 - val_loss: 0.1763 - val_accuracy: 0.9600\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.1959 - accuracy: 0.9275 - val_loss: 0.2194 - val_accuracy: 0.9100\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.1827 - accuracy: 0.9275 - val_loss: 0.1581 - val_accuracy: 0.9400\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.1875 - accuracy: 0.9325 - val_loss: 0.1407 - val_accuracy: 0.9600\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.1734 - accuracy: 0.9450 - val_loss: 0.2869 - val_accuracy: 0.8600\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.2784 - accuracy: 0.8737 - val_loss: 0.1693 - val_accuracy: 0.9450\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.1738 - accuracy: 0.9275 - val_loss: 0.1331 - val_accuracy: 0.9500\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.2053 - accuracy: 0.9150 - val_loss: 0.1768 - val_accuracy: 0.9250\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.1612 - accuracy: 0.9450 - val_loss: 0.1555 - val_accuracy: 0.9600\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 88us/sample - loss: 0.1480 - accuracy: 0.9513 - val_loss: 0.1358 - val_accuracy: 0.9500\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.1416 - accuracy: 0.9563 - val_loss: 0.3135 - val_accuracy: 0.9150\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.3051 - accuracy: 0.8838 - val_loss: 0.1224 - val_accuracy: 0.9650\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.1500 - accuracy: 0.9337 - val_loss: 0.1425 - val_accuracy: 0.9400\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.1305 - accuracy: 0.9500 - val_loss: 0.2027 - val_accuracy: 0.9350\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 93us/sample - loss: 0.1301 - accuracy: 0.9588 - val_loss: 0.1109 - val_accuracy: 0.9600\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.1223 - accuracy: 0.9550 - val_loss: 0.1326 - val_accuracy: 0.9500\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1351 - accuracy: 0.9488 - val_loss: 0.1192 - val_accuracy: 0.9700\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "cnn_model_sq = SimpleCNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "cnn_model_tri = SimpleCNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "\n",
    "CNN_hyperparams = (0.01, 50, 64)\n",
    "H_sq_CNN, sq_CNN_model = train_model(cnn_model_sq, Xsq_train, ysq_train, Xsq_test, ysq_test, num_classes, input_shape, CNN_hyperparams)\n",
    "H_tri_CNN, tri_CNN_model = train_model(cnn_model_tri, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, CNN_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) We have provided a test set of 10 spins configurations for each of the two problems. Each of the spin configurations is not necessarily at the temperatures of the training sets. Calculate your best estimate of the temperatures of these spin configuration. Upload your results to Kaggle.\n",
    "[Hint: A direct fingerprint of temperature is the distribution of spin up\n",
    "and down, because you can image that the spins fluctuate more violently\n",
    "at higher temperature. Although the mothod you use in homework 2 can also work, you may be interested in trying to take distribution into account when you\n",
    "build the model to estimate temperature and see if you can make use of this extra information. This may help you win the\n",
    "kaggle. It is totally fine if you find that the information of distribution is not helpful. Note also that a CNN kind-of does this. One possibility is that you may want a CNN that captures enough distribution information.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsq_sim_data = []\\ntri_sim_data = []\\nsim_data_temp = []\\n\\nfor temp in temps_vec:    \\n    sq_ising_simu = Ising_sq(32, temp)\\n    tri_ising_simu = Ising_tri(32, temp)\\n    sq_img = sq_ising_simu.simulate()\\n    tri_img = tri_ising_simu.simulate()\\n    \\n    sq_sim_data.append(sq_img)\\n    tri_sim_data.append(tri_img)\\n    \\ncwd = str(os.getcwd())\\nsim_data_path_2 = cwd+\"/sim_data_2.npy\"\\nsim_data_temp_path_2 = cwd+\"/sim_data_temp_2.npy\"\\nsim_data_tot_lv_mean_path = cwd+\"/sim_data_tot_lv_mean.npy\"\\n\\nnp.save(sq_sim_data_path, np.asarray(sq_sim_data))\\nnp.save(tri_sim_data_path, np.asarray(tri_sim_data))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = str(os.getcwd())\n",
    "sq_sim_data_path = cwd+\"/sq_sim_data.npy\"\n",
    "tri_sim_data_path = cwd+\"/tri_sim_data.npy\"\n",
    "\n",
    "temps_vec = np.linspace(0.01,15,num=1500)\n",
    "\n",
    "\"\"\"\n",
    "sq_sim_data = []\n",
    "tri_sim_data = []\n",
    "sim_data_temp = []\n",
    "\n",
    "for temp in temps_vec:    \n",
    "    sq_ising_simu = Ising_sq(32, temp)\n",
    "    tri_ising_simu = Ising_tri(32, temp)\n",
    "    sq_img = sq_ising_simu.simulate()\n",
    "    tri_img = tri_ising_simu.simulate()\n",
    "    \n",
    "    sq_sim_data.append(sq_img)\n",
    "    tri_sim_data.append(tri_img)\n",
    "    \n",
    "cwd = str(os.getcwd())\n",
    "sim_data_path_2 = cwd+\"/sim_data_2.npy\"\n",
    "sim_data_temp_path_2 = cwd+\"/sim_data_temp_2.npy\"\n",
    "sim_data_tot_lv_mean_path = cwd+\"/sim_data_tot_lv_mean.npy\"\n",
    "\n",
    "np.save(sq_sim_data_path, np.asarray(sq_sim_data))\n",
    "np.save(tri_sim_data_path, np.asarray(tri_sim_data))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data ready to be embedded\n",
    "sq_sim_data = np.load(sq_sim_data_path).reshape(1500,32,32,-1)\n",
    "tri_sim_data = np.load(tri_sim_data_path).reshape(1500,32,32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_sim_data, tri_sim_data = np.repeat(sq_sim_data, 5, 1), np.repeat(tri_sim_data, 5, 1)\n",
    "sq_sim_data, tri_sim_data = np.repeat(sq_sim_data, 5, 2), np.repeat(tri_sim_data, 5, 2)\n",
    "sq_sim_data, tri_sim_data = np.repeat(sq_sim_data, 3, 3), np.repeat(tri_sim_data, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 160, 160, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_sim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pre-trained MobileNetV2 \n",
    "with tf.device('/CPU:0'):\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=(160,160,3),\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')\n",
    "    base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedded data\n",
    "global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "sq_sim_data_emb = global_avg_layer(base_model.predict(sq_sim_data))\n",
    "sq_sim_data_emb = sq_sim_data_emb.numpy()/sq_sim_data_emb.numpy().max()\n",
    "\n",
    "tri_sim_data_emb = global_avg_layer(base_model.predict(tri_sim_data))\n",
    "tri_sim_data_emb = tri_sim_data_emb.numpy()/tri_sim_data_emb.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test splits\n",
    "sq_x_train, sq_x_test, sq_y_train, sq_y_test = train_test_split(sq_sim_data_emb, temps_vec, test_size=0.2, random_state=0)\n",
    "tri_x_train, tri_x_test, tri_y_train, tri_y_test = train_test_split(tri_sim_data_emb, temps_vec, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class RegressionHead:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(channels_first=False):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(num_classes, activation='linear'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "1200/1200 [==============================] - 0s 353us/sample - loss: 59.5136 - MeanSquaredError: 59.5135 - val_loss: 60.3111 - val_MeanSquaredError: 60.3111\n",
      "Epoch 2/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 46.5617 - MeanSquaredError: 46.5617 - val_loss: 47.9774 - val_MeanSquaredError: 47.9774\n",
      "Epoch 3/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 38.9227 - MeanSquaredError: 38.9227 - val_loss: 38.4351 - val_MeanSquaredError: 38.4351\n",
      "Epoch 4/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 31.6338 - MeanSquaredError: 31.6338 - val_loss: 38.2514 - val_MeanSquaredError: 38.2514\n",
      "Epoch 5/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 25.9141 - MeanSquaredError: 25.9141 - val_loss: 24.7662 - val_MeanSquaredError: 24.7662\n",
      "Epoch 6/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 20.6272 - MeanSquaredError: 20.6272 - val_loss: 22.9175 - val_MeanSquaredError: 22.9175\n",
      "Epoch 7/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 16.9375 - MeanSquaredError: 16.9375 - val_loss: 21.6822 - val_MeanSquaredError: 21.6822\n",
      "Epoch 8/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 13.8849 - MeanSquaredError: 13.8849 - val_loss: 12.8191 - val_MeanSquaredError: 12.8191\n",
      "Epoch 9/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 11.1564 - MeanSquaredError: 11.1564 - val_loss: 12.5702 - val_MeanSquaredError: 12.5702\n",
      "Epoch 10/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 9.3702 - MeanSquaredError: 9.3701 - val_loss: 10.6290 - val_MeanSquaredError: 10.6290\n",
      "Epoch 11/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 7.8936 - MeanSquaredError: 7.8936 - val_loss: 8.5423 - val_MeanSquaredError: 8.5423\n",
      "Epoch 12/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 6.8891 - MeanSquaredError: 6.8891 - val_loss: 7.3732 - val_MeanSquaredError: 7.3732\n",
      "Epoch 13/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 6.2399 - MeanSquaredError: 6.2399 - val_loss: 6.3058 - val_MeanSquaredError: 6.3058\n",
      "Epoch 14/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 5.5875 - MeanSquaredError: 5.5875 - val_loss: 6.2445 - val_MeanSquaredError: 6.2445\n",
      "Epoch 15/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 5.3208 - MeanSquaredError: 5.3208 - val_loss: 6.4247 - val_MeanSquaredError: 6.4247\n",
      "Epoch 16/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 5.1542 - MeanSquaredError: 5.1542 - val_loss: 5.6115 - val_MeanSquaredError: 5.6115\n",
      "Epoch 17/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 5.1010 - MeanSquaredError: 5.1010 - val_loss: 6.2505 - val_MeanSquaredError: 6.2505\n",
      "Epoch 18/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 4.6255 - MeanSquaredError: 4.6255 - val_loss: 3.5255 - val_MeanSquaredError: 3.5255\n",
      "Epoch 19/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 4.8279 - MeanSquaredError: 4.8279 - val_loss: 3.3472 - val_MeanSquaredError: 3.3472\n",
      "Epoch 20/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 4.5396 - MeanSquaredError: 4.5396 - val_loss: 4.1795 - val_MeanSquaredError: 4.1795\n",
      "Epoch 21/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 4.3438 - MeanSquaredError: 4.3438 - val_loss: 4.5738 - val_MeanSquaredError: 4.5738\n",
      "Epoch 22/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.1171 - MeanSquaredError: 4.1171 - val_loss: 3.1953 - val_MeanSquaredError: 3.1953\n",
      "Epoch 23/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.5508 - MeanSquaredError: 4.5508 - val_loss: 4.1882 - val_MeanSquaredError: 4.1882\n",
      "Epoch 24/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 4.5814 - MeanSquaredError: 4.5814 - val_loss: 3.3456 - val_MeanSquaredError: 3.3456\n",
      "Epoch 25/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.5477 - MeanSquaredError: 4.5477 - val_loss: 3.4999 - val_MeanSquaredError: 3.4999\n",
      "Epoch 26/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.2438 - MeanSquaredError: 4.2438 - val_loss: 3.2723 - val_MeanSquaredError: 3.2723\n",
      "Epoch 27/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.2295 - MeanSquaredError: 4.2295 - val_loss: 3.3498 - val_MeanSquaredError: 3.3498\n",
      "Epoch 28/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 4.0077 - MeanSquaredError: 4.0077 - val_loss: 3.1913 - val_MeanSquaredError: 3.1913\n",
      "Epoch 29/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 4.0545 - MeanSquaredError: 4.0545 - val_loss: 5.0332 - val_MeanSquaredError: 5.0332\n",
      "Epoch 30/50\n",
      "1200/1200 [==============================] - 0s 85us/sample - loss: 3.9175 - MeanSquaredError: 3.9175 - val_loss: 3.3140 - val_MeanSquaredError: 3.3140\n",
      "Epoch 31/50\n",
      "1200/1200 [==============================] - 0s 82us/sample - loss: 4.0934 - MeanSquaredError: 4.0934 - val_loss: 3.5446 - val_MeanSquaredError: 3.5446\n",
      "Epoch 32/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.9466 - MeanSquaredError: 3.9466 - val_loss: 3.3990 - val_MeanSquaredError: 3.3990\n",
      "Epoch 33/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.9088 - MeanSquaredError: 3.9088 - val_loss: 3.2487 - val_MeanSquaredError: 3.2487\n",
      "Epoch 34/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 4.0413 - MeanSquaredError: 4.0413 - val_loss: 3.2133 - val_MeanSquaredError: 3.2133\n",
      "Epoch 35/50\n",
      "1200/1200 [==============================] - 0s 79us/sample - loss: 3.8370 - MeanSquaredError: 3.8370 - val_loss: 3.2454 - val_MeanSquaredError: 3.2454\n",
      "Epoch 36/50\n",
      "1200/1200 [==============================] - 0s 81us/sample - loss: 4.1197 - MeanSquaredError: 4.1197 - val_loss: 3.3037 - val_MeanSquaredError: 3.3037\n",
      "Epoch 37/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.6089 - MeanSquaredError: 3.6089 - val_loss: 3.7338 - val_MeanSquaredError: 3.7338\n",
      "Epoch 38/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 4.1496 - MeanSquaredError: 4.1496 - val_loss: 3.1266 - val_MeanSquaredError: 3.1266\n",
      "Epoch 39/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.9478 - MeanSquaredError: 3.9478 - val_loss: 3.1883 - val_MeanSquaredError: 3.1883\n",
      "Epoch 40/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.7318 - MeanSquaredError: 3.7318 - val_loss: 3.0345 - val_MeanSquaredError: 3.0345\n",
      "Epoch 41/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.9546 - MeanSquaredError: 3.9546 - val_loss: 3.2005 - val_MeanSquaredError: 3.2005\n",
      "Epoch 42/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 3.9430 - MeanSquaredError: 3.9430 - val_loss: 3.0697 - val_MeanSquaredError: 3.0697\n",
      "Epoch 43/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 4.0239 - MeanSquaredError: 4.0239 - val_loss: 3.4523 - val_MeanSquaredError: 3.4523\n",
      "Epoch 44/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.8490 - MeanSquaredError: 3.8490 - val_loss: 3.1555 - val_MeanSquaredError: 3.1555\n",
      "Epoch 45/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.8038 - MeanSquaredError: 3.8038 - val_loss: 3.3809 - val_MeanSquaredError: 3.3809\n",
      "Epoch 46/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.6792 - MeanSquaredError: 3.6792 - val_loss: 3.3660 - val_MeanSquaredError: 3.3660\n",
      "Epoch 47/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.8459 - MeanSquaredError: 3.8459 - val_loss: 3.3826 - val_MeanSquaredError: 3.3826\n",
      "Epoch 48/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 4.1176 - MeanSquaredError: 4.1176 - val_loss: 3.2752 - val_MeanSquaredError: 3.2752\n",
      "Epoch 49/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.6881 - MeanSquaredError: 3.6881 - val_loss: 3.1558 - val_MeanSquaredError: 3.1558\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 71us/sample - loss: 3.6153 - MeanSquaredError: 3.6153 - val_loss: 3.0868 - val_MeanSquaredError: 3.0868\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.01# learning rate\n",
    "EPOCHS = 50 # number of epochs\n",
    "BS = 32 # batch size\n",
    "OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "LOSS = keras.losses.MeanSquaredError()\n",
    "\n",
    "sq_reg_model = RegressionHead.build()\n",
    "sq_reg_model.compile(optimizer=OPT, loss=LOSS, metrics=['MeanSquaredError'])\n",
    "sq_reg_H = sq_reg_model.fit(sq_x_train, sq_y_train, validation_data=(sq_x_test, sq_y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 3.3350 - MeanSquaredError: 3.3350 - val_loss: 3.0976 - val_MeanSquaredError: 3.0976\n",
      "Epoch 2/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.3985 - MeanSquaredError: 3.3985 - val_loss: 1.9726 - val_MeanSquaredError: 1.9726\n",
      "Epoch 3/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.1756 - MeanSquaredError: 3.1756 - val_loss: 1.8838 - val_MeanSquaredError: 1.8838\n",
      "Epoch 4/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.6048 - MeanSquaredError: 3.6048 - val_loss: 3.0977 - val_MeanSquaredError: 3.0977\n",
      "Epoch 5/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.0266 - MeanSquaredError: 3.0266 - val_loss: 1.7251 - val_MeanSquaredError: 1.7251\n",
      "Epoch 6/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.2917 - MeanSquaredError: 3.2917 - val_loss: 1.6674 - val_MeanSquaredError: 1.6674\n",
      "Epoch 7/50\n",
      "1200/1200 [==============================] - 0s 80us/sample - loss: 3.3547 - MeanSquaredError: 3.3547 - val_loss: 1.7959 - val_MeanSquaredError: 1.7959\n",
      "Epoch 8/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.3046 - MeanSquaredError: 3.3046 - val_loss: 1.6921 - val_MeanSquaredError: 1.6921\n",
      "Epoch 9/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 2.9122 - MeanSquaredError: 2.9122 - val_loss: 1.7820 - val_MeanSquaredError: 1.7820\n",
      "Epoch 10/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.2723 - MeanSquaredError: 3.2723 - val_loss: 2.0302 - val_MeanSquaredError: 2.0302\n",
      "Epoch 11/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 3.0956 - MeanSquaredError: 3.0956 - val_loss: 1.9140 - val_MeanSquaredError: 1.9140\n",
      "Epoch 12/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.0342 - MeanSquaredError: 3.0342 - val_loss: 1.7102 - val_MeanSquaredError: 1.7102\n",
      "Epoch 13/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.1060 - MeanSquaredError: 3.1060 - val_loss: 1.6355 - val_MeanSquaredError: 1.6355\n",
      "Epoch 14/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 2.9833 - MeanSquaredError: 2.9833 - val_loss: 2.2273 - val_MeanSquaredError: 2.2273\n",
      "Epoch 15/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.0514 - MeanSquaredError: 3.0514 - val_loss: 1.9627 - val_MeanSquaredError: 1.9627\n",
      "Epoch 16/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 2.8557 - MeanSquaredError: 2.8557 - val_loss: 1.5772 - val_MeanSquaredError: 1.5772\n",
      "Epoch 17/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 2.9201 - MeanSquaredError: 2.9201 - val_loss: 1.7101 - val_MeanSquaredError: 1.7101\n",
      "Epoch 18/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 3.2756 - MeanSquaredError: 3.2756 - val_loss: 1.6587 - val_MeanSquaredError: 1.6587\n",
      "Epoch 19/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 2.8095 - MeanSquaredError: 2.8095 - val_loss: 1.6467 - val_MeanSquaredError: 1.6467\n",
      "Epoch 20/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 2.9715 - MeanSquaredError: 2.9715 - val_loss: 1.7418 - val_MeanSquaredError: 1.7418\n",
      "Epoch 21/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 2.6375 - MeanSquaredError: 2.6375 - val_loss: 1.6038 - val_MeanSquaredError: 1.6038\n",
      "Epoch 22/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 2.9474 - MeanSquaredError: 2.9474 - val_loss: 1.6494 - val_MeanSquaredError: 1.6494\n",
      "Epoch 23/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 2.9429 - MeanSquaredError: 2.9429 - val_loss: 1.6115 - val_MeanSquaredError: 1.6115\n",
      "Epoch 24/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 2.9155 - MeanSquaredError: 2.9155 - val_loss: 1.6927 - val_MeanSquaredError: 1.6927\n",
      "Epoch 25/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.3015 - MeanSquaredError: 3.3015 - val_loss: 1.6371 - val_MeanSquaredError: 1.6371\n",
      "Epoch 26/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 2.9266 - MeanSquaredError: 2.9266 - val_loss: 1.7343 - val_MeanSquaredError: 1.7343\n",
      "Epoch 27/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 2.6209 - MeanSquaredError: 2.6209 - val_loss: 1.6442 - val_MeanSquaredError: 1.6442\n",
      "Epoch 28/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 3.1396 - MeanSquaredError: 3.1396 - val_loss: 1.6147 - val_MeanSquaredError: 1.6147\n",
      "Epoch 29/50\n",
      "1200/1200 [==============================] - 0s 82us/sample - loss: 3.1904 - MeanSquaredError: 3.1904 - val_loss: 1.9612 - val_MeanSquaredError: 1.9612\n",
      "Epoch 30/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 2.9637 - MeanSquaredError: 2.9637 - val_loss: 1.5744 - val_MeanSquaredError: 1.5744\n",
      "Epoch 31/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 2.8355 - MeanSquaredError: 2.8355 - val_loss: 1.6175 - val_MeanSquaredError: 1.6175\n",
      "Epoch 32/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.0411 - MeanSquaredError: 3.0411 - val_loss: 1.5407 - val_MeanSquaredError: 1.5407\n",
      "Epoch 33/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 2.9336 - MeanSquaredError: 2.9336 - val_loss: 1.5787 - val_MeanSquaredError: 1.5787\n",
      "Epoch 34/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.1009 - MeanSquaredError: 3.1009 - val_loss: 1.6874 - val_MeanSquaredError: 1.6874\n",
      "Epoch 35/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.1449 - MeanSquaredError: 3.1449 - val_loss: 1.6464 - val_MeanSquaredError: 1.6464\n",
      "Epoch 36/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 2.8565 - MeanSquaredError: 2.8565 - val_loss: 1.6206 - val_MeanSquaredError: 1.6206\n",
      "Epoch 37/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.0041 - MeanSquaredError: 3.0041 - val_loss: 1.7429 - val_MeanSquaredError: 1.7429\n",
      "Epoch 38/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.2243 - MeanSquaredError: 3.2243 - val_loss: 1.7070 - val_MeanSquaredError: 1.7070\n",
      "Epoch 39/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 2.6547 - MeanSquaredError: 2.6547 - val_loss: 1.6047 - val_MeanSquaredError: 1.6047\n",
      "Epoch 40/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 2.8129 - MeanSquaredError: 2.8129 - val_loss: 1.7773 - val_MeanSquaredError: 1.7773\n",
      "Epoch 41/50\n",
      "1200/1200 [==============================] - 0s 83us/sample - loss: 2.7360 - MeanSquaredError: 2.7360 - val_loss: 1.6168 - val_MeanSquaredError: 1.6168\n",
      "Epoch 42/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.1269 - MeanSquaredError: 3.1269 - val_loss: 1.6280 - val_MeanSquaredError: 1.6280\n",
      "Epoch 43/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 2.8503 - MeanSquaredError: 2.8503 - val_loss: 1.5728 - val_MeanSquaredError: 1.5728\n",
      "Epoch 44/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 3.0038 - MeanSquaredError: 3.0038 - val_loss: 1.6816 - val_MeanSquaredError: 1.6816\n",
      "Epoch 45/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 2.8457 - MeanSquaredError: 2.8457 - val_loss: 1.5702 - val_MeanSquaredError: 1.5702\n",
      "Epoch 46/50\n",
      "1200/1200 [==============================] - 0s 69us/sample - loss: 3.0196 - MeanSquaredError: 3.0196 - val_loss: 1.5868 - val_MeanSquaredError: 1.5868\n",
      "Epoch 47/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 2.8746 - MeanSquaredError: 2.8746 - val_loss: 1.5955 - val_MeanSquaredError: 1.5955\n",
      "Epoch 48/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 2.5931 - MeanSquaredError: 2.5931 - val_loss: 1.6565 - val_MeanSquaredError: 1.6565\n",
      "Epoch 49/50\n",
      "1200/1200 [==============================] - 0s 70us/sample - loss: 2.6910 - MeanSquaredError: 2.6910 - val_loss: 1.6990 - val_MeanSquaredError: 1.6990\n",
      "Epoch 50/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 2.9316 - MeanSquaredError: 2.9316 - val_loss: 1.5954 - val_MeanSquaredError: 1.5954\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.01# learning rate\n",
    "EPOCHS = 50 # number of epochs\n",
    "BS = 32 # batch size\n",
    "OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "LOSS = keras.losses.MeanSquaredError()\n",
    "\n",
    "tri_reg_model = RegressionHead.build()\n",
    "tri_reg_model.compile(optimizer=OPT, loss=LOSS, metrics=['MeanSquaredError'])\n",
    "tri_reg_H = sq_reg_model.fit(tri_x_train, tri_y_train, validation_data=(tri_x_test, tri_y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) *Transfer Learning*.  \n",
    "As we emphasize in class, one can freeze the training of the bottom layers of a network and retrain the top part of the network to adopt to a new situation. Use your CNN that you trained on the squarelattice data to do transfer learning on the triangular lattice data.  How does the performance compare to that of the direct methods?  Add the performance numbers for transfer learning in your table from Part (a). Note that the training time and number of training examples needed for transfer learning is far less than that for the direct  optimization. For  example,  is  50  triangle  example  sufficient  for the re-training process?  Use your transfer learning result to predict the transition temperature of triangle lattice Ising model, as demonstrated in this [Nature Physics](https://www-nature-com.ezp-prod1.hul.harvard.edu/articles/nphys4035.pdf) publication.\n",
    "\n",
    "As a guideline, you may like to just change the last `Dense` layer with `softmax` activation when you do the transfer learning. Other choices are also OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:51:16.394160Z",
     "start_time": "2020-02-24T16:51:16.390887Z"
    }
   },
   "source": [
    "Solution to (d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = sq_CNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 122,116\n",
      "Trainable params: 122,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = base_model.layers[0:5]\n",
    "trainable_layers = [\n",
    "     Flatten(),\n",
    "     Dropout(0.25),\n",
    "     Dense(32, activation='relu'),\n",
    "     Dense(num_classes, activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 89,188\n",
      "Trainable params: 89,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trans_model = keras.Sequential(base_layers+trainable_layers)\n",
    "trans_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 0s 479us/sample - loss: 0.7529 - accuracy: 0.6575 - val_loss: 0.4117 - val_accuracy: 0.8500\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.4861 - accuracy: 0.7887 - val_loss: 0.3034 - val_accuracy: 0.9300\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 0.3805 - accuracy: 0.8400 - val_loss: 0.5164 - val_accuracy: 0.7100\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.3872 - accuracy: 0.8413 - val_loss: 0.2501 - val_accuracy: 0.9350\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.2618 - accuracy: 0.9087 - val_loss: 0.2114 - val_accuracy: 0.9400\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.2837 - accuracy: 0.8863 - val_loss: 0.2339 - val_accuracy: 0.9100\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.1939 - accuracy: 0.9300 - val_loss: 0.1860 - val_accuracy: 0.9250\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.1879 - accuracy: 0.9237 - val_loss: 0.1567 - val_accuracy: 0.9500\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.4667 - accuracy: 0.8338 - val_loss: 0.1743 - val_accuracy: 0.9550\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.1733 - accuracy: 0.9400 - val_loss: 0.1620 - val_accuracy: 0.9400\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.1453 - accuracy: 0.9525 - val_loss: 0.1648 - val_accuracy: 0.9300\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.1509 - accuracy: 0.9413 - val_loss: 0.1259 - val_accuracy: 0.9500\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.1426 - accuracy: 0.9463 - val_loss: 0.1705 - val_accuracy: 0.9200\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.1604 - accuracy: 0.9287 - val_loss: 0.1182 - val_accuracy: 0.9650\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.1392 - accuracy: 0.9500 - val_loss: 0.1331 - val_accuracy: 0.9600\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 0.0958 - accuracy: 0.9638 - val_loss: 0.1137 - val_accuracy: 0.9550\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.1153 - accuracy: 0.9588 - val_loss: 0.1317 - val_accuracy: 0.9650\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.1184 - accuracy: 0.9613 - val_loss: 0.1123 - val_accuracy: 0.9650\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.1086 - accuracy: 0.9600 - val_loss: 0.1063 - val_accuracy: 0.9650\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 0.0753 - accuracy: 0.9775 - val_loss: 0.1145 - val_accuracy: 0.9600\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.0915 - accuracy: 0.9650 - val_loss: 0.4387 - val_accuracy: 0.8750\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 0.3788 - accuracy: 0.8875 - val_loss: 0.1006 - val_accuracy: 0.9700\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.0839 - accuracy: 0.9750 - val_loss: 0.1038 - val_accuracy: 0.9650\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.0868 - accuracy: 0.9787 - val_loss: 0.0963 - val_accuracy: 0.9700\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.0703 - accuracy: 0.9812 - val_loss: 0.1005 - val_accuracy: 0.9650\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "hyperparams = (0.01, 25, 32)\n",
    "H_trans, trained_trans_model = train_model(trans_model, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
