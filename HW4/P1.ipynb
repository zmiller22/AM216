{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:46:46.494051Z",
     "start_time": "2020-02-24T16:46:45.817790Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import gc\n",
    "from numba import cuda\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:46:46.494051Z",
     "start_time": "2020-02-24T16:46:45.817790Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# This is how the triagle lattice data is generated. You may find it helpful to generate some \n",
    "# of your own data\n",
    "class Ising_tri():\n",
    "    ''' Simulating the Ising model '''  \n",
    "    def __init__(self, size, temp):\n",
    "        self.temp = temp\n",
    "        self.N = int(size)\n",
    "    ## monte carlo moves\n",
    "    \n",
    "    def mcmove(self, config, N, beta):\n",
    "        ''' This is to execute the monte carlo moves using \n",
    "        Metropolis algorithm such that detailed\n",
    "        balance condition is satisified'''\n",
    "        for i in range(N):\n",
    "            for j in range(N):            \n",
    "                    a = np.random.randint(0, N) # select a row\n",
    "                    b = np.random.randint(0, N) # select a column\n",
    "                    s =  config[a, b] # current state at (a, b)\n",
    "                    if a%2:\n",
    "                        nb = config[(a+1)%N,b] +config[(a+1)%N,(b+1)%N] + config[a,(b+1)%N] + \\\n",
    "                        config[(a-1)%N,b] + config[(a-1)%N,(b+1)%N] + config[a,(b-1)%N]\n",
    "                    else:\n",
    "                        nb = config[(a+1)%N,b] +config[(a+1)%N,(b-1)%N] + config[a,(b+1)%N] + \\\n",
    "                        config[(a-1)%N,b] + config[(a-1)%N,(b-1)%N] + config[a,(b-1)%N]\n",
    "                    \n",
    "                    \n",
    "                    cost = 2*s*nb\n",
    "                    if cost < 0:\t\n",
    "                        s *= -1\n",
    "                    elif rand() < np.exp(-cost*beta):\n",
    "                        s *= -1\n",
    "                    config[a, b] = s\n",
    "        return config\n",
    "    \n",
    "    def simulate(self):   \n",
    "        ''' This module simulates the Ising model'''\n",
    "        config = 2*np.random.randint(2, size=(self.N,self.N))-1   \n",
    "        msrmnt = 81\n",
    "        for i in range(msrmnt):\n",
    "            self.mcmove(config, self.N, 1.0/self.temp)\n",
    "        return config\n",
    "\n",
    "class Ising_sq():\n",
    "    ''' Simulating the Ising model '''  \n",
    "    def __init__(self, size, temp):\n",
    "        self.temp = temp\n",
    "        self.N = int(size)\n",
    "    ## monte carlo moves\n",
    "\n",
    "    def mcmove(self, config, N, beta):\n",
    "        ''' This is to execute the monte carlo moves using \n",
    "        Metropolis algorithm such that detailed\n",
    "        balance condition is satisified'''\n",
    "        for i in range(N):\n",
    "            for j in range(N):            \n",
    "                    a = np.random.randint(0, N) # select a row\n",
    "                    b = np.random.randint(0, N) # select a column\n",
    "                    s =  config[a, b] # current state at (a, b)\n",
    "                    # periodic boundary condition imposed\n",
    "                    nb = config[(a+1)%N,b] + config[a,(b+1)%N] + config[(a-1)%N,b] + config[a,(b-1)%N]\n",
    "                    cost = 2*s*nb\n",
    "                    if cost < 0:\n",
    "                        s *= -1\n",
    "                    elif rand() < np.exp(-cost*beta):\n",
    "                        s *= -1\n",
    "                    config[a, b] = s\n",
    "        return config\n",
    "    \n",
    "    def simulate(self):   \n",
    "        ''' This module simulates the Ising model'''\n",
    "        config = 2*np.random.randint(2, size=(self.N,self.N))-1   \n",
    "        times = 100\n",
    "        for i in range(times):\n",
    "            self.mcmove(config, self.N, 1.0/self.temp)\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import 4-temp data for square and triangular lattices as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:30:37.313572Z",
     "start_time": "2020-02-20T22:30:34.531268Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "nx, ny = 32, 32\n",
    "\n",
    "Xsq = np.ndarray((4*N,nx,ny,1))\n",
    "ysq = np.ndarray(4*N)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    Xsq[i + 0*N] = np.loadtxt(\"./square_T1/square_T1/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 0*N] = 0\n",
    "    Xsq[i + 1*N] = np.loadtxt(\"./square_T2/square_T2/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 1*N] = 1\n",
    "    Xsq[i + 2*N] = np.loadtxt(\"./square_T3/square_T3/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 2*N] = 2\n",
    "    Xsq[i + 3*N] = np.loadtxt(\"./square_T4/square_T4/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ysq[i + 3*N] = 3\n",
    "\n",
    "Xsq_train, Xsq_test, ysq_train, ysq_test = train_test_split(Xsq, ysq, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:31:02.786262Z",
     "start_time": "2020-02-20T22:31:00.698414Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "nx, ny = 32, 32\n",
    "\n",
    "Xtri = np.ndarray((4*N,nx,ny,1))\n",
    "ytri = np.ndarray(4*N)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    Xtri[i + 0*N] = np.loadtxt(\"./triangle_T1/triangle_T1/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 0*N] = 0\n",
    "    Xtri[i + 1*N] = np.loadtxt(\"./triangle_T2/triangle_T2/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 1*N] = 1\n",
    "    Xtri[i + 2*N] = np.loadtxt(\"./triangle_T3/triangle_T3/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 2*N] = 2\n",
    "    Xtri[i + 3*N] = np.loadtxt(\"./triangle_T4/triangle_T4/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "    ytri[i + 3*N] = 3\n",
    "\n",
    "Xtri_train, Xtri_test, ytri_train, ytri_test = train_test_split(Xtri, ytri, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you know the shape of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T22:31:05.870456Z",
     "start_time": "2020-02-20T22:31:05.864585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:\n",
      "(800, 32, 32, 1) (800, 32, 32, 1)\n",
      "(800,) (800,)\n",
      "Shape of test data:\n",
      "(200, 32, 32, 1) (200, 32, 32, 1)\n",
      "(200,) (200,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data:\")\n",
    "print(Xsq_train.shape, Xtri_train.shape)\n",
    "print(ysq_train.shape, ytri_train.shape)\n",
    "print(\"Shape of test data:\")\n",
    "print(Xsq_test.shape, Xtri_test.shape)\n",
    "print(ysq_test.shape, ytri_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Train a fully connected neural network to do the classification on both datasets. Then, train  a  convolutional  neural  network  to  do  the  classification,  on  both datasets.   Make  a  table  of  your  performance  numbers  for  both  models  and  upload  these  numbers.   This,  together  with  your code,  should be uploaded to the course website when you turn in your homework.\n",
    "\n",
    "The temperatures for square lattice are $T = 1.5, 2.1, 2.4, 3.5$. $T = 2.5, 3.2, 3.8, 5$ for triangle lattice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class small_FNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(width, height, depth, num_classes, channels_first=False):\n",
    "        model = Sequential()\n",
    "\n",
    "        if channels_first == False:\n",
    "            inputShape = (height, width, depth)\n",
    "            chanDim = -1\n",
    "        elif channels_first == True:\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(256,  activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(128,  activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_model, train_data, train_lbls, test_data, \n",
    "                test_lbls, num_classes, input_shape, hyperparams):\n",
    "    # Ensure data is shaped properly, assumes channels last set up\n",
    "    x_train = train_data\n",
    "    x_test = test_data\n",
    "    \n",
    "    # Create categorical labels\n",
    "    y_train = keras.utils.to_categorical(train_lbls, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_lbls, num_classes)\n",
    "     \n",
    "    # Set hyperparameters\n",
    "    INIT_LR = hyperparams[0]# learning rate\n",
    "    EPOCHS = hyperparams[1] # number of epochs\n",
    "    BS = hyperparams[2] # batch size\n",
    "    OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "    \n",
    "    model = input_model\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=OPT, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    H = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)\n",
    "    \n",
    "    return H, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.9949 - accuracy: 0.2450 - val_loss: 1.7092 - val_accuracy: 0.2550\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.6895 - accuracy: 0.3088 - val_loss: 1.5665 - val_accuracy: 0.2800\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 113us/sample - loss: 1.5772 - accuracy: 0.3175 - val_loss: 1.5404 - val_accuracy: 0.2500\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 1.4693 - accuracy: 0.3663 - val_loss: 1.4915 - val_accuracy: 0.3050\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.3830 - accuracy: 0.3850 - val_loss: 1.4908 - val_accuracy: 0.3550\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 1.3456 - accuracy: 0.4150 - val_loss: 1.4304 - val_accuracy: 0.3550\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 1.3381 - accuracy: 0.4025 - val_loss: 1.3418 - val_accuracy: 0.4150\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.2902 - accuracy: 0.4475 - val_loss: 1.2973 - val_accuracy: 0.4050\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.2369 - accuracy: 0.4675 - val_loss: 1.2493 - val_accuracy: 0.4550\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.2172 - accuracy: 0.4600 - val_loss: 1.2223 - val_accuracy: 0.4600\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.2310 - accuracy: 0.4588 - val_loss: 1.1631 - val_accuracy: 0.4700\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.1364 - accuracy: 0.4938 - val_loss: 1.1307 - val_accuracy: 0.4750\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.1431 - accuracy: 0.5213 - val_loss: 1.1261 - val_accuracy: 0.5100\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.1129 - accuracy: 0.5362 - val_loss: 1.1098 - val_accuracy: 0.5100\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.0759 - accuracy: 0.5300 - val_loss: 1.0779 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 1.0832 - accuracy: 0.5250 - val_loss: 1.0719 - val_accuracy: 0.5050\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 1.0205 - accuracy: 0.5775 - val_loss: 1.0582 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.9829 - accuracy: 0.5788 - val_loss: 1.0392 - val_accuracy: 0.5000\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.9759 - accuracy: 0.5650 - val_loss: 1.0210 - val_accuracy: 0.5050\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.9628 - accuracy: 0.5950 - val_loss: 1.0157 - val_accuracy: 0.5150\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.9264 - accuracy: 0.6288 - val_loss: 1.0075 - val_accuracy: 0.5050\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.8797 - accuracy: 0.6513 - val_loss: 0.9947 - val_accuracy: 0.5350\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.9001 - accuracy: 0.6187 - val_loss: 0.9794 - val_accuracy: 0.5300\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.8756 - accuracy: 0.6513 - val_loss: 0.9789 - val_accuracy: 0.5450\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.7967 - accuracy: 0.6913 - val_loss: 0.9676 - val_accuracy: 0.5550\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.7992 - accuracy: 0.6625 - val_loss: 0.9617 - val_accuracy: 0.5450\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.7950 - accuracy: 0.6900 - val_loss: 0.9584 - val_accuracy: 0.5350\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.7475 - accuracy: 0.6913 - val_loss: 0.9604 - val_accuracy: 0.5450\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.7455 - accuracy: 0.7188 - val_loss: 0.9447 - val_accuracy: 0.5500\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.7299 - accuracy: 0.7125 - val_loss: 0.9318 - val_accuracy: 0.5550\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.7302 - accuracy: 0.7225 - val_loss: 0.9417 - val_accuracy: 0.5500\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.7117 - accuracy: 0.7362 - val_loss: 0.9411 - val_accuracy: 0.5450\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.7000 - accuracy: 0.7262 - val_loss: 0.9473 - val_accuracy: 0.5450\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.6934 - accuracy: 0.7275 - val_loss: 0.9361 - val_accuracy: 0.5350\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.6616 - accuracy: 0.7362 - val_loss: 0.9497 - val_accuracy: 0.5300\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.6014 - accuracy: 0.7788 - val_loss: 0.9520 - val_accuracy: 0.5400\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.6640 - accuracy: 0.7538 - val_loss: 0.9649 - val_accuracy: 0.5400\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.6230 - accuracy: 0.7675 - val_loss: 0.9527 - val_accuracy: 0.5650\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.6045 - accuracy: 0.7862 - val_loss: 0.9474 - val_accuracy: 0.5450\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.5718 - accuracy: 0.7800 - val_loss: 0.9312 - val_accuracy: 0.5400\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.6242 - accuracy: 0.7738 - val_loss: 0.9554 - val_accuracy: 0.5200\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.5454 - accuracy: 0.8100 - val_loss: 0.9624 - val_accuracy: 0.5350\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.5708 - accuracy: 0.7962 - val_loss: 0.9552 - val_accuracy: 0.5400\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.5646 - accuracy: 0.7987 - val_loss: 0.9424 - val_accuracy: 0.5450\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.5306 - accuracy: 0.8213 - val_loss: 0.9425 - val_accuracy: 0.5250\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.5306 - accuracy: 0.7987 - val_loss: 0.9419 - val_accuracy: 0.5500\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.5379 - accuracy: 0.8050 - val_loss: 0.9595 - val_accuracy: 0.5500\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.5065 - accuracy: 0.8175 - val_loss: 0.9574 - val_accuracy: 0.5500\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 0.4775 - accuracy: 0.8313 - val_loss: 0.9469 - val_accuracy: 0.5700\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 0.4692 - accuracy: 0.8350 - val_loss: 0.9591 - val_accuracy: 0.5300\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.6952 - accuracy: 0.2537 - val_loss: 1.4078 - val_accuracy: 0.2250\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.5224 - accuracy: 0.3137 - val_loss: 1.3956 - val_accuracy: 0.2300\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.4165 - accuracy: 0.3475 - val_loss: 1.3547 - val_accuracy: 0.3250\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.3200 - accuracy: 0.3875 - val_loss: 1.2964 - val_accuracy: 0.3550\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.2653 - accuracy: 0.3988 - val_loss: 1.2554 - val_accuracy: 0.4100\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 108us/sample - loss: 1.2368 - accuracy: 0.4150 - val_loss: 1.2238 - val_accuracy: 0.3900\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 1.1849 - accuracy: 0.4500 - val_loss: 1.1675 - val_accuracy: 0.4400\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 1.1646 - accuracy: 0.4688 - val_loss: 1.1436 - val_accuracy: 0.4800\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 1.1024 - accuracy: 0.4975 - val_loss: 1.1244 - val_accuracy: 0.4550\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 1.0773 - accuracy: 0.5025 - val_loss: 1.0957 - val_accuracy: 0.5050\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 1.0443 - accuracy: 0.5200 - val_loss: 1.0675 - val_accuracy: 0.4800\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 1.0439 - accuracy: 0.5063 - val_loss: 1.0512 - val_accuracy: 0.4950\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 129us/sample - loss: 1.0282 - accuracy: 0.5100 - val_loss: 1.0425 - val_accuracy: 0.5050\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.9894 - accuracy: 0.5550 - val_loss: 1.0143 - val_accuracy: 0.5150\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 1.0028 - accuracy: 0.5188 - val_loss: 1.0026 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.9483 - accuracy: 0.5800 - val_loss: 0.9839 - val_accuracy: 0.5050\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.9442 - accuracy: 0.5575 - val_loss: 0.9659 - val_accuracy: 0.5150\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 109us/sample - loss: 0.9343 - accuracy: 0.5688 - val_loss: 0.9561 - val_accuracy: 0.5450\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 108us/sample - loss: 0.9079 - accuracy: 0.5913 - val_loss: 0.9499 - val_accuracy: 0.5350\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 0.8713 - accuracy: 0.6037 - val_loss: 0.9488 - val_accuracy: 0.5400\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.8813 - accuracy: 0.6062 - val_loss: 0.9319 - val_accuracy: 0.5500\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.8672 - accuracy: 0.6288 - val_loss: 0.9212 - val_accuracy: 0.5400\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.8450 - accuracy: 0.6250 - val_loss: 0.9052 - val_accuracy: 0.5500\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.8536 - accuracy: 0.6175 - val_loss: 0.9014 - val_accuracy: 0.5500\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.8275 - accuracy: 0.6288 - val_loss: 0.8930 - val_accuracy: 0.5350\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.8053 - accuracy: 0.6338 - val_loss: 0.8971 - val_accuracy: 0.5350\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.7858 - accuracy: 0.6438 - val_loss: 0.9090 - val_accuracy: 0.5500\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.7577 - accuracy: 0.6900 - val_loss: 0.9010 - val_accuracy: 0.5700\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.7780 - accuracy: 0.6662 - val_loss: 0.9003 - val_accuracy: 0.5500\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 95us/sample - loss: 0.7250 - accuracy: 0.7075 - val_loss: 0.8914 - val_accuracy: 0.5500\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.7514 - accuracy: 0.6700 - val_loss: 0.8954 - val_accuracy: 0.5550\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.7191 - accuracy: 0.6862 - val_loss: 0.8981 - val_accuracy: 0.5450\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.7094 - accuracy: 0.7050 - val_loss: 0.9039 - val_accuracy: 0.5700\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 0.7272 - accuracy: 0.6888 - val_loss: 0.8997 - val_accuracy: 0.5450\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.6562 - accuracy: 0.7262 - val_loss: 0.8981 - val_accuracy: 0.5350\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.6919 - accuracy: 0.6950 - val_loss: 0.8793 - val_accuracy: 0.5550\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.6718 - accuracy: 0.6938 - val_loss: 0.8864 - val_accuracy: 0.5600\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 0.6544 - accuracy: 0.7150 - val_loss: 0.8840 - val_accuracy: 0.5700\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.6397 - accuracy: 0.7412 - val_loss: 0.8864 - val_accuracy: 0.5550\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.6217 - accuracy: 0.7400 - val_loss: 0.8984 - val_accuracy: 0.5500\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 0.6421 - accuracy: 0.7013 - val_loss: 0.8919 - val_accuracy: 0.5450\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 0.6231 - accuracy: 0.7163 - val_loss: 0.9036 - val_accuracy: 0.5200\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 103us/sample - loss: 0.5993 - accuracy: 0.7475 - val_loss: 0.8946 - val_accuracy: 0.5250\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.6163 - accuracy: 0.7337 - val_loss: 0.9067 - val_accuracy: 0.5450\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.5911 - accuracy: 0.7487 - val_loss: 0.9121 - val_accuracy: 0.5250\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.6308 - accuracy: 0.7188 - val_loss: 0.9070 - val_accuracy: 0.5150\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 101us/sample - loss: 0.5885 - accuracy: 0.7563 - val_loss: 0.9112 - val_accuracy: 0.5450\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.5716 - accuracy: 0.7525 - val_loss: 0.9143 - val_accuracy: 0.5400\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.5764 - accuracy: 0.7337 - val_loss: 0.9193 - val_accuracy: 0.5200\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 0.5618 - accuracy: 0.7500 - val_loss: 0.9143 - val_accuracy: 0.5300\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "fnn_model_sq = small_FNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "fnn_model_tri = small_FNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "\n",
    "# hyperparams should be a tuple of: INIT_LR, EPOCHS, BS\n",
    "FNN_hyperparams = (0.01, 50, 32)\n",
    "H_sq_FNN, sq_FNN_model = train_model(fnn_model_sq, Xsq_train, ysq_train, Xsq_test, ysq_test, num_classes, input_shape, FNN_hyperparams)\n",
    "H_tri_FNN, tri_FNN_model = train_model(fnn_model_tri, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, FNN_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Train a convolutional neural network to do the classification, on both datasets. Make a table of your performance numbers for (a) and (b). \n",
    "Try to optimize the performance of your models and compare the result.\n",
    "\n",
    "solution to (b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(width, height, depth, num_classes, channels_first=False):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        \n",
    "        if channels_first == False:\n",
    "            inputShape = (height, width, depth)\n",
    "            chanDim = -1\n",
    "        elif channels_first == True:\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        model.add(Conv2D(32, (5, 5), activation='relu', input_shape=inputShape))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "                  \n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 1.3768 - accuracy: 0.3050 - val_loss: 1.3112 - val_accuracy: 0.3200\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 1.2189 - accuracy: 0.4938 - val_loss: 1.0972 - val_accuracy: 0.3950\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 73us/sample - loss: 0.9192 - accuracy: 0.6250 - val_loss: 1.0112 - val_accuracy: 0.4950\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.9485 - accuracy: 0.6125 - val_loss: 0.6330 - val_accuracy: 0.7550\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.6851 - accuracy: 0.7013 - val_loss: 0.5666 - val_accuracy: 0.7200\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.5612 - accuracy: 0.7638 - val_loss: 0.9801 - val_accuracy: 0.5250\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.6339 - accuracy: 0.7387 - val_loss: 0.4882 - val_accuracy: 0.7800\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.5796 - accuracy: 0.7563 - val_loss: 0.4918 - val_accuracy: 0.7850\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 0.3834 - accuracy: 0.8325 - val_loss: 0.4098 - val_accuracy: 0.8450\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.3553 - accuracy: 0.8550 - val_loss: 1.7150 - val_accuracy: 0.5550\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.6273 - accuracy: 0.7837 - val_loss: 0.5076 - val_accuracy: 0.7650\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.3333 - accuracy: 0.8775 - val_loss: 0.3561 - val_accuracy: 0.8450\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 73us/sample - loss: 0.3178 - accuracy: 0.8700 - val_loss: 0.3292 - val_accuracy: 0.8700\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.2911 - accuracy: 0.8913 - val_loss: 0.4312 - val_accuracy: 0.8100\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.2604 - accuracy: 0.8963 - val_loss: 0.4436 - val_accuracy: 0.8050\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 71us/sample - loss: 0.4154 - accuracy: 0.8475 - val_loss: 0.3182 - val_accuracy: 0.8650\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.2272 - accuracy: 0.9225 - val_loss: 0.2874 - val_accuracy: 0.8750\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.2573 - accuracy: 0.8963 - val_loss: 0.3502 - val_accuracy: 0.8450\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.2313 - accuracy: 0.9187 - val_loss: 0.3076 - val_accuracy: 0.8700\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.2604 - accuracy: 0.8900 - val_loss: 0.2799 - val_accuracy: 0.8650\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.2459 - accuracy: 0.9162 - val_loss: 0.2576 - val_accuracy: 0.8800\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.2202 - accuracy: 0.9075 - val_loss: 0.2328 - val_accuracy: 0.9000\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.3041 - accuracy: 0.8725 - val_loss: 0.4315 - val_accuracy: 0.8050\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.2124 - accuracy: 0.9200 - val_loss: 0.2559 - val_accuracy: 0.8750\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1707 - accuracy: 0.9413 - val_loss: 0.2476 - val_accuracy: 0.8800\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.1917 - accuracy: 0.9287 - val_loss: 0.2047 - val_accuracy: 0.9150\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.1973 - accuracy: 0.9262 - val_loss: 0.3634 - val_accuracy: 0.8450\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.2042 - accuracy: 0.9237 - val_loss: 0.2281 - val_accuracy: 0.9050\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.1823 - accuracy: 0.9300 - val_loss: 0.2046 - val_accuracy: 0.9200\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1761 - accuracy: 0.9375 - val_loss: 0.1922 - val_accuracy: 0.9200\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.1847 - accuracy: 0.9350 - val_loss: 0.1951 - val_accuracy: 0.9200\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.2294 - accuracy: 0.9087 - val_loss: 0.1973 - val_accuracy: 0.9250\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.1558 - accuracy: 0.9475 - val_loss: 0.1931 - val_accuracy: 0.9000\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 78us/sample - loss: 0.1589 - accuracy: 0.9400 - val_loss: 0.2041 - val_accuracy: 0.9250\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1523 - accuracy: 0.9450 - val_loss: 0.5475 - val_accuracy: 0.7850\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.2231 - accuracy: 0.9150 - val_loss: 0.1973 - val_accuracy: 0.8850\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.1391 - accuracy: 0.9538 - val_loss: 0.2074 - val_accuracy: 0.9100\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 0.1507 - accuracy: 0.9500 - val_loss: 0.1667 - val_accuracy: 0.9050\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 0.1489 - accuracy: 0.9425 - val_loss: 0.1916 - val_accuracy: 0.9150\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 95us/sample - loss: 0.1556 - accuracy: 0.9375 - val_loss: 0.1611 - val_accuracy: 0.9050\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 95us/sample - loss: 0.1342 - accuracy: 0.9513 - val_loss: 0.2218 - val_accuracy: 0.9100\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 92us/sample - loss: 0.1293 - accuracy: 0.9538 - val_loss: 0.1549 - val_accuracy: 0.9200\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1332 - accuracy: 0.9575 - val_loss: 0.1883 - val_accuracy: 0.9050\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 74us/sample - loss: 0.1307 - accuracy: 0.9538 - val_loss: 0.5812 - val_accuracy: 0.7750\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.1557 - accuracy: 0.9450 - val_loss: 0.1419 - val_accuracy: 0.9400\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1064 - accuracy: 0.9700 - val_loss: 0.1867 - val_accuracy: 0.9100\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.1240 - accuracy: 0.9538 - val_loss: 0.1418 - val_accuracy: 0.9500\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.1309 - accuracy: 0.9413 - val_loss: 0.1455 - val_accuracy: 0.9350\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1144 - accuracy: 0.9613 - val_loss: 0.2117 - val_accuracy: 0.9000\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1357 - accuracy: 0.9525 - val_loss: 0.2112 - val_accuracy: 0.9000\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 0s 483us/sample - loss: 1.3736 - accuracy: 0.2587 - val_loss: 1.3051 - val_accuracy: 0.2800\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 71us/sample - loss: 1.2703 - accuracy: 0.3738 - val_loss: 1.1722 - val_accuracy: 0.4600\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 73us/sample - loss: 1.0728 - accuracy: 0.5225 - val_loss: 1.0771 - val_accuracy: 0.3450\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 72us/sample - loss: 0.9942 - accuracy: 0.5387 - val_loss: 0.8769 - val_accuracy: 0.5250\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.7803 - accuracy: 0.6363 - val_loss: 0.6475 - val_accuracy: 0.6350\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 83us/sample - loss: 0.7510 - accuracy: 0.6538 - val_loss: 0.6622 - val_accuracy: 0.6850\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.6515 - accuracy: 0.6737 - val_loss: 0.6132 - val_accuracy: 0.6750\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.6175 - accuracy: 0.6825 - val_loss: 0.4350 - val_accuracy: 0.8250\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.5530 - accuracy: 0.7487 - val_loss: 1.1766 - val_accuracy: 0.4500\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.5309 - accuracy: 0.7563 - val_loss: 0.4078 - val_accuracy: 0.8450\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.3915 - accuracy: 0.8400 - val_loss: 0.4559 - val_accuracy: 0.7850\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 0.3865 - accuracy: 0.8450 - val_loss: 0.3322 - val_accuracy: 0.8650\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 0.5032 - accuracy: 0.7713 - val_loss: 1.3733 - val_accuracy: 0.3750\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 118us/sample - loss: 0.4520 - accuracy: 0.8025 - val_loss: 0.3910 - val_accuracy: 0.8700\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 0.3435 - accuracy: 0.8562 - val_loss: 0.3772 - val_accuracy: 0.8700\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.3266 - accuracy: 0.8712 - val_loss: 0.3694 - val_accuracy: 0.8600\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 0.3277 - accuracy: 0.8637 - val_loss: 0.3032 - val_accuracy: 0.9000\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 0.3114 - accuracy: 0.8712 - val_loss: 0.7886 - val_accuracy: 0.6350\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 0.4942 - accuracy: 0.7875 - val_loss: 0.4849 - val_accuracy: 0.7850\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.3167 - accuracy: 0.8788 - val_loss: 0.4789 - val_accuracy: 0.7800\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.3380 - accuracy: 0.8675 - val_loss: 0.2969 - val_accuracy: 0.9000\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.2782 - accuracy: 0.8963 - val_loss: 0.2415 - val_accuracy: 0.9300\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.2833 - accuracy: 0.8788 - val_loss: 0.3631 - val_accuracy: 0.8700\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 79us/sample - loss: 0.2768 - accuracy: 0.8938 - val_loss: 0.4003 - val_accuracy: 0.8400\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.2597 - accuracy: 0.9062 - val_loss: 0.2160 - val_accuracy: 0.9350\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.3339 - accuracy: 0.8537 - val_loss: 0.2126 - val_accuracy: 0.9400\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.2444 - accuracy: 0.9000 - val_loss: 0.2101 - val_accuracy: 0.9350\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 94us/sample - loss: 0.2105 - accuracy: 0.9250 - val_loss: 0.2373 - val_accuracy: 0.9400\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.2158 - accuracy: 0.9187 - val_loss: 0.2303 - val_accuracy: 0.8950\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.2074 - accuracy: 0.9237 - val_loss: 0.2416 - val_accuracy: 0.9150\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 89us/sample - loss: 0.2163 - accuracy: 0.9075 - val_loss: 0.1694 - val_accuracy: 0.9550\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 104us/sample - loss: 0.2417 - accuracy: 0.9062 - val_loss: 0.1984 - val_accuracy: 0.9250\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1690 - accuracy: 0.9388 - val_loss: 0.1760 - val_accuracy: 0.9300\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 71us/sample - loss: 0.2544 - accuracy: 0.8838 - val_loss: 0.1807 - val_accuracy: 0.9500\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.1532 - accuracy: 0.9525 - val_loss: 0.1719 - val_accuracy: 0.9400\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 76us/sample - loss: 0.1829 - accuracy: 0.9350 - val_loss: 0.2235 - val_accuracy: 0.9100\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.2219 - accuracy: 0.9050 - val_loss: 0.1696 - val_accuracy: 0.9550\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 77us/sample - loss: 0.1452 - accuracy: 0.9538 - val_loss: 0.1609 - val_accuracy: 0.9550\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 75us/sample - loss: 0.1506 - accuracy: 0.9425 - val_loss: 0.2326 - val_accuracy: 0.8950\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 80us/sample - loss: 0.1614 - accuracy: 0.9475 - val_loss: 0.1575 - val_accuracy: 0.9600\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 81us/sample - loss: 0.1342 - accuracy: 0.9538 - val_loss: 0.1491 - val_accuracy: 0.9600\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 0.1714 - accuracy: 0.9312 - val_loss: 0.1560 - val_accuracy: 0.9650\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.1521 - accuracy: 0.9362 - val_loss: 0.1591 - val_accuracy: 0.9400\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.1796 - accuracy: 0.9325 - val_loss: 0.1427 - val_accuracy: 0.9600\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.1389 - accuracy: 0.9563 - val_loss: 0.1611 - val_accuracy: 0.9400\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.1155 - accuracy: 0.9563 - val_loss: 0.1326 - val_accuracy: 0.9500\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 86us/sample - loss: 0.1414 - accuracy: 0.9463 - val_loss: 0.1276 - val_accuracy: 0.9650\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.1512 - accuracy: 0.9438 - val_loss: 0.1243 - val_accuracy: 0.9600\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 0.0980 - accuracy: 0.9675 - val_loss: 0.1254 - val_accuracy: 0.9600\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 0.1773 - accuracy: 0.9400 - val_loss: 0.1604 - val_accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "cnn_model_sq = SimpleCNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "cnn_model_tri = SimpleCNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "\n",
    "CNN_hyperparams = (0.01, 50, 64)\n",
    "H_sq_CNN, sq_CNN_model = train_model(cnn_model_sq, Xsq_train, ysq_train, Xsq_test, ysq_test, num_classes, input_shape, CNN_hyperparams)\n",
    "H_tri_CNN, tri_CNN_model = train_model(cnn_model_tri, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, CNN_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) We have provided a test set of 10 spins configurations for each of the two problems. Each of the spin configurations is not necessarily at the temperatures of the training sets. Calculate your best estimate of the temperatures of these spin configuration. Upload your results to Kaggle.\n",
    "[Hint: A direct fingerprint of temperature is the distribution of spin up\n",
    "and down, because you can image that the spins fluctuate more violently\n",
    "at higher temperature. Although the mothod you use in homework 2 can also work, you may be interested in trying to take distribution into account when you\n",
    "build the model to estimate temperature and see if you can make use of this extra information. This may help you win the\n",
    "kaggle. It is totally fine if you find that the information of distribution is not helpful. Note also that a CNN kind-of does this. One possibility is that you may want a CNN that captures enough distribution information.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsq_sim_data = []\\ntri_sim_data = []\\nsim_data_temp = []\\n\\nfor temp in temps_vec:    \\n    sq_ising_simu = Ising_sq(32, temp)\\n    tri_ising_simu = Ising_tri(32, temp)\\n    sq_img = sq_ising_simu.simulate()\\n    tri_img = tri_ising_simu.simulate()\\n    \\n    sq_sim_data.append(sq_img)\\n    tri_sim_data.append(tri_img)\\n    \\ncwd = str(os.getcwd())\\nsim_data_path_2 = cwd+\"/sim_data_2.npy\"\\nsim_data_temp_path_2 = cwd+\"/sim_data_temp_2.npy\"\\nsim_data_tot_lv_mean_path = cwd+\"/sim_data_tot_lv_mean.npy\"\\n\\nnp.save(sq_sim_data_path, np.asarray(sq_sim_data))\\nnp.save(tri_sim_data_path, np.asarray(tri_sim_data))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = str(os.getcwd())\n",
    "sq_sim_data_path = cwd+\"/sq_sim_data.npy\"\n",
    "tri_sim_data_path = cwd+\"/tri_sim_data.npy\"\n",
    "\n",
    "temps_vec = np.linspace(0.01,15,num=1500)\n",
    "\n",
    "\"\"\"\n",
    "sq_sim_data = []\n",
    "tri_sim_data = []\n",
    "sim_data_temp = []\n",
    "\n",
    "for temp in temps_vec:    \n",
    "    sq_ising_simu = Ising_sq(32, temp)\n",
    "    tri_ising_simu = Ising_tri(32, temp)\n",
    "    sq_img = sq_ising_simu.simulate()\n",
    "    tri_img = tri_ising_simu.simulate()\n",
    "    \n",
    "    sq_sim_data.append(sq_img)\n",
    "    tri_sim_data.append(tri_img)\n",
    "    \n",
    "cwd = str(os.getcwd())\n",
    "sim_data_path_2 = cwd+\"/sim_data_2.npy\"\n",
    "sim_data_temp_path_2 = cwd+\"/sim_data_temp_2.npy\"\n",
    "sim_data_tot_lv_mean_path = cwd+\"/sim_data_tot_lv_mean.npy\"\n",
    "\n",
    "np.save(sq_sim_data_path, np.asarray(sq_sim_data))\n",
    "np.save(tri_sim_data_path, np.asarray(tri_sim_data))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data ready to be embedded\n",
    "sq_sim_img = np.load(sq_sim_data_path).reshape(1500,32,32,-1)\n",
    "tri_sim_img = np.load(tri_sim_data_path).reshape(1500,32,32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_sim_img_rs, tri_sim_img_rs = np.repeat(sq_sim_img, 5, 1), np.repeat(tri_sim_img, 5, 1)\n",
    "sq_sim_img_rs, tri_sim_img_rs = np.repeat(sq_sim_img_rs, 5, 2), np.repeat(tri_sim_img_rs, 5, 2)\n",
    "sq_sim_img_rs, tri_sim_img_rs = np.repeat(sq_sim_img_rs, 3, 3), np.repeat(tri_sim_img_rs, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pre-trained MobileNetV2 \n",
    "with tf.device('/CPU:0'):\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=(160,160,3),\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet',\n",
    "                                                   classes=4)\n",
    "    base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedded data\n",
    "global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "sq_sim_data_emb = global_avg_layer(base_model.predict(sq_sim_img_rs))\n",
    "sq_sim_data_emb = sq_sim_data_emb.numpy()/sq_sim_data_emb.numpy().max()\n",
    "\n",
    "tri_sim_data_emb = global_avg_layer(base_model.predict(tri_sim_img_rs))\n",
    "tri_sim_data_emb = tri_sim_data_emb.numpy()/tri_sim_data_emb.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test splits\n",
    "sq_x_train, sq_x_test, sq_y_train, sq_y_test = train_test_split(sq_sim_data_emb, temps_vec, test_size=0.2, random_state=0)\n",
    "tri_x_train, tri_x_test, tri_y_train, tri_y_test = train_test_split(tri_sim_data_emb, temps_vec, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class RegressionHead:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(channels_first=False):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(612, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "1200/1200 [==============================] - 0s 350us/sample - loss: 54.8917 - MeanSquaredError: 54.8917 - val_loss: 53.3974 - val_MeanSquaredError: 53.3974\n",
      "Epoch 2/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 41.4913 - MeanSquaredError: 41.4913 - val_loss: 59.1502 - val_MeanSquaredError: 59.1502\n",
      "Epoch 3/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 32.6003 - MeanSquaredError: 32.6003 - val_loss: 26.9313 - val_MeanSquaredError: 26.9313\n",
      "Epoch 4/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 25.2280 - MeanSquaredError: 25.2280 - val_loss: 36.5285 - val_MeanSquaredError: 36.5285\n",
      "Epoch 5/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 19.8566 - MeanSquaredError: 19.8566 - val_loss: 19.2372 - val_MeanSquaredError: 19.2372\n",
      "Epoch 6/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 15.2053 - MeanSquaredError: 15.2053 - val_loss: 14.9686 - val_MeanSquaredError: 14.9686\n",
      "Epoch 7/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 12.0966 - MeanSquaredError: 12.0966 - val_loss: 9.2823 - val_MeanSquaredError: 9.2823\n",
      "Epoch 8/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 9.9749 - MeanSquaredError: 9.9749 - val_loss: 4.7872 - val_MeanSquaredError: 4.7872\n",
      "Epoch 9/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 8.3611 - MeanSquaredError: 8.3611 - val_loss: 12.6164 - val_MeanSquaredError: 12.6164\n",
      "Epoch 10/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 7.5618 - MeanSquaredError: 7.5618 - val_loss: 4.2778 - val_MeanSquaredError: 4.2778\n",
      "Epoch 11/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 6.7499 - MeanSquaredError: 6.7499 - val_loss: 4.3698 - val_MeanSquaredError: 4.3698\n",
      "Epoch 12/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 6.1221 - MeanSquaredError: 6.1221 - val_loss: 4.2860 - val_MeanSquaredError: 4.2860\n",
      "Epoch 13/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 5.1573 - MeanSquaredError: 5.1573 - val_loss: 3.9314 - val_MeanSquaredError: 3.9314\n",
      "Epoch 14/50\n",
      "1200/1200 [==============================] - 0s 83us/sample - loss: 5.4045 - MeanSquaredError: 5.4045 - val_loss: 3.1213 - val_MeanSquaredError: 3.1213\n",
      "Epoch 15/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 4.9143 - MeanSquaredError: 4.9143 - val_loss: 4.2971 - val_MeanSquaredError: 4.2971\n",
      "Epoch 16/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 4.8278 - MeanSquaredError: 4.8278 - val_loss: 3.6304 - val_MeanSquaredError: 3.6304\n",
      "Epoch 17/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 4.7040 - MeanSquaredError: 4.7040 - val_loss: 3.1437 - val_MeanSquaredError: 3.1437\n",
      "Epoch 18/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.7385 - MeanSquaredError: 4.7385 - val_loss: 3.2105 - val_MeanSquaredError: 3.2105\n",
      "Epoch 19/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.5323 - MeanSquaredError: 4.5323 - val_loss: 3.0721 - val_MeanSquaredError: 3.0721\n",
      "Epoch 20/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.5059 - MeanSquaredError: 4.5059 - val_loss: 2.9838 - val_MeanSquaredError: 2.9838\n",
      "Epoch 21/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 4.4536 - MeanSquaredError: 4.4536 - val_loss: 3.3223 - val_MeanSquaredError: 3.3223\n",
      "Epoch 22/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.4486 - MeanSquaredError: 4.4486 - val_loss: 4.0058 - val_MeanSquaredError: 4.0058\n",
      "Epoch 23/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.1930 - MeanSquaredError: 4.1930 - val_loss: 3.5299 - val_MeanSquaredError: 3.5299\n",
      "Epoch 24/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.1989 - MeanSquaredError: 4.1989 - val_loss: 3.0066 - val_MeanSquaredError: 3.0066\n",
      "Epoch 25/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 4.0975 - MeanSquaredError: 4.0975 - val_loss: 3.1873 - val_MeanSquaredError: 3.1873\n",
      "Epoch 26/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 4.0803 - MeanSquaredError: 4.0803 - val_loss: 4.1332 - val_MeanSquaredError: 4.1332\n",
      "Epoch 27/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 4.0499 - MeanSquaredError: 4.0499 - val_loss: 3.5525 - val_MeanSquaredError: 3.5525\n",
      "Epoch 28/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.4607 - MeanSquaredError: 4.4607 - val_loss: 2.9520 - val_MeanSquaredError: 2.9520\n",
      "Epoch 29/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 4.2685 - MeanSquaredError: 4.2686 - val_loss: 3.0515 - val_MeanSquaredError: 3.0515\n",
      "Epoch 30/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.9543 - MeanSquaredError: 3.9543 - val_loss: 3.5369 - val_MeanSquaredError: 3.5369\n",
      "Epoch 31/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.0223 - MeanSquaredError: 4.0223 - val_loss: 3.1590 - val_MeanSquaredError: 3.1590\n",
      "Epoch 32/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 4.3638 - MeanSquaredError: 4.3638 - val_loss: 3.2289 - val_MeanSquaredError: 3.2289\n",
      "Epoch 33/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.9157 - MeanSquaredError: 3.9157 - val_loss: 3.5117 - val_MeanSquaredError: 3.5117\n",
      "Epoch 34/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.7986 - MeanSquaredError: 3.7986 - val_loss: 3.1698 - val_MeanSquaredError: 3.1698\n",
      "Epoch 35/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 4.1123 - MeanSquaredError: 4.1123 - val_loss: 3.3700 - val_MeanSquaredError: 3.3700\n",
      "Epoch 36/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 3.6643 - MeanSquaredError: 3.6643 - val_loss: 2.9886 - val_MeanSquaredError: 2.9886\n",
      "Epoch 37/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.6752 - MeanSquaredError: 3.6752 - val_loss: 3.3336 - val_MeanSquaredError: 3.3336\n",
      "Epoch 38/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.9658 - MeanSquaredError: 3.9658 - val_loss: 3.3760 - val_MeanSquaredError: 3.3760\n",
      "Epoch 39/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 4.1232 - MeanSquaredError: 4.1232 - val_loss: 2.9658 - val_MeanSquaredError: 2.9658\n",
      "Epoch 40/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.8776 - MeanSquaredError: 3.8776 - val_loss: 3.1110 - val_MeanSquaredError: 3.1110\n",
      "Epoch 41/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.7286 - MeanSquaredError: 3.7286 - val_loss: 3.0674 - val_MeanSquaredError: 3.0674\n",
      "Epoch 42/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.5515 - MeanSquaredError: 3.5515 - val_loss: 2.9451 - val_MeanSquaredError: 2.9451\n",
      "Epoch 43/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.4142 - MeanSquaredError: 3.4142 - val_loss: 3.2968 - val_MeanSquaredError: 3.2968\n",
      "Epoch 44/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 3.7762 - MeanSquaredError: 3.7762 - val_loss: 3.0606 - val_MeanSquaredError: 3.0606\n",
      "Epoch 45/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.8991 - MeanSquaredError: 3.8991 - val_loss: 3.1037 - val_MeanSquaredError: 3.1037\n",
      "Epoch 46/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.6385 - MeanSquaredError: 3.6385 - val_loss: 3.1730 - val_MeanSquaredError: 3.1730\n",
      "Epoch 47/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.8716 - MeanSquaredError: 3.8716 - val_loss: 2.9837 - val_MeanSquaredError: 2.9837\n",
      "Epoch 48/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.8525 - MeanSquaredError: 3.8525 - val_loss: 3.0330 - val_MeanSquaredError: 3.0330\n",
      "Epoch 49/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.7489 - MeanSquaredError: 3.7489 - val_loss: 3.9223 - val_MeanSquaredError: 3.9223\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.6139 - MeanSquaredError: 3.6139 - val_loss: 3.0518 - val_MeanSquaredError: 3.0518\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.01# learning rate\n",
    "EPOCHS = 50 # number of epochs\n",
    "BS = 32 # batch size\n",
    "OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "LOSS = keras.losses.MeanSquaredError()\n",
    "\n",
    "sq_reg_model = RegressionHead.build()\n",
    "sq_reg_model.compile(optimizer=OPT, loss=LOSS, metrics=['MeanSquaredError'])\n",
    "sq_reg_H = sq_reg_model.fit(sq_x_train, sq_y_train, validation_data=(sq_x_test, sq_y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "1200/1200 [==============================] - 0s 357us/sample - loss: 53.6751 - MeanSquaredError: 53.6751 - val_loss: 62.8468 - val_MeanSquaredError: 62.8468\n",
      "Epoch 2/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 41.1033 - MeanSquaredError: 41.1033 - val_loss: 45.0915 - val_MeanSquaredError: 45.0915\n",
      "Epoch 3/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 32.9938 - MeanSquaredError: 32.9938 - val_loss: 32.9598 - val_MeanSquaredError: 32.9598\n",
      "Epoch 4/50\n",
      "1200/1200 [==============================] - 0s 86us/sample - loss: 25.7127 - MeanSquaredError: 25.7127 - val_loss: 33.0060 - val_MeanSquaredError: 33.0060\n",
      "Epoch 5/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 19.6843 - MeanSquaredError: 19.6843 - val_loss: 15.9706 - val_MeanSquaredError: 15.9706\n",
      "Epoch 6/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 15.3271 - MeanSquaredError: 15.3271 - val_loss: 7.7773 - val_MeanSquaredError: 7.7773\n",
      "Epoch 7/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 12.0722 - MeanSquaredError: 12.0722 - val_loss: 8.5353 - val_MeanSquaredError: 8.5353\n",
      "Epoch 8/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 9.5408 - MeanSquaredError: 9.5408 - val_loss: 4.9699 - val_MeanSquaredError: 4.9699\n",
      "Epoch 9/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 8.1685 - MeanSquaredError: 8.1685 - val_loss: 5.5288 - val_MeanSquaredError: 5.5288\n",
      "Epoch 10/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 6.9719 - MeanSquaredError: 6.9719 - val_loss: 6.2165 - val_MeanSquaredError: 6.2165\n",
      "Epoch 11/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 5.7679 - MeanSquaredError: 5.7679 - val_loss: 3.1532 - val_MeanSquaredError: 3.1532\n",
      "Epoch 12/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.9029 - MeanSquaredError: 4.9029 - val_loss: 3.4889 - val_MeanSquaredError: 3.4889\n",
      "Epoch 13/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 4.7209 - MeanSquaredError: 4.7209 - val_loss: 3.8418 - val_MeanSquaredError: 3.8418\n",
      "Epoch 14/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 4.3068 - MeanSquaredError: 4.3068 - val_loss: 2.7168 - val_MeanSquaredError: 2.7168\n",
      "Epoch 15/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 4.3820 - MeanSquaredError: 4.3820 - val_loss: 1.8305 - val_MeanSquaredError: 1.8305\n",
      "Epoch 16/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.9771 - MeanSquaredError: 3.9771 - val_loss: 2.9270 - val_MeanSquaredError: 2.9270\n",
      "Epoch 17/50\n",
      "1200/1200 [==============================] - 0s 72us/sample - loss: 3.8379 - MeanSquaredError: 3.8379 - val_loss: 2.0179 - val_MeanSquaredError: 2.0179\n",
      "Epoch 18/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.9035 - MeanSquaredError: 3.9035 - val_loss: 1.9676 - val_MeanSquaredError: 1.9676\n",
      "Epoch 19/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.4993 - MeanSquaredError: 3.4993 - val_loss: 3.1838 - val_MeanSquaredError: 3.1838\n",
      "Epoch 20/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.6971 - MeanSquaredError: 3.6971 - val_loss: 2.0137 - val_MeanSquaredError: 2.0137\n",
      "Epoch 21/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 3.6540 - MeanSquaredError: 3.6540 - val_loss: 1.9217 - val_MeanSquaredError: 1.9217\n",
      "Epoch 22/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.5938 - MeanSquaredError: 3.5938 - val_loss: 2.4421 - val_MeanSquaredError: 2.4421\n",
      "Epoch 23/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.3412 - MeanSquaredError: 3.3412 - val_loss: 1.6721 - val_MeanSquaredError: 1.6721\n",
      "Epoch 24/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.4061 - MeanSquaredError: 3.4061 - val_loss: 2.1990 - val_MeanSquaredError: 2.1990\n",
      "Epoch 25/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.4358 - MeanSquaredError: 3.4358 - val_loss: 1.9567 - val_MeanSquaredError: 1.9567\n",
      "Epoch 26/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.2897 - MeanSquaredError: 3.2897 - val_loss: 1.7944 - val_MeanSquaredError: 1.7944\n",
      "Epoch 27/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.5087 - MeanSquaredError: 3.5087 - val_loss: 2.5610 - val_MeanSquaredError: 2.5610\n",
      "Epoch 28/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.3682 - MeanSquaredError: 3.3682 - val_loss: 2.4213 - val_MeanSquaredError: 2.4213\n",
      "Epoch 29/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.3998 - MeanSquaredError: 3.3998 - val_loss: 1.8244 - val_MeanSquaredError: 1.8244\n",
      "Epoch 30/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.1482 - MeanSquaredError: 3.1482 - val_loss: 1.9586 - val_MeanSquaredError: 1.9586\n",
      "Epoch 31/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.1765 - MeanSquaredError: 3.1765 - val_loss: 1.7258 - val_MeanSquaredError: 1.7258\n",
      "Epoch 32/50\n",
      "1200/1200 [==============================] - 0s 71us/sample - loss: 3.2408 - MeanSquaredError: 3.2408 - val_loss: 1.8788 - val_MeanSquaredError: 1.8788\n",
      "Epoch 33/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 3.3864 - MeanSquaredError: 3.3864 - val_loss: 1.6984 - val_MeanSquaredError: 1.6984\n",
      "Epoch 34/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.2584 - MeanSquaredError: 3.2584 - val_loss: 1.6357 - val_MeanSquaredError: 1.6357\n",
      "Epoch 35/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.2597 - MeanSquaredError: 3.2597 - val_loss: 2.5518 - val_MeanSquaredError: 2.5518\n",
      "Epoch 36/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.3160 - MeanSquaredError: 3.3160 - val_loss: 1.7890 - val_MeanSquaredError: 1.7890\n",
      "Epoch 37/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 3.0757 - MeanSquaredError: 3.0757 - val_loss: 1.6716 - val_MeanSquaredError: 1.6716\n",
      "Epoch 38/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.1689 - MeanSquaredError: 3.1689 - val_loss: 1.7352 - val_MeanSquaredError: 1.7352\n",
      "Epoch 39/50\n",
      "1200/1200 [==============================] - 0s 78us/sample - loss: 3.3159 - MeanSquaredError: 3.3159 - val_loss: 1.8881 - val_MeanSquaredError: 1.8881\n",
      "Epoch 40/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 2.9167 - MeanSquaredError: 2.9167 - val_loss: 1.8256 - val_MeanSquaredError: 1.8256\n",
      "Epoch 41/50\n",
      "1200/1200 [==============================] - 0s 75us/sample - loss: 3.2338 - MeanSquaredError: 3.2338 - val_loss: 1.6207 - val_MeanSquaredError: 1.6207\n",
      "Epoch 42/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 3.0087 - MeanSquaredError: 3.0087 - val_loss: 1.6136 - val_MeanSquaredError: 1.6136\n",
      "Epoch 43/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 2.8927 - MeanSquaredError: 2.8927 - val_loss: 1.6009 - val_MeanSquaredError: 1.6009\n",
      "Epoch 44/50\n",
      "1200/1200 [==============================] - 0s 77us/sample - loss: 3.0971 - MeanSquaredError: 3.0971 - val_loss: 1.7390 - val_MeanSquaredError: 1.7390\n",
      "Epoch 45/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 2.8659 - MeanSquaredError: 2.8659 - val_loss: 1.5985 - val_MeanSquaredError: 1.5985\n",
      "Epoch 46/50\n",
      "1200/1200 [==============================] - 0s 76us/sample - loss: 2.7735 - MeanSquaredError: 2.7735 - val_loss: 1.6287 - val_MeanSquaredError: 1.6287\n",
      "Epoch 47/50\n",
      "1200/1200 [==============================] - 0s 74us/sample - loss: 3.0820 - MeanSquaredError: 3.0820 - val_loss: 2.5458 - val_MeanSquaredError: 2.5458\n",
      "Epoch 48/50\n",
      "1200/1200 [==============================] - 0s 73us/sample - loss: 2.9744 - MeanSquaredError: 2.9744 - val_loss: 1.8360 - val_MeanSquaredError: 1.8360\n",
      "Epoch 49/50\n",
      "1200/1200 [==============================] - 0s 88us/sample - loss: 3.2927 - MeanSquaredError: 3.2927 - val_loss: 1.7428 - val_MeanSquaredError: 1.7428\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 75us/sample - loss: 2.8289 - MeanSquaredError: 2.8289 - val_loss: 1.7063 - val_MeanSquaredError: 1.7063\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.01# learning rate\n",
    "EPOCHS = 50 # number of epochs\n",
    "BS = 32 # batch size\n",
    "OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "LOSS = keras.losses.MeanSquaredError()\n",
    "\n",
    "tri_reg_model = RegressionHead.build()\n",
    "tri_reg_model.compile(optimizer=OPT, loss=LOSS, metrics=['MeanSquaredError'])\n",
    "tri_reg_H = tri_reg_model.fit(tri_x_train, tri_y_train, validation_data=(tri_x_test, tri_y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_sim_data_probas = sq_CNN_model.predict_proba(sq_sim_img)\n",
    "tri_sim_data_probas = sq_CNN_model.predict_proba(tri_sim_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_import(dir):\n",
    "    temp = []\n",
    "    for i in range(len(os.listdir(dir+'/'))):\n",
    "        temp.append(np.loadtxt((dir+'/')+str(i).zfill(3), delimiter=','))\n",
    "    return np.array(temp)\n",
    "\n",
    "def T10_import(dir):\n",
    "    temp = []\n",
    "    for i in range(0,10):\n",
    "        for j in range(0,10):\n",
    "            temp.append(np.loadtxt((dir+'/T0'+str(i)+'#')+str(j).zfill(2), delimiter=','))\n",
    "    return np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_t10_img = T10_import(cwd+'/triangle_10T').reshape(100,32,32,-1)\n",
    "sq_t10_img = T10_import(cwd+'/square_10T').reshape(100,32,32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_t10_img_rs, tri_t10_img_rs = np.repeat(sq_t10_img, 5, 1), np.repeat(tri_t10_img, 5, 1)\n",
    "sq_t10_img_rs, tri_t10_img_rs = np.repeat(sq_t10_img_rs, 5, 2), np.repeat(tri_t10_img_rs, 5, 2)\n",
    "sq_t10_img_rs, tri_t10_img_rs = np.repeat(sq_t10_img_rs, 3, 3), np.repeat(tri_t10_img_rs, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 160, 160, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square_t10_img_rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_t10_data_emb = global_avg_layer(base_model.predict(sq_t10_img_rs))\n",
    "sq_t10_data_emb = sq_t10_data_emb.numpy()/sq_t10_data_emb.numpy().max()\n",
    "\n",
    "tri_t10_data_emb = global_avg_layer(base_model.predict(tri_t10_img_rs))\n",
    "tri_t10_data_emb = tri_t10_data_emb.numpy()/tri_t10_data_emb.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "pca = PCA(n_components=2)\n",
    "tsne_img_emb = tsne.fit_transform(tri_t10_data_emb)\n",
    "pca_img_emb = pca.fit_transform(tri_t10_data_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    plt.plot(tsne_img_emb.transpose()[0],tsne_img_emb.transpose()[1],'.',markersize=12)\n",
    "plt.title(\"TSNE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    plt.plot(pca_img_emb.transpose()[0], pca_img_emb.transpose()[1],'.',markersize=12)\n",
    "plt.title(\"PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_pred_temps = tri_reg_model.predict(tri_t10_data_emb).reshape(10,-1)\n",
    "sq_pred_temps = tri_reg_model.predict(sq_t10_data_emb).reshape(10,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tri_class_probas = tri_CNN_model.predict_proba(tri_t10_img).reshape(10,-1,4)\n",
    "all_sq_class_probas = sq_CNN_model.predict_proba(sq_t10_img).reshape(10,-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tri_class_probas = np.mean(tri_class_probas, axis=1)\n",
    "avg_sq_class_probas = np.mean(sq_class_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "tri_temp_list = []\n",
    "sq_temp_list = []\n",
    "\n",
    "for proba in avg_tri_class_probas:\n",
    "    dist_list = []\n",
    "    for proba_2 in tri_sim_data_probas:\n",
    "        dist_list.append(distance.euclidean(proba, proba_2))\n",
    "    max_idx = dist_list.index(min(dist_list))\n",
    "    tri_temp_list.append(temps_vec[max_idx])\n",
    "    \n",
    "for proba in avg_sq_class_probas:\n",
    "    dist_list = []\n",
    "    for proba_2 in sq_sim_data_probas:\n",
    "        dist_list.append(distance.euclidean(proba, proba_2))\n",
    "    max_idx = dist_list.index(min(dist_list))\n",
    "    sq_temp_list.append(temps_vec[max_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.79,\n",
       " 2.6599999999999997,\n",
       " 2.26,\n",
       " 1.1600000000000001,\n",
       " 1.46,\n",
       " 3.06,\n",
       " 2.57,\n",
       " 1.98,\n",
       " 4.17,\n",
       " 0.7100000000000001]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.06,\n",
       " 3.9,\n",
       " 4.6499999999999995,\n",
       " 3.9,\n",
       " 4.7299999999999995,\n",
       " 4.6899999999999995,\n",
       " 4.93,\n",
       " 2.1799999999999997,\n",
       " 4.87,\n",
       " 2.6799999999999997]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) *Transfer Learning*.  \n",
    "As we emphasize in class, one can freeze the training of the bottom layers of a network and retrain the top part of the network to adopt to a new situation. Use your CNN that you trained on the squarelattice data to do transfer learning on the triangular lattice data.  How does the performance compare to that of the direct methods?  Add the performance numbers for transfer learning in your table from Part (a). Note that the training time and number of training examples needed for transfer learning is far less than that for the direct  optimization. For  example,  is  50  triangle  example  sufficient  for the re-training process?  Use your transfer learning result to predict the transition temperature of triangle lattice Ising model, as demonstrated in this [Nature Physics](https://www-nature-com.ezp-prod1.hul.harvard.edu/articles/nphys4035.pdf) publication.\n",
    "\n",
    "As a guideline, you may like to just change the last `Dense` layer with `softmax` activation when you do the transfer learning. Other choices are also OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T16:51:16.394160Z",
     "start_time": "2020-02-24T16:51:16.390887Z"
    }
   },
   "source": [
    "Solution to (d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = sq_CNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 122,116\n",
      "Trainable params: 122,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = base_model.layers[0:5]\n",
    "trainable_layers = [\n",
    "     Flatten(),\n",
    "     Dropout(0.25),\n",
    "     Dense(32, activation='relu'),\n",
    "     Dense(num_classes, activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 89,188\n",
      "Trainable params: 89,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trans_model = keras.Sequential(base_layers+trainable_layers)\n",
    "trans_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      "800/800 [==============================] - 0s 462us/sample - loss: 0.8062 - accuracy: 0.6475 - val_loss: 0.4865 - val_accuracy: 0.7800\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 0s 98us/sample - loss: 0.5662 - accuracy: 0.7513 - val_loss: 0.6686 - val_accuracy: 0.6250\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.3535 - accuracy: 0.8650 - val_loss: 0.2452 - val_accuracy: 0.9400\n",
      "Epoch 4/25\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.3366 - accuracy: 0.8687 - val_loss: 0.4026 - val_accuracy: 0.8350\n",
      "Epoch 5/25\n",
      "800/800 [==============================] - 0s 105us/sample - loss: 0.4222 - accuracy: 0.8438 - val_loss: 0.2281 - val_accuracy: 0.9350\n",
      "Epoch 6/25\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.2991 - accuracy: 0.8700 - val_loss: 0.1878 - val_accuracy: 0.9350\n",
      "Epoch 7/25\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.2132 - accuracy: 0.9237 - val_loss: 1.1569 - val_accuracy: 0.6400\n",
      "Epoch 8/25\n",
      "800/800 [==============================] - 0s 102us/sample - loss: 0.2756 - accuracy: 0.8975 - val_loss: 0.1649 - val_accuracy: 0.9700\n",
      "Epoch 9/25\n",
      "800/800 [==============================] - 0s 106us/sample - loss: 0.1797 - accuracy: 0.9362 - val_loss: 0.2105 - val_accuracy: 0.9500\n",
      "Epoch 10/25\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.1564 - accuracy: 0.9463 - val_loss: 0.1615 - val_accuracy: 0.9600\n",
      "Epoch 11/25\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.1799 - accuracy: 0.9325 - val_loss: 0.1671 - val_accuracy: 0.9350\n",
      "Epoch 12/25\n",
      "800/800 [==============================] - 0s 121us/sample - loss: 0.1563 - accuracy: 0.9400 - val_loss: 0.1413 - val_accuracy: 0.9650\n",
      "Epoch 13/25\n",
      "800/800 [==============================] - 0s 117us/sample - loss: 0.1387 - accuracy: 0.9563 - val_loss: 0.1196 - val_accuracy: 0.9600\n",
      "Epoch 14/25\n",
      "800/800 [==============================] - 0s 112us/sample - loss: 0.1172 - accuracy: 0.9575 - val_loss: 0.1430 - val_accuracy: 0.9500\n",
      "Epoch 15/25\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.1384 - accuracy: 0.9450 - val_loss: 0.1252 - val_accuracy: 0.9600\n",
      "Epoch 16/25\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.1115 - accuracy: 0.9613 - val_loss: 0.1144 - val_accuracy: 0.9600\n",
      "Epoch 17/25\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.0918 - accuracy: 0.9650 - val_loss: 0.1069 - val_accuracy: 0.9600\n",
      "Epoch 18/25\n",
      "800/800 [==============================] - 0s 115us/sample - loss: 0.1046 - accuracy: 0.9575 - val_loss: 0.1026 - val_accuracy: 0.9650\n",
      "Epoch 19/25\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.1114 - accuracy: 0.9563 - val_loss: 0.1017 - val_accuracy: 0.9600\n",
      "Epoch 20/25\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 0.1044 - accuracy: 0.9588 - val_loss: 0.1762 - val_accuracy: 0.9550\n",
      "Epoch 21/25\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.1114 - accuracy: 0.9550 - val_loss: 0.1424 - val_accuracy: 0.9250\n",
      "Epoch 22/25\n",
      "800/800 [==============================] - 0s 130us/sample - loss: 0.0736 - accuracy: 0.9775 - val_loss: 0.0939 - val_accuracy: 0.9650\n",
      "Epoch 23/25\n",
      "800/800 [==============================] - 0s 116us/sample - loss: 0.0788 - accuracy: 0.9712 - val_loss: 0.1542 - val_accuracy: 0.9650\n",
      "Epoch 24/25\n",
      "800/800 [==============================] - 0s 114us/sample - loss: 0.0933 - accuracy: 0.9712 - val_loss: 0.0929 - val_accuracy: 0.9700\n",
      "Epoch 25/25\n",
      "800/800 [==============================] - 0s 120us/sample - loss: 0.0710 - accuracy: 0.9775 - val_loss: 0.1148 - val_accuracy: 0.9650\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 1)\n",
    "num_classes = 4\n",
    "hyperparams = (0.01, 25, 32)\n",
    "H_trans, trained_trans_model = train_model(trans_model, Xtri_train, ytri_train, Xtri_test, ytri_test, num_classes, input_shape, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
