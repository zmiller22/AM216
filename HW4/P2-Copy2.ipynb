{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaOV8WHFZYQy"
   },
   "source": [
    "## Loading the Inception model from the [Applications of Keras](https://keras.io/applications/) or [Transfer learning with a pretrained ConvNet](https://www.tensorflow.org/tutorials/images/transfer_learning)\n",
    "Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\n",
    "\n",
    "Weights are downloaded automatically when instantiating a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mt8Ilz-KZYQ6"
   },
   "source": [
    "## (a) The Ising Model – try your show that the square lattice data can be trained perfectly using the embeddings of Inception.\n",
    "\n",
    "Get the embeddings first, then build a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import gc\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that tf is running on gpu\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure \n",
    "#tf.compat.v2.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data and shape it for training\n",
    "with tf.device('/CPU:0'):\n",
    "\n",
    "    N = 250\n",
    "    nx, ny = 32, 32\n",
    "\n",
    "    Xsq = np.ndarray((4*N,nx,ny,1))\n",
    "    ysq = np.ndarray(4*N)\n",
    "\n",
    "    for i in np.arange(N):\n",
    "        Xsq[i + 0*N] = np.loadtxt(\"./square_T1/square_T1/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "        ysq[i + 0*N] = 0\n",
    "        Xsq[i + 1*N] = np.loadtxt(\"./square_T2/square_T2/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "        ysq[i + 1*N] = 1\n",
    "        Xsq[i + 2*N] = np.loadtxt(\"./square_T3/square_T3/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "        ysq[i + 2*N] = 2\n",
    "        Xsq[i + 3*N] = np.loadtxt(\"./square_T4/square_T4/{:03d}\".format(i), delimiter=\",\").reshape(nx,ny,1)\n",
    "        ysq[i + 3*N] = 3\n",
    "    \n",
    "    # create train/test splits\n",
    "    Xsq_train, Xsq_test, ysq_train, ysq_test = train_test_split(Xsq, ysq, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # Reshape the data for input into transfer model\n",
    "    Xsq_train, Xsq_test = np.repeat(Xsq_train, 8, 1), np.repeat(Xsq_test, 8, 1)\n",
    "    Xsq_train, Xsq_test = np.repeat(Xsq_train, 8, 2), np.repeat(Xsq_test, 8, 2)\n",
    "    Xsq_train, Xsq_test = np.repeat(Xsq_train, 3, 3), np.repeat(Xsq_test, 3, 3)\n",
    "    #Xsq_train, Xsq_test = tf.cast(Xsq_train, tf.float32), tf.cast(Xsq_test, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the inception model without last layer and disable training\n",
    "with tf.device('/CPU:0'):\n",
    "    base_model = keras.applications.inception_v3.InceptionV3(include_top=False, \n",
    "                                                            weights='imagenet', \n",
    "                                                            input_shape=Xsq_train[0].shape\n",
    "                                                            )\n",
    "\n",
    "    base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedded data and normalize\n",
    "global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "Xsq_train_emb, Xsq_test_emb = global_avg_layer(base_model.predict(Xsq_train)), global_avg_layer(base_model.predict(Xsq_test))\n",
    "Xsq_train_emb, Xsq_test_emb = Xsq_train_emb.numpy()/Xsq_train_emb.numpy().max(), Xsq_test_emb.numpy()/Xsq_test_emb.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "pca = PCA(n_components=2)\n",
    "tsne_img_emb = tsne.fit_transform(Xsq_train_emb)\n",
    "pca_img_emb = pca.fit_transform(Xsq_train_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    plt.plot(tsne_img_emb.transpose()[0],tsne_img_emb.transpose()[1],'.',markersize=12)\n",
    "plt.title(\"TSNE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    plt.plot(pca_img_emb.transpose()[0], pca_img_emb.transpose()[1],'.',markersize=12)\n",
    "plt.title(\"PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class small_FNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(num_classes, channels_first=False):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(612, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_layers():\n",
    "        layers = [\n",
    "        tf.keras.layers.Dense(612, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ]\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_model, train_data, train_lbls, test_data, test_lbls, \n",
    "                          num_classes, input_shape, hyperparams):\n",
    "    x_train = train_data\n",
    "    x_test = test_data\n",
    "    \n",
    "    # Create categorical labels\n",
    "    y_train = keras.utils.to_categorical(train_lbls, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_lbls, num_classes)\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = input_model\n",
    "    \n",
    "    # Set hyperparameters\n",
    "    INIT_LR = hyperparams[0]# learning rate\n",
    "    EPOCHS = hyperparams[1] # number of epochs\n",
    "    BS = hyperparams[2] # batch size\n",
    "    OPT = tf.keras.optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=OPT, metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    H = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)\n",
    "    \n",
    "    return H, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (2048,)\n",
    "num_classes = 4\n",
    "fnn_model = small_FNN.build(4)\n",
    "\n",
    "# hyperparams should be a tuple of: INIT_LR, EPOCHS, BS\n",
    "hyperparams = (0.001, 50, 32)\n",
    "H_sq_FNN, sq_FNN_model = train_model(fnn_model, Xsq_train_emb, ysq_train,\n",
    "                                     Xsq_test_emb, ysq_test, num_classes, input_shape, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CnhTWD1ZYQ7"
   },
   "source": [
    "## (b)  [Rayleigh-Bénard Convection](https://en.wikipedia.org/wiki/Rayleigh%E2%80%93B%C3%A9nard_convection)\n",
    "\n",
    "RB convection, in which a flow is heated from below and cooled  from  top,  is  one  of  the  paradigmatic  system  in  fluid  dynamics. When the temperature difference between the two plates (in dimensionless form Rayleigh number Ra) is beyond certain threshold, hot fluid tends to go up and cold fluid tends to go down, thus forming convection cells. What we supply here are the temperature snapshots from four different Ra, i.e., $Ra=10^{14}$ as `class0`,$Ra= 10^{13}$ as `class1`, $Ra= 10^{12}$ as `class2`,and $Ra= 10^{11}$ as `class3`.  The flow you see is highly turbulent; not only there are big convection cells but also lots of small vortices.  The original dataset  is  around  4000*2000.   We  have  already  downsampled  the  data into the zip file `fluid_org.zip`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EyHNtp3wZYQ8"
   },
   "source": [
    "### (1) Train the data in `fluid_org.zip` with inception.  Show that these images can be classified  into  different $Ra$ nicely  with  inception.  \n",
    "\n",
    "Take the length 2048 embeddings from the Inception model first. Then visualizing how the embeddings distribute using a two component PCA or two component T-SNE, whichever you prefer. Then use any of the previously learned method to train a classifier using the embeddings as input. **Note that T-SNE normally gives you better separation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import gc\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T04:18:40.675119Z",
     "start_time": "2020-02-24T04:18:32.328259Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "skbbAOw8ZYQ9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    imgs = []\n",
    "    lbls = []\n",
    "    for file in os.listdir('./fluid_org/fluid_org/'):\n",
    "        imgs.append(np.array(Image.open('./fluid_org/fluid_org/'+file))/255)\n",
    "        lbls.append(int(file[-5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T04:19:30.816271Z",
     "start_time": "2020-02-24T04:19:26.835782Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QStrmCBWZYRA"
   },
   "outputs": [],
   "source": [
    "imgs = np.array(imgs)\n",
    "lbls = np.array(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751, 250, 500, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zack/anaconda3/envs/am216/lib/python3.7/site-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    base_model_2 = keras.applications.MobileNetV2(include_top=False, \n",
    "                                                            weights='imagenet', \n",
    "                                                            input_shape=imgs[0].shape\n",
    "                                                            )\n",
    "\n",
    "    base_model_2.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_layer = keras.layers.GlobalAveragePooling2D()\n",
    "imgs_emb = global_avg_layer(base_model_2.predict(imgs))\n",
    "imgs_emb = imgs_emb.numpy()/imgs_emb.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751, 1280)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "pca = PCA(n_components=2)\n",
    "tsne_img_emb = tsne.fit_transform(imgs_emb)\n",
    "pca_img_emb = pca.fit_transform(imgs_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    plt.plot(tsne_img_emb.transpose()[0][lbls==i],tsne_img_emb.transpose()[1][lbls==i],'.',markersize=12)\n",
    "plt.title(\"TSNE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    plt.plot(pca_img_emb.transpose()[0][lbls==i],pca_img_emb.transpose()[1][lbls==i],'.',markersize=12)\n",
    "plt.title(\"PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3Klki-zZYRb"
   },
   "source": [
    "### (2) For advanced use of trainsfer learning from the pre-trained models such as fine-tuning, we need to do the transfer learning in-place, by building a network consists of the Inception and your classifier layers. \n",
    "Freeze the part you take from Inception, train\n",
    "the model and report the accuracy. Then do the fine-tuning. Report\n",
    "how much increase of accuracy you can manage to get. Fine tuning\n",
    "by making the top few layer of the Inception model trainable instead\n",
    "of freezing all the layers. Due to the slowness of training, unleash the\n",
    "layers one by one. Make comments about how the accuracy change. It is\n",
    "highly recommended that you train this on Google Colab with the GPU\n",
    "activated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_2(input_model, train_data, train_lbls, test_data, \n",
    "                test_lbls, num_classes, input_shape, hyperparams):\n",
    "    # Ensure data is shaped properly, assumes channels last set up\n",
    "    x_train = train_data\n",
    "    x_test = test_data\n",
    "    \n",
    "    # Create categorical labels\n",
    "    y_train = keras.utils.to_categorical(train_lbls, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_lbls, num_classes)\n",
    "    \n",
    "    # Set hyperparameters\n",
    "    INIT_LR = hyperparams[0]# learning rate\n",
    "    EPOCHS = hyperparams[1] # number of epochs\n",
    "    BS = hyperparams[2] # batch size\n",
    "    OPT = tf.keras.optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "    \n",
    "    model = input_model\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=OPT, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    H = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)\n",
    "    \n",
    "    return H, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class small_FNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(num_classes, channels_first=False):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(612, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_layers(num_classes):\n",
    "        layers = [\n",
    "        tf.keras.layers.Dense(612, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ]\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenetv2_1.00_224\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 250, 500, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_pad (ZeroPadding2D)       (None, 251, 501, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 125, 250, 32) 864         Conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 125, 250, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 125, 250, 32) 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 125, 250, 32) 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 125, 250, 32) 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 125, 250, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 125, 250, 16) 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 125, 250, 16) 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 125, 250, 96) 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 125, 250, 96) 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 125, 250, 96) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 127, 251, 96) 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 63, 125, 96)  864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 63, 125, 96)  384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 63, 125, 96)  0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 63, 125, 24)  2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 63, 125, 24)  96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 63, 125, 144) 3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 63, 125, 144) 576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 63, 125, 144) 0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 63, 125, 144) 1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 63, 125, 144) 576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 63, 125, 144) 0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 63, 125, 24)  3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 63, 125, 24)  96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 63, 125, 24)  0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 63, 125, 144) 3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 63, 125, 144) 576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 63, 125, 144) 0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 65, 127, 144) 0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 32, 63, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 32, 63, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 32, 63, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 32, 63, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 32, 63, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 32, 63, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 32, 63, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 32, 63, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 32, 63, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 32, 63, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 32, 63, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 32, 63, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 32, 63, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 32, 63, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 32, 63, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 32, 63, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 32, 63, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 32, 63, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 32, 63, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 32, 63, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 32, 63, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 32, 63, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 32, 63, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 32, 63, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 32, 63, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 32, 63, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 33, 65, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 16, 32, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 16, 32, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 16, 32, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 16, 32, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 16, 32, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 16, 32, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 16, 32, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 16, 32, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 16, 32, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 16, 32, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 16, 32, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 16, 32, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 16, 32, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 16, 32, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 16, 32, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 16, 32, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 16, 32, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 16, 32, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 16, 32, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 16, 32, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 16, 32, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 16, 32, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 16, 32, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 16, 32, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 16, 32, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 16, 32, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 16, 32, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 16, 32, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 16, 32, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 16, 32, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 16, 32, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 16, 32, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 16, 32, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 16, 32, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 16, 32, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 16, 32, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 16, 32, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 16, 32, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 16, 32, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 16, 32, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 16, 32, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 16, 32, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 16, 32, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 16, 32, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 16, 32, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 16, 32, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 16, 32, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 16, 32, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 16, 32, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 16, 32, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 16, 32, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 16, 32, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 16, 32, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 16, 32, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 16, 32, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 16, 32, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 16, 32, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 16, 32, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 16, 32, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 16, 32, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 16, 32, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 17, 33, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 8, 16, 576)   5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 8, 16, 576)   2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 8, 16, 576)   0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 8, 16, 160)   92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 8, 16, 160)   640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 8, 16, 960)   153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 8, 16, 960)   3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 8, 16, 960)   0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 8, 16, 960)   8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 8, 16, 960)   3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 8, 16, 960)   0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 8, 16, 160)   153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 8, 16, 160)   640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 8, 16, 160)   0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 8, 16, 960)   153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 8, 16, 960)   3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 8, 16, 960)   0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 8, 16, 960)   8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 8, 16, 960)   3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 8, 16, 960)   0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 8, 16, 160)   153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 8, 16, 160)   640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 8, 16, 160)   0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 8, 16, 960)   153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 8, 16, 960)   3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 8, 16, 960)   0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 8, 16, 960)   8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 8, 16, 960)   3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 8, 16, 960)   0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 8, 16, 320)   307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 8, 16, 320)   1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 8, 16, 1280)  409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 8, 16, 1280)  5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 8, 16, 1280)  0           Conv_1_bn[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,257,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full model\n",
    "trainable_model = small_FNN.get_layers(4)\n",
    "layer_list = [base_model_2, global_avg_layer]+trainable_model\n",
    "\n",
    "model = tf.keras.Sequential(layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 8, 16, 1280)       2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 612)               783972    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 612)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                19616     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 3,061,832\n",
      "Trainable params: 803,784\n",
      "Non-trainable params: 2,258,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Look at the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test splits\n",
    "train_imgs, test_imgs, train_lbls, test_lbls = train_test_split(imgs, lbls, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 151 samples\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 7s 11ms/sample - loss: 0.7981 - accuracy: 0.6900 - val_loss: 1.0839 - val_accuracy: 0.5563\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.3010 - accuracy: 0.9150 - val_loss: 1.0436 - val_accuracy: 0.5430\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.1950 - accuracy: 0.9567 - val_loss: 1.0056 - val_accuracy: 0.5828\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.1404 - accuracy: 0.9750 - val_loss: 0.9546 - val_accuracy: 0.6556\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.1144 - accuracy: 0.9767 - val_loss: 0.9192 - val_accuracy: 0.6490\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0980 - accuracy: 0.9867 - val_loss: 0.8926 - val_accuracy: 0.7020\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0929 - accuracy: 0.9883 - val_loss: 0.8596 - val_accuracy: 0.7152\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.1179 - accuracy: 0.9750 - val_loss: 0.8499 - val_accuracy: 0.7152\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0985 - accuracy: 0.9833 - val_loss: 0.8759 - val_accuracy: 0.6159\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0841 - accuracy: 0.9883 - val_loss: 0.7945 - val_accuracy: 0.7682\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0946 - accuracy: 0.9783 - val_loss: 0.7716 - val_accuracy: 0.8079\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0597 - accuracy: 0.9933 - val_loss: 0.7468 - val_accuracy: 0.8212\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0502 - accuracy: 0.9983 - val_loss: 0.7365 - val_accuracy: 0.8146\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0573 - accuracy: 0.9950 - val_loss: 0.7159 - val_accuracy: 0.8212\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0496 - accuracy: 0.9950 - val_loss: 0.7089 - val_accuracy: 0.8212\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0632 - accuracy: 0.9933 - val_loss: 0.7047 - val_accuracy: 0.8278\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0504 - accuracy: 0.9917 - val_loss: 0.6931 - val_accuracy: 0.8411\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0548 - accuracy: 0.9917 - val_loss: 0.6848 - val_accuracy: 0.8344\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.6760 - val_accuracy: 0.8411\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 5s 8ms/sample - loss: 0.0460 - accuracy: 0.9950 - val_loss: 0.6777 - val_accuracy: 0.8278\n"
     ]
    }
   ],
   "source": [
    "hyperparams = (0.01, 20, 64)\n",
    "H, trained_model = train_model_2(model, train_imgs, train_lbls, test_imgs, test_lbls, 4, (250, 500, 3), hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_2.trainable = True\n",
    "\n",
    "for layer in base_model_2.layers[:-2]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adagrad(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 8, 16, 1280)       2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 612)               783972    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 612)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                19616     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 3,061,832\n",
      "Trainable params: 806,344\n",
      "Non-trainable params: 2,255,488\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 151 samples\n",
      "Epoch 20/30\n",
      "600/600 [==============================] - 7s 11ms/sample - loss: 0.0730 - accuracy: 0.9817 - val_loss: 0.6688 - val_accuracy: 0.8344\n",
      "Epoch 21/30\n",
      "600/600 [==============================] - 5s 8ms/sample - loss: 0.0513 - accuracy: 0.9933 - val_loss: 0.6677 - val_accuracy: 0.8278\n",
      "Epoch 22/30\n",
      "600/600 [==============================] - 5s 8ms/sample - loss: 0.0449 - accuracy: 0.9950 - val_loss: 0.6639 - val_accuracy: 0.8212\n",
      "Epoch 23/30\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0454 - accuracy: 0.9967 - val_loss: 0.6613 - val_accuracy: 0.8146\n",
      "Epoch 24/30\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0515 - accuracy: 0.9900 - val_loss: 0.6585 - val_accuracy: 0.8146\n",
      "Epoch 25/30\n",
      "600/600 [==============================] - 5s 8ms/sample - loss: 0.0583 - accuracy: 0.9900 - val_loss: 0.6577 - val_accuracy: 0.8146\n",
      "Epoch 26/30\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0627 - accuracy: 0.9867 - val_loss: 0.6557 - val_accuracy: 0.8212\n",
      "Epoch 27/30\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0597 - accuracy: 0.9883 - val_loss: 0.6558 - val_accuracy: 0.8212\n",
      "Epoch 28/30\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0472 - accuracy: 0.9950 - val_loss: 0.6540 - val_accuracy: 0.8212\n",
      "Epoch 29/30\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0523 - accuracy: 0.9933 - val_loss: 0.6538 - val_accuracy: 0.8212\n",
      "Epoch 30/30\n",
      "600/600 [==============================] - 5s 9ms/sample - loss: 0.0539 - accuracy: 0.9917 - val_loss: 0.6539 - val_accuracy: 0.8212\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(train_lbls, 4)\n",
    "y_test = keras.utils.to_categorical(test_lbls, 4)\n",
    "H_fine = model.fit(train_imgs, y_train, epochs=30, initial_epoch=H.epoch[-1], validation_data=(test_imgs, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, unlocking the last label of MobileNetV2 and doing some fine tuning training did not improve the model, in fact it only really caused it to begin overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuQscq_aZYRk"
   },
   "source": [
    "### (3) Explore the potential of transfer learning on cropped data `fluid-crop`, which are randomly choosen regions of 100*100 pixels from each original 4000*2000 pictures, i.e.,just around 1% of the original picture! \n",
    "You can use either method you use in (1) or (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import gc\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    imgs = []\n",
    "    lbls = []\n",
    "    for file in os.listdir('./fluid_crop/fluid_crop/'):\n",
    "        imgs.append(np.array(Image.open('./fluid_org/fluid_org/'+file))/255)\n",
    "        lbls.append(int(file[-5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.array(imgs)\n",
    "lbls = np.array(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zack/anaconda3/envs/am216/lib/python3.7/site-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    base_model_2 = keras.applications.MobileNetV2(include_top=False, \n",
    "                                                            weights='imagenet', \n",
    "                                                            input_shape=imgs[0].shape\n",
    "                                                            )\n",
    "\n",
    "    base_model_2.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_layer = keras.layers.GlobalAveragePooling2D()\n",
    "imgs_emb = global_avg_layer(base_model_2.predict(imgs))\n",
    "imgs_emb = imgs_emb.numpy()/imgs_emb.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class small_FNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(num_classes, channels_first=False):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(612, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_layers():\n",
    "        layers = [\n",
    "        tf.keras.layers.Dense(612, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ]\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_model, train_data, train_lbls, test_data, test_lbls, \n",
    "                          num_classes, input_shape, hyperparams):\n",
    "    x_train = train_data\n",
    "    x_test = test_data\n",
    "    \n",
    "    # Create categorical labels\n",
    "    y_train = keras.utils.to_categorical(train_lbls, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_lbls, num_classes)\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = input_model\n",
    "    \n",
    "    # Set hyperparameters\n",
    "    INIT_LR = hyperparams[0]# learning rate\n",
    "    EPOCHS = hyperparams[1] # number of epochs\n",
    "    BS = hyperparams[2] # batch size\n",
    "    OPT = tf.keras.optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=OPT, metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    H = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)\n",
    "    \n",
    "    return H, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test splits\n",
    "train_imgs, test_imgs, train_lbls, test_lbls = train_test_split(imgs_emb, lbls, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 151 samples\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 0s 634us/sample - loss: 0.5989 - accuracy: 0.7850 - val_loss: 1.0753 - val_accuracy: 0.9404\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 0s 79us/sample - loss: 0.1713 - accuracy: 0.9750 - val_loss: 1.0083 - val_accuracy: 0.9868\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 0s 81us/sample - loss: 0.1131 - accuracy: 0.9917 - val_loss: 0.9198 - val_accuracy: 0.9868\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 0s 77us/sample - loss: 0.0878 - accuracy: 0.9950 - val_loss: 0.8413 - val_accuracy: 1.0000\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 0s 79us/sample - loss: 0.0686 - accuracy: 0.9967 - val_loss: 0.7632 - val_accuracy: 1.0000\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 0s 79us/sample - loss: 0.0571 - accuracy: 0.9967 - val_loss: 0.6827 - val_accuracy: 1.0000\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 0s 79us/sample - loss: 0.0572 - accuracy: 0.9950 - val_loss: 0.6017 - val_accuracy: 1.0000\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 0s 78us/sample - loss: 0.0488 - accuracy: 0.9950 - val_loss: 0.5257 - val_accuracy: 1.0000\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 0s 81us/sample - loss: 0.0534 - accuracy: 0.9933 - val_loss: 0.4598 - val_accuracy: 1.0000\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 0s 84us/sample - loss: 0.0419 - accuracy: 0.9950 - val_loss: 0.3888 - val_accuracy: 1.0000\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 0s 86us/sample - loss: 0.0438 - accuracy: 0.9933 - val_loss: 0.3259 - val_accuracy: 1.0000\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 0s 84us/sample - loss: 0.0329 - accuracy: 0.9967 - val_loss: 0.2623 - val_accuracy: 1.0000\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 1.00 - 0s 79us/sample - loss: 0.0307 - accuracy: 0.9983 - val_loss: 0.2146 - val_accuracy: 1.0000\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 0s 88us/sample - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.1734 - val_accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 0s 83us/sample - loss: 0.0291 - accuracy: 0.9983 - val_loss: 0.1344 - val_accuracy: 1.0000\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 0s 81us/sample - loss: 0.0302 - accuracy: 0.9983 - val_loss: 0.1015 - val_accuracy: 1.0000\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 0s 79us/sample - loss: 0.0359 - accuracy: 0.9950 - val_loss: 0.0868 - val_accuracy: 1.0000\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 0s 99us/sample - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 0s 80us/sample - loss: 0.0329 - accuracy: 0.9950 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 0s 80us/sample - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 0s 79us/sample - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 0s 80us/sample - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 0s 80us/sample - loss: 0.0248 - accuracy: 0.9983 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 0s 80us/sample - loss: 0.0197 - accuracy: 0.9983 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 0s 76us/sample - loss: 0.0213 - accuracy: 0.9983 - val_loss: 0.0095 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "input_shape = (1280,)\n",
    "num_classes = 4\n",
    "fnn_model = small_FNN.build(num_classes=num_classes)\n",
    "\n",
    "# hyperparams should be a tuple of: INIT_LR, EPOCHS, BS\n",
    "hyperparams = (0.01, 25, 32)\n",
    "H_3, model_3 = train_model(fnn_model, train_imgs, train_lbls,\n",
    "                                     test_imgs, test_lbls, 4, input_shape, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hzdh9wTPZYRk"
   },
   "source": [
    "### (4) Build your own classifier for (2) and (3) without using Inception. Compare the performance of your own classifier with the result in (2) and (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3d6daf7cbe5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import gc\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        model = self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(width, height, depth, num_classes, channels_first=False):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        \n",
    "        if channels_first == False:\n",
    "            inputShape = (height, width, depth)\n",
    "            chanDim = -1\n",
    "        elif channels_first == True:\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        model.add(Conv2D(32, (5, 5), activation='relu', input_shape=inputShape))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "                  \n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_model, train_data, train_lbls, test_data, \n",
    "                test_lbls, num_classes, input_shape, hyperparams):\n",
    "    # Ensure data is shaped properly, assumes channels last set up\n",
    "    x_train = train_data\n",
    "    x_test = test_data\n",
    "    \n",
    "    # Create categorical labels\n",
    "    y_train = keras.utils.to_categorical(train_lbls, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_lbls, num_classes)\n",
    "     \n",
    "    # Set hyperparameters\n",
    "    INIT_LR = hyperparams[0]# learning rate\n",
    "    EPOCHS = hyperparams[1] # number of epochs\n",
    "    BS = hyperparams[2] # batch size\n",
    "    OPT = optimizers.Adagrad(lr=INIT_LR) # optimizing function\n",
    "    \n",
    "    model = input_model\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=OPT, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    H = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=EPOCHS,\n",
    "                  batch_size=BS)\n",
    "    \n",
    "    return H, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    imgs_reg = []\n",
    "    lbls_reg = []\n",
    "    for file in os.listdir('./fluid_org/fluid_org/'):\n",
    "        imgs_reg.append(np.array(Image.open('./fluid_org/fluid_org/'+file))/255)\n",
    "        lbls_reg.append(int(file[-5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_reg = np.array(imgs_reg)\n",
    "lbls = np.array(lbls_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751, 250, 500, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test splits\n",
    "x_train_reg, x_test_reg, y_train_reg, y_test_reg = train_test_split(imgs_reg, lbls_reg, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 151 samples\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 8s 13ms/sample - loss: 2.0621 - accuracy: 0.2350 - val_loss: 1.3806 - val_accuracy: 0.2715\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 1.3830 - accuracy: 0.2517 - val_loss: 1.3803 - val_accuracy: 0.2715\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 1.3751 - accuracy: 0.3333 - val_loss: 1.3987 - val_accuracy: 0.2384\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 1.3785 - accuracy: 0.3533 - val_loss: 1.3487 - val_accuracy: 0.2914\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 1.3776 - accuracy: 0.3283 - val_loss: 1.2797 - val_accuracy: 0.6225\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 1.5294 - accuracy: 0.4283 - val_loss: 1.3428 - val_accuracy: 0.5695\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 1.3164 - accuracy: 0.5500 - val_loss: 0.8112 - val_accuracy: 0.7881\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 1.0121 - accuracy: 0.6133 - val_loss: 0.4500 - val_accuracy: 0.9669\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 1.0819 - accuracy: 0.6817 - val_loss: 0.6895 - val_accuracy: 0.6954\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.4288 - accuracy: 0.8567 - val_loss: 0.2758 - val_accuracy: 0.8013\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.1083 - accuracy: 0.9683 - val_loss: 0.0442 - val_accuracy: 0.9934\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0438 - accuracy: 0.9900 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 9.1481e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 7.5602e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 4s 7ms/sample - loss: 7.5659e-04 - accuracy: 1.0000 - val_loss: 9.6175e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "input_shape = (500, 250, 3)\n",
    "num_classes = 4\n",
    "cnn_model = SimpleCNN.build(width=input_shape[0], height=input_shape[1], depth=input_shape[2],\n",
    "                   num_classes=num_classes)\n",
    "\n",
    "hyperparams = (0.01, 25, 64)\n",
    "H_reg, reg_model = train_model(cnn_model, x_train_reg, y_train_reg, x_test_reg, y_test_reg, num_classes, input_shape, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xxmW9iPZYRl"
   },
   "source": [
    "### (5) Continue (3), construct two examples where a different layer's output is used as the embedding. There are over 300 layers in Inception. Pick one at around the 100th layer and one at around 200th layer. The exact layer you pick is based on your preference. Show the following.\n",
    "- (i) The distributions of the embeddings similar to what you've done in (1). Together with the result you get in (1), comment the similarity and difference between what you get using the three embedding layers.\n",
    "- (ii) What is the test accuracy of the three classifiers. What is the test accuracy of the three classifiers? For speeding up the training you can choose to get the embeddings first and put those into a classifier, as you did in (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to (5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import gc\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    imgs = []\n",
    "    lbls = []\n",
    "    for file in os.listdir('./fluid_crop/fluid_crop/'):\n",
    "        imgs.append(np.array(Image.open('./fluid_org/fluid_org/'+file))/255)\n",
    "        lbls.append(int(file[-5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.array(imgs)\n",
    "lbls = np.array(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    base_model_2 = keras.applications.MobileNetV2(include_top=False, \n",
    "                                                            weights='imagenet', \n",
    "                                                            input_shape=imgs[0].shape\n",
    "                                                            )\n",
    "\n",
    "    base_model_2.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_layer = keras.layers.GlobalAveragePooling2D()\n",
    "imgs_emb_1 = global_avg_layer(base_model_2.predict(imgs))\n",
    "imgs_emb_1 = imgs_emb.numpy()/imgs_emb.numpy().max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
