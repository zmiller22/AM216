{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liV-01vGobll"
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "In this homework problem, you are going to use `tensorflow-probability` to deal with an unfair dice, i.e., a dice that has different probability of settling with each of the face 1 to 6 facing up, instead of the $p=1/6$ equal probability in the case of a fair dice.\n",
    "\n",
    "You are provided with 5000 data entries of this dice. Each entry is a length-6 vector with one element being 1 and 0 for the rest. For example, $[0,1,0,0,0,0]$ means the \"2\" face of this dice landed facing up. This form is also the data form generated by `tfp.distribution.Bernoulli` when you feed multiple probability to it.\n",
    "\n",
    "You are going to estimate the 6 probabilities describing this unfair dice (6 face). $\\tilde{p} = [p1,p2,p3,p4,p5,p6]$. Keep in mind that they sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "An alternate way of formulating this question is to consider the result of each of the n throws of the die as independent random variables $R$, where $R$ is distributed according to $R\\sim Multinomial(n,\\vec{p}) \\text{ for } \\vec{p}=(p_1, p_2, p_3,..., p_6)$. Then define $X_k$ to be the total number of rolls with result $k$. It can be shown that the MLE estimate for $p_k$ is given by $\\hat{p}_k=x_k/n$. Calculating below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WR5t6IabO5ou"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the roll data, the roll sums (x_k's), and the number of rolls (n)\n",
    "roll_data = tf.constant(np.loadtxt('unfair_dice.txt'))\n",
    "roll_sums = tf.reduce_sum(roll_data, axis=0)\n",
    "num_rolls = roll_data.shape[0]\n",
    "\n",
    "# Calculate the MLE\n",
    "p_hat_vec = tf.divide(roll_sums, num_rolls)\n",
    "\n",
    "# Check that probabilities sum to 1, else adjust for rounding error\n",
    "if tf.reduce_sum(p_hat_vec)==1:\n",
    "    tf.print('MLE of p vector:', p_hat_vec)\n",
    "else:\n",
    "    p_hat_vec = tf.divide(p_hat_vec, tf.reduce_sum(p_hat_vec))\n",
    "    tf.print('MLE of p vector:', p_hat_vec)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Md6Yu2YIqXpO"
   },
   "source": [
    "### (2) Use MAP to estimate $\\tilde{p}$. Selecte three different prior distribution (if the distribution is parametrized, select three different enough parameters). Using 5000 sample, compare which prior gives the best estimation of why.\n",
    "\n",
    "[Hint: If the optimization takes too long, try to run a certain amount of steps instead of setting a criteria for the gradient. Check the remaining gradient and determine whether to increase the number of steps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "An alternate way to obtain a point estimate for $\\vec{p}$ is using MAP, which is more akin to the Bayesian approach to creating point estimates in that it multiplies the likelyhood by a prior distribution $P(\\vec{p})$, allowing us to bake in prior beliefs about the distribution of $\\vec{p}$. This is Bayesian because we are now allowing ourselves to talk about $\\vec{p}$ in terms of probabilities, something that is not allowed under the frequentist perspective.\n",
    "\n",
    "Let's play the role of a casino that has noticed a suspicous amount of 1's and 4's being rolled at a specific craps table. We suspect that some of the die at the table are each weighted to land on one of these values more often than the others. To investigate, we have analyzed hours of casino floor film to get 5000 roll results of one particular die (giving our data above), and now we want to investigate if this die is weighted to 1, 4, or is unweighted. Let's make the prior for $\\vec{p}$ a Dirlechet distribution, and use three different $\\alpha$ paramaterizations to reflect our three prior beleifs. We will let all elements of $\\vec{\\alpha}$ equal 1 for the first paramaterization (prior beleif that die is not loaded), we will let $\\alpha_1=5$ with all others being 1 for the second paramaterization (prior beleif that the die is loaded to land on 1), and we will let $\\alpha_4=5$ with all others being 1 for the third paramaterization.\n",
    "\n",
    "Now we want to calculate the MAP estimate for each of these paramaterizations below. However, it is much harder to derive the argmax of the resulting function with respect to $\\tilde{\\vec{p}}$ of this equation analytically, so we will isntead use gradient ascent to find the maximum. Doing this below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cfd02febc5b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a list of params to try\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparam_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mparam_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparam_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparam_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a list of params to try\n",
    "param_0 = np.ones(6)\n",
    "param_1 = np.copy(param_0)\n",
    "param_5 = np.copy(param_0)\n",
    "param_1[0] = 5\n",
    "param_5[3] = 5\n",
    "\n",
    "param_0 = tf.constant(param_0)\n",
    "param_1 = tf.constant(param_1)\n",
    "param_5 = tf.constant(param_5)\n",
    "\n",
    "param_list = [param_1, param_5]\n",
    "map_est_list = []\n",
    "\n",
    "for param in param_list:\n",
    "    \n",
    "    LR = 0.001\n",
    "    \n",
    "    # Create some values to keep track of\n",
    "    p_vec_est = tf.constant((1.0/6.0)*np.ones(6))\n",
    "    norm_grad = np.ones(6)\n",
    "    norm_grad_list = []\n",
    "    \n",
    "    # Define the dirichlet distribution with the proper params\n",
    "    prior_dis = tfd.Dirichlet(concentration=param)\n",
    "    i = 0\n",
    "    while norm_grad.all() > 0.01:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(p_vec_est)\n",
    "            \n",
    "            # Define the likelyhood distribution\n",
    "            lh_dis = tfd.Multinomial(total_count=num_rolls, probs=p_vec_est)\n",
    "            \n",
    "            # Define the MAP loss function, we will optimize with respect to the total number of counts\n",
    "            # of each rather than raw data since it will result in less extreme values. We can do this\n",
    "            # since they are proportional and we really dont care what order the rolls occured in \n",
    "            map_loss = (lh_dis.log_prob(value=roll_sums)+prior_dis.log_prob(value=p_vec_est))/num_rolls\n",
    "            \n",
    "            # Take the gradient\n",
    "            grad = tape.gradient(map_loss, p_vec_est)\n",
    "            norm_grad = grad.numpy()\n",
    "        \n",
    "        p_vec_est += LR*norm_grad\n",
    "        p_vec_est = tf.divide(p_vec_est, tf.reduce_sum(p_vec_est))\n",
    "        norm_grad_list.append(norm_grad)\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(\"Iteration:\", i)\n",
    "            tf.print(map_loss)\n",
    "            tf.print(\"p_vec_est\", p_vec_est)\n",
    "            tf.print(\"Likelyhood Log Prob:\", lh_dis.log_prob(value=roll_sums))\n",
    "            tf.print(\"Prior Log Prob:\", prior_dis.log_prob(value=p_vec_est))\n",
    "            tf.print(\"Gradient:\", grad)\n",
    "            print()\n",
    "        elif i>10000:\n",
    "            i = 0\n",
    "            break;\n",
    "        i+=1\n",
    "    \n",
    "    print(p_vec_est)\n",
    "    map_est_list.append(p_vec_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=-1506.1565747316326>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_vec_est = tf.constant((1/6)*np.ones(6))\n",
    "lh_dis = tfd.Multinomial(total_count=num_rolls, probs=p_vec_est, validate_args=True)\n",
    "\n",
    "lh_dis.log_prob(roll_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'sample last-dimension must sum to `1`'\nb'x and y not equal to tolerance rtol = tf.Tensor(2.220446049250313e-15, shape=(), dtype=float64), atol = tf.Tensor(2.220446049250313e-15, shape=(), dtype=float64)'\nb'x (shape=() dtype=float64) = '\n1.0\nb'y (shape=() dtype=float64) = '\n60000000.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1f1398c0283a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprior_dis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcentration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprior_dis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_vec_est\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/am216/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m     \"\"\"\n\u001b[0;32m--> 874\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/am216/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m_call_log_prob\u001b[0;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m           value, name='value', dtype_hint=self.dtype)\n\u001b[1;32m    855\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_log_prob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_prob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/am216/lib/python3.7/site-packages/tensorflow_probability/python/internal/distribution_util.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/am216/lib/python3.7/site-packages/tensorflow_probability/python/distributions/dirichlet.py\u001b[0m in \u001b[0;36m_log_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdistribution_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAppendDocstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dirichlet_sample_note\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_assert_valid_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m       \u001b[0mconcentration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcentration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m       return (tf.reduce_sum(tf.math.xlogy(concentration - 1., x), axis=-1) -\n",
      "\u001b[0;32m~/anaconda3/envs/am216/lib/python3.7/site-packages/tensorflow_probability/python/distributions/dirichlet.py\u001b[0m in \u001b[0;36m_maybe_assert_valid_sample\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             message='sample last-dimension must sum to `1`'),\n\u001b[0m\u001b[1;32m    310\u001b[0m     ]\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/am216/lib/python3.7/site-packages/tensorflow_core/python/ops/check_ops.py\u001b[0m in \u001b[0;36massert_near\u001b[0;34m(x, y, rtol, atol, data, summarize, message, name)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0mcondition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAssert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/am216/lib/python3.7/site-packages/tensorflow_core/python/util/tf_should_use.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m       return _add_should_use_warning(fn(*args, **kwargs),\n\u001b[0m\u001b[1;32m    236\u001b[0m                                      \u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                                      error_in_function=error_in_function)\n",
      "\u001b[0;32m~/anaconda3/envs/am216/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAssert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m    154\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m           \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Expected '%s' to be true. Summarized data: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m           (condition, \"\\n\".join(data_str)))\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'sample last-dimension must sum to `1`'\nb'x and y not equal to tolerance rtol = tf.Tensor(2.220446049250313e-15, shape=(), dtype=float64), atol = tf.Tensor(2.220446049250313e-15, shape=(), dtype=float64)'\nb'x (shape=() dtype=float64) = '\n1.0\nb'y (shape=() dtype=float64) = '\n60000000.0"
     ]
    }
   ],
   "source": [
    "prior_dis = tfd.Dirichlet(concentration=tf.constant(np.ones(6)), validate_args=True)\n",
    "prior_dis.log_prob(p_vec_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [0.5,0,0,0,0.5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=119.99999999999997>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_dis.prob(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Using Monte-Carlo sampling, generate posterial samples and estimate the 6 probabilities again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Create a single trivariate Dirichlet, with the 3rd class being three times\n",
    "# more frequent than the first. I.e., batch_shape=[], event_shape=[3].\n",
    "alpha = [1, 1, 1, 1, 1, 1]\n",
    "dist = tfd.Dirichlet(alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=120.00001>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([10, .1, .1, .1, .1, .1]) \n",
    "dist.prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Create a single trivariate Dirichlet, with the 3rd class being three times\n",
    "# more frequent than the first. I.e., batch_shape=[], event_shape=[3].\n",
    "alpha = [.2, .1, .1]\n",
    "dist = tfd.Dirichlet(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=9.696046e-09>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([.2, 10000, 1000]) \n",
    "dist.prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
