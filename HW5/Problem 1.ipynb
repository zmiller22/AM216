{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liV-01vGobll"
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "In this homework problem, you are going to use `tensorflow-probability` to deal with an unfair dice, i.e., a dice that has different probability of settling with each of the face 1 to 6 facing up, instead of the $p=1/6$ equal probability in the case of a fair dice.\n",
    "\n",
    "You are provided with 5000 data entries of this dice. Each entry is a length-6 vector with one element being 1 and 0 for the rest. For example, $[0,1,0,0,0,0]$ means the \"2\" face of this dice landed facing up. This form is also the data form generated by `tfp.distribution.Bernoulli` when you feed multiple probability to it.\n",
    "\n",
    "You are going to estimate the 6 probabilities describing this unfair dice (6 face). $\\tilde{p} = [p1,p2,p3,p4,p5,p6]$. Keep in mind that they sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "An alternate way of formulating this question is to consider the result of each of the n throws of the die as independent random variables $R$, where $R$ is distributed according to $R\\sim Multinomial(n,\\vec{p}) \\text{ for } \\vec{p}=(p_1, p_2, p_3,..., p_6)$. Then define $X_k$ to be the total number of rolls with result $k$. It can be shown that the MLE estimate for $p_k$ is given by $\\hat{p}_k=x_k/n$. Calculating below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WR5t6IabO5ou"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that allows us to write code that runs eagerly or not\n",
    "def evaluate(tensors):\n",
    "    if tf.executing_eagerly():\n",
    "         return tf.contrib.framework.nest.pack_sequence_as(\n",
    "             tensors,\n",
    "             [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
    "             for t in tf.contrib.framework.nest.flatten(tensors)])\n",
    "    with tf.Session() as sess:\n",
    "        return sess.run(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE of p vector: [0.049 0.099 0.146 0.4486 0.0508 0.2066]\n"
     ]
    }
   ],
   "source": [
    "# Get the roll data, the roll sums (x_k's), and the number of rolls (n)\n",
    "roll_data = tf.constant(np.loadtxt('unfair_dice.txt'), dtype=tf.float32)\n",
    "roll_sums = tf.reduce_sum(roll_data, axis=0)\n",
    "num_rolls = roll_data.shape[0]\n",
    "\n",
    "# Calculate the MLE\n",
    "p_hat_vec = tf.divide(roll_sums, num_rolls)\n",
    "\n",
    "# Check that probabilities sum to 1, else adjust for rounding error\n",
    "if tf.reduce_sum(p_hat_vec)==1:\n",
    "    tf.print('MLE of p vector:', p_hat_vec)\n",
    "else:\n",
    "    p_hat_vec = tf.divide(p_hat_vec, tf.reduce_sum(p_hat_vec))\n",
    "    tf.print('MLE of p vector:', p_hat_vec)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Md6Yu2YIqXpO"
   },
   "source": [
    "### (2) Use MAP to estimate $\\tilde{p}$. Selecte three different prior distribution (if the distribution is parametrized, select three different enough parameters). Using 5000 sample, compare which prior gives the best estimation of why.\n",
    "\n",
    "[Hint: If the optimization takes too long, try to run a certain amount of steps instead of setting a criteria for the gradient. Check the remaining gradient and determine whether to increase the number of steps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "An alternate way to obtain a point estimate for $\\vec{p}$ is using MAP, which is more akin to the Bayesian approach to creating point estimates in that it multiplies the likelyhood by a prior distribution $P(\\vec{p})$, allowing us to bake in prior beliefs about the distribution of $\\vec{p}$. This is Bayesian because we are now allowing ourselves to talk about $\\vec{p}$ in terms of probabilities, something that is not allowed under the frequentist perspective.\n",
    "\n",
    "Let's play the role of a casino that has noticed a suspicous amount of 1's and 4's being rolled at a specific craps table. We suspect that some of the die at the table are each weighted to land on one of these values more often than the others. To investigate, we have analyzed hours of casino floor film to get 5000 roll results of one particular die (giving our data above), and now we want to investigate if this die is weighted to 1, 4, or is unweighted. Let's make the prior for $\\vec{p}$ a Dirlechet distribution, and use three different $\\alpha$ paramaterizations to reflect our three prior beleifs. We will let all elements of $\\vec{\\alpha}$ equal 1 for the first paramaterization (prior beleif that die is not loaded), we will let $\\alpha_1=5$ with all others being 1 for the second paramaterization (prior beleif that the die is loaded to land on 1), and we will let $\\alpha_4=5$ with all others being 1 for the third paramaterization.\n",
    "\n",
    "Now we want to calculate the MAP estimate for each of these paramaterizations below. However, it is much harder to derive the argmax of the resulting function with respect to $\\tilde{\\vec{p}}$ of this equation analytically, so we will isntead use gradient ascent to find the maximum. Doing this below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tfd.Multinomial(total_count=num_rolls, probs=p_hat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_vec_est for alpha = [1. 1. 1. 1. 1. 1.]:\n",
      "[0.04900011 0.09899996 0.14599997 0.44860008 0.05079995 0.20659997]\n",
      "p_vec_est for alpha = [5. 1. 1. 1. 1. 1.]:\n",
      "[0.04976034 0.09892079 0.14588326 0.44824144 0.05075938 0.20643483]\n",
      "p_vec_est for alpha = [1. 1. 1. 5. 1. 1.]:\n",
      "[0.04896094 0.09892081 0.14588326 0.44904083 0.05075934 0.20643483]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of params to try\n",
    "param_0 = np.ones(6)\n",
    "param_1 = np.copy(param_0)\n",
    "param_5 = np.copy(param_0)\n",
    "param_1[0] = 5\n",
    "param_5[3] = 5\n",
    "\n",
    "param_0 = tf.constant(param_0, dtype=tf.float32)\n",
    "param_1 = tf.constant(param_1, dtype=tf.float32)\n",
    "param_5 = tf.constant(param_5, dtype=tf.float32)\n",
    "\n",
    "param_list = [param_0, param_1, param_5]\n",
    "map_est_list = []\n",
    "\n",
    "for param in param_list:\n",
    "    \n",
    "    LR = 0.001\n",
    "    \n",
    "    # Create some values to keep track of\n",
    "    logit_p_vec_est = tf.constant(np.ones(6), dtype=tf.float32)\n",
    "    abs_grad = np.ones(6)\n",
    "    grad_list = []\n",
    "    \n",
    "    # Define the dirichlet distribution with the proper params as the prior distribution\n",
    "    prior_dis = tfd.Dirichlet(concentration=param)\n",
    "    \n",
    "    while True:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(logit_p_vec_est)\n",
    "            \n",
    "            # Define the likelyhood distribution\n",
    "            lh_dis = tfd.Multinomial(total_count=num_rolls, logits=logit_p_vec_est)\n",
    "            \n",
    "            # Define the MAP loss function\n",
    "            map_loss = (lh_dis.log_prob(value=roll_sums) + \\\n",
    "                        prior_dis.log_prob(value=tf.nn.softmax(logit_p_vec_est)))\n",
    "            \n",
    "            # Take the gradient\n",
    "            grad = tape.gradient(map_loss, logit_p_vec_est)\n",
    "            abs_grad = np.abs(grad.numpy())\n",
    "        \n",
    "        # Adjust p_vec\n",
    "        logit_p_vec_est += LR*grad\n",
    "        \n",
    "        grad_list.append(grad.numpy())\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.all(abs_grad<0.001):\n",
    "            break\n",
    "\n",
    "    \n",
    "    p_vec_est = tf.nn.softmax(logit_p_vec_est)\n",
    "    tf.print(f'p_vec_est for alpha = {param}:\\n{p_vec_est}')\n",
    "    map_est_list.append(p_vec_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the outputs above, we can see that using an uninformative prior returns the same esimate as the MLE. Then for the priors where we alpha to five $\\alpha_1$ or $\\alpha_4$, we get very slightly higher probabilities for $p_1$ and $p_4$, respectively. This is to be expected as the prior is putting higher probabilities on these elements being high, but the effect is minimal since there is so much data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Using Monte-Carlo sampling, generate posterial samples and estimate the 6 probabilities again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate the distribtion for $P(\\vec{p}|D)$ by drawing samples from this distribution using MCMC. We will do this using the fact that $P(\\vec{p}|D)\\propto P(D|\\vec{p})P(\\vec{p})\\propto P(D|\\vec{p})$ for an uninformative prior. Since we know $P(D|\\vec{p})\\sim Multinomial(\\vec{p})$, we are in the perfect situation to estimate the distribuiton of $P(\\vec{p}|D)$ using MCMC. Below, we do this using the Hamiltonian Monte Carlo algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a function for getting the unormalized log probability of logit_p_vec_est given data\n",
    "def unnormalized_log_prob(data, logit_p_vec_est):\n",
    "    dist_est = tfd.Multinomial(total_count=data.shape[0], logits=logit_p_vec_est)\n",
    "    class_counts = class_counts = tf.reduce_sum(data, axis=0)\n",
    "    \n",
    "    return tf.constant(dist_est.log_prob(class_counts))\n",
    "\n",
    "hmc_unnormalized_log_prob = lambda p_vec: unnormalized_log_prob(roll_data, p_vec)\n",
    "\n",
    "# Set nessecary parameters\n",
    "initial_chain_state = [tf.constant(np.ones(6), dtype=tf.float32)]\n",
    "step_size = tf.constant(0.01, dtype=tf.float32)\n",
    "leapfrog_steps_num = 3\n",
    "sample_size = int(10e3)\n",
    "burnin = int(10e3)\n",
    "\n",
    "# Set up tfp to choose adaptive step sizes for the Hamiltonian MC \n",
    "adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "    tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=hmc_unnormalized_log_prob,\n",
    "        num_leapfrog_steps=leapfrog_steps_num,\n",
    "        step_size=step_size\n",
    "    ),\n",
    "    num_adaptation_steps=int(burnin*0.008)\n",
    ")\n",
    "\n",
    "# Create funciton to gather MCMC samples\n",
    "def run_chain():\n",
    "    samples_hmc = tfp.mcmc.sample_chain(\n",
    "        num_results=sample_size,\n",
    "        num_burnin_steps=burnin,\n",
    "        current_state=initial_chain_state,\n",
    "        kernel=adaptive_hmc,\n",
    "        trace_fn=None\n",
    "    )\n",
    "    return samples_hmc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_vec_est: [[0.04897422 0.09898818 0.1458871  0.44879252 0.05074805 0.20660995]]\n"
     ]
    }
   ],
   "source": [
    "# Gather samples and calculate the estimated p_vec\n",
    "samples_hmc = run_chain()\n",
    "p_vec_est = tf.nn.softmax(tf.reduce_mean(samples_hmc, axis=1))\n",
    "tf.print(f'p_vec_est: {p_vec_est}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the estiamate for our vector p above, we can see that HMC gets approximately the same answer as the previous methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
